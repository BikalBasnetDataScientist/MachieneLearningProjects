{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hIbr52I7Z7U"
   },
   "source": [
    "CogniFrame Assignment : MNIST Digit Classification Model\n",
    "=============\n",
    "\n",
    "Task : \n",
    "\n",
    "1. Construct a classification model on the MNIST data set in the form of a deep neural network of your choice.\n",
    "\n",
    "2. Perform and compare two or more methods of hyperparameter optimization on this model, and comment on the comparison.\n",
    "\n",
    "\n",
    "Link : https://www.kaggle.com/c/digit-recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "apJbCsBHl-2A"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "from datetime import datetime as dt\n",
    "from scipy import ndimage\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4riXK3IoHgx6"
   },
   "source": [
    "# 1 :  Data  Load\n",
    "\n",
    "MNIST Data as downloaded from \"http://yann.lecun.com/exdb/mnist/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "mndata = MNIST('./data')\n",
    "images, labels = mndata.load_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Completed 2018-05-24 15:49:23.856543\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "from scipy.misc import imsave\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
    "WORK_DIRECTORY = 'data'\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "\n",
    "def maybe_download(filename):\n",
    "  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n",
    "  if not tf.gfile.Exists(WORK_DIRECTORY):\n",
    "    tf.gfile.MakeDirs(WORK_DIRECTORY)\n",
    "  filepath = os.path.join(WORK_DIRECTORY, filename)\n",
    "  if not tf.gfile.Exists(filepath):\n",
    "    filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
    "    with tf.gfile.GFile(filepath) as f:\n",
    "      size = f.Size()\n",
    "    print('Successfully downloaded', filename, size, 'bytes.')\n",
    "  return filepath\n",
    "\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "  \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "  Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "  \"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(16)\n",
    "    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "    #data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "  \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(8)\n",
    "    buf = bytestream.read(1 * num_images)\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "  return labels\n",
    "\n",
    "train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n",
    "train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n",
    "test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n",
    "test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "# Extract it into np arrays.\n",
    "X_train = extract_data(train_data_filename, 60000)\n",
    "y_train = extract_labels(train_labels_filename, 60000)\n",
    "X_test = extract_data(test_data_filename, 10000)\n",
    "y_test = extract_labels(test_labels_filename, 10000)\n",
    "\n",
    "print('Completed', dt.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid = X_test\n",
    "y_valid = y_test\n",
    "X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   3.,  18.,  18.,  18., 126., 136., 175.,  26., 166., 255.,\n",
       "        247., 127.,   0.,   0.,   0.,   0.],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  30.,  36.,  94.,\n",
       "        154., 170., 253., 253., 253., 253., 253., 225., 172., 253., 242.,\n",
       "        195.,  64.,   0.,   0.,   0.,   0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0,5:7,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Simple Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list(set(tr['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Defining the confusion matrix function\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):   \n",
    "    import itertools\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "                \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "def eval_classification(y_test, ypred_test ):\n",
    "    print('Accuracy is ', accuracy_score(y_test, ypred_test) )\n",
    "    print('Confusion report is')\n",
    "    print(classification_report(y_test, ypred_test) )    \n",
    "    conf_matrix = confusion_matrix(y_test, ypred_test)\n",
    "    plot_confusion_matrix(conf_matrix, classes = list(set(tr['label'])), title = 'Confusion Matrix')\n",
    "    return(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 GridSearchCV for optimal parameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Simple Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_tree(X_train, y_train, X_test, y_test, \\\n",
    "            max_depth = None, class_weight = None):\n",
    "    from sklearn import tree\n",
    "    clf = tree.DecisionTreeClassifier( max_depth =   max_depth,\\\n",
    "                        class_weight = class_weight )\n",
    "    clf.fit(X_train, y_train)\n",
    "    ypred_test = clf.predict(X_test)\n",
    "\n",
    "    eval_classification(y_test, ypred_test )\n",
    "    return clf\n",
    "    #import pydotplus \n",
    "    #from IPython.display import Image  \n",
    "    #dot_data = tree.export_graphviz( clf, feature_names=feature_cols,out_file=None)  \n",
    "    #graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "    #return Image(graph.create_png())\n",
    "\n",
    "# clf = simple_tree(X_train, y_train, X_test, y_test, 15, 'balanced' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Simple Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 50, max_depth = 20, random_state= 40)\n",
    "#forest.fit(X_train, y_train)\n",
    "#eval_classification(y_test,  forest.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.1 Scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape\n",
    "raw = (X_train / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "        0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.36862746, 0.6039216 , 0.6666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235295, 0.6745098 ,\n",
       "        0.99215686, 0.9490196 , 0.7647059 , 0.2509804 , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.shape\n",
    "raw[0,5:7,10:28,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0.,   3.,  18.,  18.,  18., 126., 136., 175.,  26., 166.,\n",
       "        255., 247., 127.,   0.,   0.,   0.,   0.],\n",
       "       [ 94., 154., 170., 253., 253., 253., 253., 253., 225., 172., 253.,\n",
       "        242., 195.,  64.,   0.,   0.,   0.,   0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0,5:7,10:28,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## #  scale the pixels  #  3. Scale 0-255 to (-0.5  to 0.5)\n",
    "def scale_img(tr):\n",
    "    #return ( (tr - 255.0) / 2.0 ) / 255.0\n",
    "    return  tr / 255.0\n",
    "\n",
    "X_train = scale_img(X_train)\n",
    "X_test  = scale_img(X_test)\n",
    "X_valid = scale_img(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train.max(axis=1).sort_values(ascending=False)[0:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.2 Reformat Image to fit ConvNets\n",
    "\n",
    "Convert image into 3d array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make training easier down the road.\n",
    "\n",
    "\n",
    "Reformat into a TensorFlow-friendly shape:\n",
    "\n",
    "    convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "    labels as float 1-hot encodings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape ::  (60000, 28, 28, 1) . Train Label Shape  (60000,)\n",
      "To be converted into TensofrFlow friendly shape\n"
     ]
    }
   ],
   "source": [
    "print('Training Data Shape :: ',X_train.shape, '. Train Label Shape ', y_train.shape )\n",
    "print('To be converted into TensofrFlow friendly shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7],\n",
       "       [2],\n",
       "       [1],\n",
       "       ...,\n",
       "       [4],\n",
       "       [5],\n",
       "       [6]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labels[:,None])     for   (28000,)\n",
    "y_test[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = 10\n",
    "import numpy as np\n",
    "\n",
    "#trs = X_train.as_matrix().reshape((-1, 28, 28, channel_in)).astype(np.float32)\n",
    "#trs.shape\n",
    "#labels = (np.arange(num_labels) == labels.as[:,None]).astype(np.float32)\n",
    "#np.arange(num_labels)\n",
    "( np.arange(num_labels) == y_test[:,None] ).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (60000, 28, 28, 1) (60000,)\n",
      "a is in ab\n"
     ]
    }
   ],
   "source": [
    "print('Training set', X_train.shape, y_train.shape)\n",
    "ab = ('a','b')\n",
    "if 'a' in ab:  print('a is in ab')\n",
    "if 'c' in ab:  print('c is in ab')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (60000, 28, 28, 1) (60000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "channel_in = 1 # grayscale\n",
    "\n",
    "def reformat(dataset, labels, limit_to = ('dataset','labels') ):\n",
    "  import numpy as np\n",
    "  if 'dataset' in limit_to:\n",
    "      dataset = dataset.as_matrix().reshape(\n",
    "            (-1, image_size, image_size, channel_in)).astype(np.float32)\n",
    "  if 'labels' in limit_to:\n",
    "      #labels = (np.arange(num_labels) == labels.as_matrix()[:,:]).astype(np.float32)\n",
    "      labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(X_train, y_train, ('labels') )\n",
    "valid_dataset, valid_labels = reformat(X_valid, y_valid, ('labels') )\n",
    "test_dataset, test_labels = reformat(X_test, y_test, ('labels'))\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "#print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print('Test set', test_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0.2 Extend data with rotation, stretch, shift and zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Training set (39900, 28, 28, 1) (39900, 10)\n",
      "To Training augmentation set (159600, 28, 28, 1) (159600, 10)\n",
      "TEST WITH DATA AUGMENTATION FAILED. THE accuracy in fact decreased.\n"
     ]
    }
   ],
   "source": [
    "def augment_data(dataset, dataset_labels, augementation_factor=1, use_random_rotation=True, use_random_shear=True, use_random_shift=True, use_random_zoom=True):\n",
    "    import tensorflow as  tf\n",
    "    import numpy as np\n",
    "    augmented_image = []\n",
    "    augmented_image_labels = []\n",
    "\n",
    "    for num in range (0, dataset.shape[0]):\n",
    "\n",
    "        for i in range(0, augementation_factor):\n",
    "            # original image:\n",
    "            augmented_image.append(dataset[num])\n",
    "            augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "            if use_random_rotation:\n",
    "                augmented_image.append(tf.contrib.keras.preprocessing.image.random_rotation(dataset[num], 20, row_axis=0, col_axis=1, channel_axis=2))\n",
    "                augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "            if use_random_shear:\n",
    "                augmented_image.append(tf.contrib.keras.preprocessing.image.random_shear(dataset[num], 0.2, row_axis=0, col_axis=1, channel_axis=2))\n",
    "                augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "            if use_random_shift:\n",
    "                augmented_image.append(tf.contrib.keras.preprocessing.image.random_shift(dataset[num], 0.2, 0.2, row_axis=0, col_axis=1, channel_axis=2))\n",
    "                augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "            #if use_random_zoom:\n",
    "            #    augmented_image.append(tf.contrib.keras.preprocessing.image.random_zoom(dataset[num], 0.9, row_axis=0, col_axis=1, channel_axis=2))\n",
    "            #    augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "    return np.array(augmented_image), np.array(augmented_image_labels)\n",
    "\n",
    "print('From Training set', train_dataset.shape, train_labels.shape)\n",
    "#train_dataset,train_labels  = augment_data(train_dataset, train_labels)\n",
    "print('To Training augmentation set', train_dataset.shape, train_labels.shape)\n",
    "print('TEST WITH DATA AUGMENTATION FAILED. THE accuracy in fact decreased.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 TensforFlow Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0.1 Tensorflow evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0.2 TensorFlow Model : 2 Conv Layer + 1 FC layer\n",
    "Two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "channel_in = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "def accuracy(predictions, labels):\n",
    "  #predictions = tf.argmax(predictions,1)\n",
    "  #labels = tf.argmax(labels, 1)\n",
    "  #correct_prediction =  tf.equal(predictions, labels )\n",
    "  #accuracy = tf.reduce_mean( tf.cast(correct_prediction, tf.float32))\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "  # gives [True, False, True, True,...] based on whethe predictions and lables is equal or not\n",
    "  print(datetime.datetime.now())\n",
    "\n",
    "def GetOptimiser(tf, gradient_optimizer, loss, rate):    \n",
    "    #print('gradient_optimizer is :: ',gradient_optimizer)\n",
    "    if gradient_optimizer == 'GradientDescent':\n",
    "        print('SELECTED OPTIMISER :: Gradient Descent optimisier with default rate - 0.05')\n",
    "        return tf.train.GradientDescentOptimizer(0.05).minimize(loss)        \n",
    "    elif  gradient_optimizer == 'Adam':\n",
    "        print('SELECTED OPTIMISER :: Adam with learning rate - ', rate)\n",
    "        return tf.train.AdamOptimizer(rate).minimize(loss)\n",
    "    else:\n",
    "        print('SELECTED OPTIMISER :: Invalid optimiser selected. Valid Optimiser : {GradientDescent | Adam}')\n",
    "        return False\n",
    "\n",
    "def InitNRunTfModel(num_steps,  batch_size, patch_size, depth,  num_hidden,\\\n",
    "                report_interval_steps,  random_seed, gradient_optimizer, learning_rate, beta\\\n",
    "                   ,learning_decay_impl):\n",
    "    import tensorflow as tf\n",
    "    import random\n",
    "    print('patch size ', patch_size)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "      # Model.\n",
    "      def conv2d(data, weights, biases, strides, padding = 'SAME'):\n",
    "        conv = tf.nn.conv2d(data, weights, strides, padding='SAME') # sweep a 2-D filter over a batch of images, with tf.nn.conv2d  \n",
    "        return tf.nn.relu(tf.add(conv , biases))                \n",
    "    \n",
    "      def maxpool2d(data , k=2):\n",
    "        return tf.nn.max_pool(data, ksize = [1,k,k,1], strides = [1,k,k,1], padding = 'SAME')\n",
    "      \n",
    "      def weight_var(shape):\n",
    "        return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "      \n",
    "      def bias_var(shape):\n",
    "        return tf.Variable(tf.constant(1.0, shape=shape))\n",
    "      \n",
    "      # conv layer = Conv + Max pool  \n",
    "      def conv_layer(data, weights, biases, strides, padding):\n",
    "            conv = tf.nn.conv2d(data, weights, strides, padding='SAME') # sweep a 2-D filter over a batch of images, with tf.nn.conv2d  \n",
    "            actv_out = tf.nn.relu(tf.add(conv , biases))\n",
    "            k = 2\n",
    "            return tf.nn.max_pool(actv_out, ksize = [1,k,k,1], strides = [1,k,k,1], padding = 'SAME')\n",
    "               \n",
    "      def fc_layer(data, channel_in, channel_out, name):\n",
    "        with tf.name_scope(name):\n",
    "            fc_weights = tf.Variable( tf.truncated_normal([channel_in, channel_out] ) ,name = \"fc_weights\")\n",
    "            fc_biases = tf.Variable(tf.constant(1.0,shape=[channel_out]), name =\"fc_biases\")\n",
    "            return tf.nn.relu(tf.add(tf.matmul(data, fc_weights), fc_biases))\n",
    "        \n",
    "      # Input data.\n",
    "      tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, channel_in))\n",
    "      tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "      tf_valid_dataset = tf.constant(valid_dataset)\n",
    "      tf_test_dataset = tf.constant(test_dataset)\n",
    "      \n",
    "        # Variables.\n",
    "      channel_out = { 1 : depth , 2 : depth*4 , 3 : num_hidden , 4 : num_labels }\n",
    "      weights = {11 : weight_var([ 1, 1, channel_in, channel_out[1] ]),\n",
    "                 13 : weight_var([ 3, 3, channel_in, channel_out[1] ]),\n",
    "                 15 : weight_var([ 5, 5, channel_in, channel_out[1] ]),\n",
    "                 17 : weight_var([ 5, 5, channel_in, channel_out[1] ]),                 \n",
    "                 21 : weight_var([ 1 , 1, channel_out[1]*4 ,channel_out[2] ]),\n",
    "                 23 : weight_var([ 3 , 3, channel_out[1]*4 ,channel_out[2] ]),\n",
    "                 25 : weight_var([ 5 , 5, channel_out[1]*4 ,channel_out[2] ]),\n",
    "                 27 : weight_var([ 7 , 7, channel_out[1]*4 ,channel_out[2] ]),\n",
    "                 \n",
    "                 201 : weight_var([patch_size, patch_size, channel_out[2]*4, 256 ]),\n",
    "                 3 : weight_var([256, num_hidden]),\n",
    "                 4 : weight_var([num_hidden, num_labels]),\n",
    "                }\n",
    "      biases = {1 : bias_var( [channel_in] ),\n",
    "                2 : bias_var( [channel_out[1]*4] ),                \n",
    "                201: bias_var( [256] ),\n",
    "                3 : bias_var( [num_hidden] ),\n",
    "                4 : bias_var( [num_labels] ),\n",
    "               }          \n",
    "         \n",
    "    # Image -> Conv+MaxPool -> Conv+MaxPool -> FC-> O/p\n",
    "      data = tf_train_dataset\n",
    "      def model(data):       \n",
    "            print('Using  Vanilla With no max pool. Drop out not supported yet')            \n",
    "            strides = [1, 2, 2, 1]   # A stride of sliding windows for each dimension. Here two dimension hence 4 stride param                 \n",
    "            conv1x1_1 = conv_layer(data, weights[11], biases[1], strides,'SAME'  )\n",
    "            conv3x3_1 = conv_layer(data, weights[13], biases[1], strides,'SAME'  )\n",
    "            conv5x5_1 = conv_layer(data, weights[15], biases[1], strides,'SAME'  )\n",
    "            conv7x7_1 = conv_layer(data, weights[17], biases[1], strides,'SAME'  )\n",
    "            conv1 = tf.concat([conv1x1_1, conv3x3_1, conv5x5_1, conv7x7_1 ],3)\n",
    "          \n",
    "            conv1x1_2 = conv_layer(conv1, weights[21], biases[2], strides,'SAME'  )\n",
    "            conv3x3_2 = conv_layer(conv1, weights[23], biases[2], strides,'SAME'  )\n",
    "            conv5x5_2 = conv_layer(conv1, weights[25], biases[2], strides,'SAME'  )\n",
    "            conv7x7_2 = conv_layer(conv1, weights[27], biases[2], strides,'SAME'  )\n",
    "            conv2 = tf.concat([conv1x1_2, conv3x3_2, conv5x5_2, conv7x7_2 ],3)\n",
    "            \n",
    "            conv3 = conv_layer(conv2, weights[201], biases[201], strides,'SAME'  )\n",
    "            maxpool2 = conv3\n",
    "            shape = maxpool2.get_shape().as_list()\n",
    "            reshape = tf.reshape(maxpool2, [shape[0],shape[1] * shape[2] * shape[3] ])\n",
    "            fc = tf.nn.relu(tf.nn.bias_add(tf.matmul(reshape, weights[3]) , biases[3]))\n",
    "            # adding dropout\n",
    "            #keep_prob = tf.placeholder(tf.float32)            \n",
    "            #fc = tf.nn.dropout(fc,  keep_prob)\n",
    "            return tf.nn.bias_add(tf.matmul(fc, weights[4]), biases[4] )        \n",
    "              # Training computation.\n",
    "      logits = model(tf_train_dataset)\n",
    "      loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))      \n",
    "                # Optimizer.\n",
    "      # l2 regularisation\n",
    "      if(beta != 0):\n",
    "          print('L2 reg invoked')\n",
    "          #regularizer = 0.000\n",
    "          #for k,v in weights.items():\n",
    "          #      v = tf.Print(v,[v]) \n",
    "          regularizer = tf.nn.l2_loss(weights[11]) + tf.nn.l2_loss(weights[13]) + \\\n",
    "                        tf.nn.l2_loss(weights[15]) + tf.nn.l2_loss(weights[17]) + \\\n",
    "                        tf.nn.l2_loss(weights[21]) + tf.nn.l2_loss(weights[23]) + \\\n",
    "                        tf.nn.l2_loss(weights[25]) + tf.nn.l2_loss(weights[27]) + \\\n",
    "                        tf.nn.l2_loss(weights[3]) + tf.nn.l2_loss(weights[4]) \n",
    "          loss = tf.reduce_mean(loss + beta* regularizer )\n",
    "      else:\n",
    "          print('NO L2 reg invoked. To use it, set flag => l2_reg=True')\n",
    "        \n",
    "      if(learning_decay_impl == True):\n",
    "          print('''Decaying learning rate Implemented''')\n",
    "          global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "          start_learning_rate = 0.5\n",
    "          learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
    "          #optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "          optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "      else:\n",
    "        print('''NO Decaying learning rate Implemented''')\n",
    "        optimizer = GetOptimiser(tf, gradient_optimizer, loss, rate = learning_rate)\n",
    "      #optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "      #optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "              # Predictions for the training, validation, and test data.\n",
    "      train_prediction = tf.nn.softmax(logits)\n",
    "      valid_prediction = tf.nn.softmax( model(tf_valid_dataset))\n",
    "      test_prediction = tf.nn.softmax( model(tf_test_dataset))\n",
    "    #print('TensorFlow Graph Initialisation completed at ',datetime.datetime.now())\n",
    "    \n",
    "#def RunTfModel(graph, num_steps, batch_size, report_interval_steps = 50, random_seed = 12  ):        \n",
    "    ################\n",
    "    ### Run TF model\n",
    "    ###############\n",
    "    start_time = datetime.datetime.now()\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      #tf.global_variables_initializer().run()\n",
    "      tf.initialize_all_variables().run()\n",
    "      print('Initialized')\n",
    "      for step in range(num_steps):\n",
    "        random.seed(random_seed*step)    \n",
    "        offset = random.randint( 1, train_labels.shape[0] - batch_size )        \n",
    "        #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)        \n",
    "        if (step % report_interval_steps == 0):\n",
    "            print('Step %4d :: Minibatch ==> Loss: %f .Accuracy: %.1f%% .Validation Acc.: %.1f%% DataOffset:%7d'\\\n",
    "                    % (step, l, accuracy(predictions, batch_labels), accuracy(valid_prediction.eval(), valid_labels), offset ))\n",
    "            print('Time ', datetime.datetime.now() )\n",
    "      #print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "      test_prediction_0n9_percent =  test_prediction.eval()\n",
    "      res = pd.DataFrame(np.argmax(test_prediction_0n9_percent,1))\n",
    "      res.index += 1\n",
    "      res.to_csv('results.csv')     \n",
    "      #tf.reset_default_graph()\n",
    "      session.close()    \n",
    "    tf.reset_default_graph()    \n",
    "    print('Total Time taken', datetime.datetime.now() - start_time)\n",
    "    \n",
    "def RunNN(num_steps = 101, batch_size = 16, patch_size = 5, depth = 16, num_hidden = 64 \\\n",
    "         , report_interval_steps = 50, random_seed = 12, gradient_optimizer = 'None'\\\n",
    "         , learning_rate = 0.0001, beta = 0, learning_decay_impl = False):        \n",
    "    InitNRunTfModel( num_steps = num_steps, batch_size = batch_size, patch_size = patch_size, \\\n",
    "            depth = depth, num_hidden = num_hidden , report_interval_steps = report_interval_steps,\\\n",
    "            random_seed = random_seed, gradient_optimizer= gradient_optimizer,\\\n",
    "            learning_rate = learning_rate, beta = beta, learning_decay_impl = learning_decay_impl)\n",
    "    #RunTfModel(graph, num_steps = num_steps , batch_size = batch_size ,report_interval_steps = report_interval_steps, random_seed = random_seed  )\n",
    "    \n",
    "print('NN Function Init Completed', datetime.datetime.now()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch size  5\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "WARNING:tensorflow:From <ipython-input-25-9bc04e0acc8b>:115: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "L2 reg invoked\n",
      "Decaying learning rate Implemented\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Step    0 :: Minibatch ==> Loss: 24.527287 .Accuracy: 15.0% .Validation Acc.: 10.3% DataOffset:  25248\n",
      "Step    5 :: Minibatch ==> Loss: 3512.427979 .Accuracy: 5.0% .Validation Acc.: 10.0% DataOffset:  20173\n",
      "Total Time taken 0:01:45.120737\n",
      "Total time taken 0:02:39.045693\n"
     ]
    }
   ],
   "source": [
    "# to do : implement elu to relu\n",
    "#change plain plus to tf.add functions # no significant imporvement in performance\n",
    "# reduce learning rate to 0.0001 because learning seems to plateau at 96 %. Go to 0.00001 if needed.\n",
    "#st = datetime.datetime.now()\n",
    "st = datetime.datetime.now()\n",
    "#RunNN( num_steps = 10, batch_size = 20, patch_size = 5, depth = 16, num_hidden = 64 \\\n",
    "#            , report_interval_steps = 5 , random_seed = 12, gradient_optimizer = 'Adam'\\\n",
    "#      , learning_rate = 0.001, beta = 0.01, learning_decay_impl = True  )\n",
    "print('Total time taken',datetime.datetime.now()- st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0.3 Run Vanilla ConvNet\n",
    "Test across diff patch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch size  5\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "L2 reg invoked\n",
      "NO Decaying learning rate Implemented\n",
      "SELECTED OPTIMISER :: Adam with learning rate -  0.001\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Initialized\n",
      "Step    0 :: Minibatch ==> Loss: 24.425711 .Accuracy: 15.0% .Validation Acc.: 9.9% DataOffset:  25248\n",
      "Time  2018-05-21 02:58:12.118533\n",
      "Step 5000 :: Minibatch ==> Loss: 0.840437 .Accuracy: 80.0% .Validation Acc.: 91.8% DataOffset:  27308\n",
      "Time  2018-05-21 03:09:59.419559\n",
      "Step 10000 :: Minibatch ==> Loss: 0.403290 .Accuracy: 85.0% .Validation Acc.: 93.1% DataOffset:   5118\n",
      "Time  2018-05-21 03:21:28.593204\n",
      "Step 15000 :: Minibatch ==> Loss: 0.168340 .Accuracy: 100.0% .Validation Acc.: 93.3% DataOffset:   8704\n",
      "Time  2018-05-21 03:33:00.676229\n",
      "Step 20000 :: Minibatch ==> Loss: 0.432823 .Accuracy: 90.0% .Validation Acc.: 94.9% DataOffset:  19073\n",
      "Time  2018-05-21 03:44:16.294486\n",
      "Step 25000 :: Minibatch ==> Loss: 0.519874 .Accuracy: 90.0% .Validation Acc.: 94.2% DataOffset:  35123\n",
      "Time  2018-05-21 03:55:39.414677\n",
      "Step 30000 :: Minibatch ==> Loss: 0.267904 .Accuracy: 95.0% .Validation Acc.: 94.5% DataOffset:  32530\n",
      "Time  2018-05-21 04:07:03.954609\n",
      "Step 35000 :: Minibatch ==> Loss: 0.186259 .Accuracy: 100.0% .Validation Acc.: 95.3% DataOffset:  28560\n",
      "Time  2018-05-21 04:18:30.385349\n",
      "Step 40000 :: Minibatch ==> Loss: 0.207391 .Accuracy: 100.0% .Validation Acc.: 95.4% DataOffset:  10963\n",
      "Time  2018-05-21 04:29:49.244245\n",
      "Step 45000 :: Minibatch ==> Loss: 0.202812 .Accuracy: 100.0% .Validation Acc.: 95.5% DataOffset:  27525\n",
      "Time  2018-05-21 04:41:14.639662\n",
      "Step 50000 :: Minibatch ==> Loss: 0.339173 .Accuracy: 95.0% .Validation Acc.: 92.4% DataOffset:  26795\n",
      "Time  2018-05-21 04:52:32.320065\n",
      "Step 55000 :: Minibatch ==> Loss: 0.144501 .Accuracy: 100.0% .Validation Acc.: 95.7% DataOffset:  15353\n",
      "Time  2018-05-21 05:03:55.069233\n",
      "Step 60000 :: Minibatch ==> Loss: 0.265487 .Accuracy: 95.0% .Validation Acc.: 95.3% DataOffset:  31240\n",
      "Time  2018-05-21 05:15:17.470120\n",
      "Step 65000 :: Minibatch ==> Loss: 0.371069 .Accuracy: 95.0% .Validation Acc.: 94.4% DataOffset:  14106\n",
      "Time  2018-05-21 05:26:42.981422\n",
      "Step 70000 :: Minibatch ==> Loss: 0.205625 .Accuracy: 95.0% .Validation Acc.: 94.1% DataOffset:  15754\n",
      "Time  2018-05-21 05:38:07.113214\n",
      "Step 75000 :: Minibatch ==> Loss: 0.503322 .Accuracy: 90.0% .Validation Acc.: 95.6% DataOffset:    714\n",
      "Time  2018-05-21 05:49:33.702598\n",
      "Step 80000 :: Minibatch ==> Loss: 0.357122 .Accuracy: 90.0% .Validation Acc.: 94.5% DataOffset:  16888\n",
      "Time  2018-05-21 06:00:58.056330\n",
      "Step 85000 :: Minibatch ==> Loss: 0.131229 .Accuracy: 100.0% .Validation Acc.: 94.5% DataOffset:  35144\n",
      "Time  2018-05-21 06:12:28.898057\n",
      "Step 90000 :: Minibatch ==> Loss: 0.127729 .Accuracy: 100.0% .Validation Acc.: 95.7% DataOffset:   6852\n",
      "Time  2018-05-21 06:23:55.286749\n",
      "Step 95000 :: Minibatch ==> Loss: 0.131350 .Accuracy: 100.0% .Validation Acc.: 95.3% DataOffset:  32901\n",
      "Time  2018-05-21 06:36:05.251778\n",
      "Step 100000 :: Minibatch ==> Loss: 0.204214 .Accuracy: 95.0% .Validation Acc.: 95.5% DataOffset:  33693\n",
      "Time  2018-05-21 06:47:28.433536\n",
      "Step 105000 :: Minibatch ==> Loss: 0.125555 .Accuracy: 100.0% .Validation Acc.: 95.4% DataOffset:  12415\n",
      "Time  2018-05-21 06:58:56.900408\n",
      "Step 110000 :: Minibatch ==> Loss: 0.488993 .Accuracy: 95.0% .Validation Acc.: 95.2% DataOffset:   9339\n",
      "Time  2018-05-21 07:10:22.319119\n",
      "Step 115000 :: Minibatch ==> Loss: 0.405898 .Accuracy: 95.0% .Validation Acc.: 95.0% DataOffset:   7068\n",
      "Time  2018-05-21 07:21:47.234651\n",
      "Total Time taken 4:41:50.905431\n",
      "Total time taken 4:41:55.060996\n"
     ]
    }
   ],
   "source": [
    "st = datetime.datetime.now()\n",
    "#RunNN( num_steps = 120000, batch_size = 20, patch_size = 5, depth = 16, num_hidden = 64 \\\n",
    "#            , report_interval_steps = 5000 , random_seed = 12, gradient_optimizer = 'Adam'\\\n",
    "#      , learning_rate = 0.001, learning_decay_impl = False ,beta = 0 )\n",
    "print('Total time taken',datetime.datetime.now()- st)\n",
    "#python operator plus = 59s, 54s, 57s - 95.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch size  5\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "L2 reg invoked\n",
      "NO Decaying learning rate Implemented\n",
      "SELECTED OPTIMISER :: Adam with learning rate -  0.001\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Initialized\n",
      "Step    0 :: Minibatch ==> Loss: 262.676086 .Accuracy: 10.0% .Validation Acc.: 10.3% DataOffset:  25248\n",
      "Step 3000 :: Minibatch ==> Loss: 1.521674 .Accuracy: 90.0% .Validation Acc.: 91.7% DataOffset:  18896\n",
      "Step 6000 :: Minibatch ==> Loss: 0.666630 .Accuracy: 100.0% .Validation Acc.: 94.7% DataOffset:  28157\n",
      "Step 9000 :: Minibatch ==> Loss: 0.355961 .Accuracy: 95.0% .Validation Acc.: 95.5% DataOffset:  20336\n",
      "Step 12000 :: Minibatch ==> Loss: 0.218071 .Accuracy: 100.0% .Validation Acc.: 96.2% DataOffset:  33444\n",
      "Step 15000 :: Minibatch ==> Loss: 0.145269 .Accuracy: 100.0% .Validation Acc.: 95.2% DataOffset:   8704\n",
      "Step 18000 :: Minibatch ==> Loss: 0.227120 .Accuracy: 95.0% .Validation Acc.: 96.6% DataOffset:  16659\n",
      "Step 21000 :: Minibatch ==> Loss: 0.211687 .Accuracy: 95.0% .Validation Acc.: 94.9% DataOffset:  28494\n",
      "Step 24000 :: Minibatch ==> Loss: 0.131312 .Accuracy: 100.0% .Validation Acc.: 96.4% DataOffset:  31850\n",
      "Step 27000 :: Minibatch ==> Loss: 0.123735 .Accuracy: 100.0% .Validation Acc.: 94.7% DataOffset:  36021\n",
      "Step 30000 :: Minibatch ==> Loss: 0.122291 .Accuracy: 100.0% .Validation Acc.: 96.3% DataOffset:  32530\n",
      "Step 33000 :: Minibatch ==> Loss: 0.118472 .Accuracy: 100.0% .Validation Acc.: 95.9% DataOffset:  29872\n",
      "Step 36000 :: Minibatch ==> Loss: 0.126344 .Accuracy: 100.0% .Validation Acc.: 95.8% DataOffset:  29335\n",
      "Step 39000 :: Minibatch ==> Loss: 0.113258 .Accuracy: 100.0% .Validation Acc.: 95.4% DataOffset:  17566\n",
      "Step 42000 :: Minibatch ==> Loss: 0.116628 .Accuracy: 100.0% .Validation Acc.: 96.1% DataOffset:   6520\n",
      "Step 45000 :: Minibatch ==> Loss: 0.119752 .Accuracy: 100.0% .Validation Acc.: 95.7% DataOffset:  27525\n",
      "Step 48000 :: Minibatch ==> Loss: 0.152280 .Accuracy: 100.0% .Validation Acc.: 96.3% DataOffset:  34851\n",
      "Total Time taken 1:47:09.165993\n",
      "Total time taken 1:47:12.397762\n"
     ]
    }
   ],
   "source": [
    "st = datetime.datetime.now()\n",
    "#RunNN( num_steps = 50000, batch_size = 20, patch_size = 5, depth = 16, num_hidden = 64 \\\n",
    "#            , report_interval_steps = 3000 , random_seed = 12, gradient_optimizer = 'Adam'\\\n",
    "#      , learning_rate = 0.001, learning_decay_impl = False,l2_reg = True )\n",
    "print('Total time taken',datetime.datetime.now()- st)\n",
    "#python operator plus = 59s, 54s, 57s - 95.7%\n",
    "# python two plus operator replaced with tf.nn.bias_add : 61s, 57s, 57s - 95.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-23 22:26:39.871248\n",
      "NN Function Init Completed 2018-05-23 22:26:39.872545\n"
     ]
    }
   ],
   "source": [
    "num_channels = 1\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "  # gives [True, False, True, True,...] based on whethe predictions and lables is equal or not\n",
    "  #correct_prediction = tf.equal( tf.argmax(predictions, 1), tf.argmax(labels, 1)  )\n",
    "  # cast converts bool to float [True, False, True, True] => [1,0,1,1]\n",
    "  #return   tf.cast( (tf.reduce_mean(tf.cast(correct_prediction, tf.float32))), float)\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "def GetOptimiser(tf, gradient_optimizer, loss, rate):    \n",
    "    #print('gradient_optimizer is :: ',gradient_optimizer)\n",
    "    if gradient_optimizer == 'GradientDescent':\n",
    "        print('SELECTED OPTIMISER :: Gradient Descent optimisier with default rate - 0.05')\n",
    "        return tf.train.GradientDescentOptimizer(0.05).minimize(loss)        \n",
    "    elif  gradient_optimizer == 'Adam':\n",
    "        print('SELECTED OPTIMISER :: Adam with learning rate - ', rate)\n",
    "        return tf.train.AdamOptimizer(rate).minimize(loss)\n",
    "    else:\n",
    "        print('SELECTED OPTIMISER :: Invalid optimiser selected. Valid Optimiser : {GradientDescent | Adam}')\n",
    "        return False\n",
    "\n",
    "def InitNRunTfModel1(num_steps,  batch_size, patch_size, depth,  num_hidden,\\\n",
    "                report_interval_steps,  random_seed, gradient_optimizer, learning_rate, beta = 0.001):\n",
    "    import tensorflow as tf\n",
    "    import random\n",
    "    print('patch size ', patch_size)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "      # Input data.\n",
    "      tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "      tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "      tf_valid_dataset = tf.constant(valid_dataset)\n",
    "      tf_test_dataset = tf.constant(test_dataset)\n",
    "              # Variables.\n",
    "      layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "      layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "      layer2_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "      layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "      #layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      #    [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "      #layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "      layer3_weights = tf.Variable(tf.truncated_normal([64, num_hidden], stddev=0.1))\n",
    "      layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "      layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "          [num_hidden, num_labels], stddev=0.1))\n",
    "      layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "               # Model.\n",
    "      def conv2d(data, weights, biases, strides, padding = 'SAME'):\n",
    "        conv = tf.nn.conv2d(data, weights, strides, padding='SAME') # sweep a 2-D filter over a batch of images, with tf.nn.conv2d  \n",
    "        hidden = tf.nn.relu(tf.add(conv , biases))\n",
    "        #hidden = tf.nn.elu(tf.add(conv , biases))\n",
    "        return  hidden\n",
    "      def maxpool2d(data , k=2):\n",
    "        return tf.nn.max_pool(data, ksize = [1,k,k,1], strides = [1,k,k,1], padding = 'SAME')\n",
    "      \n",
    "      # Image -> Conv+MaxPool -> Conv+MaxPool -> FC-> O/p\n",
    "      def model(data):       \n",
    "            print('Using  Vanilla With no max pool. Drop out not supported yet')\n",
    "            strides = [1, 2, 2, 1]   # A stride of sliding windows for each dimension. Here two dimension hence 4 stride param     \n",
    "            conv1 = conv2d(data, layer1_weights, layer1_biases, strides, 'SAME')\n",
    "            maxpool1 = maxpool2d(conv1, 2)\n",
    "            conv2 = conv2d( maxpool1, layer2_weights, layer2_biases, strides, 'SAME')\n",
    "            maxpool2 = maxpool2d(conv2, 2)\n",
    "            #conv = tf.nn.conv2d(data, layer1_weights, strides, padding='SAME') # sweep a 2-D filter over a batch of images, with tf.nn.conv2d  \n",
    "            #hidden = tf.nn.relu(tf.nn.bias_add(conv , layer1_biases))\n",
    "            #conv = tf.nn.conv2d(hidden, layer2_weights, strides, padding='SAME')\n",
    "            #hidden = tf.nn.relu(tf.nn.bias_add(conv , layer2_biases))           \n",
    "            shape = maxpool2.get_shape().as_list()\n",
    "            #reshape = tf.reshape(maxpool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            reshape = tf.reshape(maxpool2, [shape[0],shape[1] * shape[2] * shape[3] ])\n",
    "            fc = tf.nn.relu(tf.nn.bias_add(tf.matmul(reshape, layer3_weights) , layer3_biases))\n",
    "            return tf.nn.bias_add(tf.matmul(fc, layer4_weights) , layer4_biases)        \n",
    "              # Training computation.\n",
    "      logits = model(tf_train_dataset)\n",
    "      loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))      \n",
    "                # Optimizer.\n",
    "      print('L2 reg invoked')\n",
    "      #regularizer = 0.000\n",
    "      #for k,v in weights.items():\n",
    "      #      v = tf.Print(v,[v]) \n",
    "      regularizer = tf.nn.l2_loss( layer1_weights ) + tf.nn.l2_loss(layer2_weights ) + \\\n",
    "                    tf.nn.l2_loss( layer3_weights ) + tf.nn.l2_loss(layer4_weights ) \n",
    "                    #tf.nn.l2_loss( layer1_weights ) + tf.nn.l2_loss(layer2_weights ) + \\\n",
    "      #regularizer = tf.Print(regularizer,[regularizer])      \n",
    "      #loss_no_reg = loss\n",
    "      loss = tf.reduce_mean(loss + beta* regularizer )\n",
    "      #optimizer = GetOptimiser(tf, gradient_optimizer, loss, rate = learning_rate)\n",
    "      #optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "      optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "              # Predictions for the training, validation, and test data.\n",
    "      train_prediction = tf.nn.softmax(logits)\n",
    "      valid_prediction = tf.nn.softmax( model(tf_valid_dataset))\n",
    "      test_prediction = tf.nn.softmax( model(tf_test_dataset))\n",
    "    #print('TensorFlow Graph Initialisation completed at ',datetime.datetime.now())\n",
    "    \n",
    "#def RunTfModel(graph, num_steps, batch_size, report_interval_steps = 50, random_seed = 12  ):        \n",
    "    ################\n",
    "    ### Run TF model\n",
    "    ###############\n",
    "    start_time = datetime.datetime.now()\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      #tf.global_variables_initializer().run()\n",
    "      tf.initialize_all_variables().run()\n",
    "      print('Initialized')\n",
    "      for step in range(num_steps):\n",
    "        random.seed(random_seed*step)    \n",
    "        offset = random.randint( 1, train_labels.shape[0] - batch_size )        \n",
    "        #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)        \n",
    "        if (step % report_interval_steps == 0):            \n",
    "            print('Step %4d :: Minibatch ==> Loss: %f .Accuracy: %.1f%% .Validation Acc.: %.1f%% DataOffset:%7d'\\\n",
    "                    % (step, l, accuracy(predictions, batch_labels), accuracy(valid_prediction.eval(), valid_labels), offset ))          \n",
    "            print('Time ', datetime.datetime.now() )                        \n",
    "      #print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "      test_prediction_0n9_percent =  test_prediction.eval()\n",
    "      res = pd.DataFrame(np.argmax(test_prediction_0n9_percent,1))\n",
    "      res.index += 1\n",
    "      res.to_csv('results.csv')     \n",
    "      #tf.reset_default_graph()\n",
    "      session.close()    \n",
    "    tf.reset_default_graph()    \n",
    "    print('Training Completed. Total Time taken', datetime.datetime.now() - start_time)\n",
    "    \n",
    "    \n",
    "def RunNN1(num_steps = 101, batch_size = 16, patch_size = 5, depth = 16, num_hidden = 64 \\\n",
    "         , report_interval_steps = 50, random_seed = 12, gradient_optimizer = 'None'\\\n",
    "         , learning_rate = 0.0001,beta = 0.001):        \n",
    "    InitNRunTfModel1( num_steps = num_steps, batch_size = batch_size, patch_size = patch_size, \\\n",
    "            depth = depth, num_hidden = num_hidden , report_interval_steps = report_interval_steps,\\\n",
    "            random_seed = random_seed, gradient_optimizer= gradient_optimizer, learning_rate = learning_rate, beta=0.001)\n",
    "    #RunTfModel(graph, num_steps = num_steps , batch_size = batch_size ,report_interval_steps = report_interval_steps, random_seed = random_seed  )\n",
    "    \n",
    "print('NN Function Init Completed', datetime.datetime.now()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "#from skopt.plots import plot_convergence\n",
    "#from skopt.plots import plot_objective, plot_evaluations\n",
    "#from skopt.plots import plot_histogram, plot_objective_2D\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Real' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ee7dfb49e3d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdim_learning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'log-uniform'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Real' is not defined"
     ]
    }
   ],
   "source": [
    "# Defining HyperParamter Search Boundaries\n",
    "dim_learning_rate = Real(low=1e-6, high=1e-2, prior='log-uniform', name='learning_rate')\n",
    "dim_num_dense_layers = Integer(low=1, high=5, name='num_dense_layers')\n",
    "dim_num_dense_nodes = Integer(low=5, high=512, name='num_dense_nodes')\n",
    "dim_activation = Categorical(categories=['relu', 'sigmoid'], name='activation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch size  5\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "WARNING:tensorflow:From <ipython-input-46-c9ca3f2aef95>:79: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "L2 reg invoked\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Step    0 :: Minibatch ==> Loss: 2.710370 .Accuracy: 5.0% .Validation Acc.: 9.8% DataOffset:  55341\n",
      "Time  2018-05-23 22:27:07.741669\n",
      "Step 10000 :: Minibatch ==> Loss: 0.151283 .Accuracy: 95.0% .Validation Acc.: 98.3% DataOffset:   5118\n",
      "Time  2018-05-23 22:29:03.280594\n",
      "Step 20000 :: Minibatch ==> Loss: 0.100629 .Accuracy: 100.0% .Validation Acc.: 98.3% DataOffset:  19073\n",
      "Time  2018-05-23 22:31:52.301506\n",
      "Step 30000 :: Minibatch ==> Loss: 0.105723 .Accuracy: 97.5% .Validation Acc.: 98.7% DataOffset:  58814\n",
      "Time  2018-05-23 22:33:57.441736\n",
      "Step 40000 :: Minibatch ==> Loss: 0.146674 .Accuracy: 97.5% .Validation Acc.: 98.4% DataOffset:  10963\n",
      "Time  2018-05-23 22:36:17.425465\n",
      "Step 50000 :: Minibatch ==> Loss: 0.065723 .Accuracy: 100.0% .Validation Acc.: 98.7% DataOffset:  26795\n",
      "Time  2018-05-23 22:38:19.942155\n",
      "Step 60000 :: Minibatch ==> Loss: 0.113542 .Accuracy: 97.5% .Validation Acc.: 98.5% DataOffset:  58796\n",
      "Time  2018-05-23 22:40:11.326679\n",
      "Step 70000 :: Minibatch ==> Loss: 0.070910 .Accuracy: 100.0% .Validation Acc.: 98.8% DataOffset:  44680\n",
      "Time  2018-05-23 22:42:04.950972\n",
      "Step 80000 :: Minibatch ==> Loss: 0.063037 .Accuracy: 100.0% .Validation Acc.: 98.9% DataOffset:  16888\n",
      "Time  2018-05-23 22:43:55.558539\n",
      "Step 90000 :: Minibatch ==> Loss: 0.078770 .Accuracy: 100.0% .Validation Acc.: 98.7% DataOffset:   6852\n",
      "Time  2018-05-23 22:45:47.690734\n",
      "Step 100000 :: Minibatch ==> Loss: 0.062331 .Accuracy: 100.0% .Validation Acc.: 98.4% DataOffset:  53485\n",
      "Time  2018-05-23 22:47:38.676095\n",
      "Step 110000 :: Minibatch ==> Loss: 0.083807 .Accuracy: 100.0% .Validation Acc.: 98.5% DataOffset:   9339\n",
      "Time  2018-05-23 22:49:42.520657\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-5e9f41fefbde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mRunNN1\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m             \u001b[0;34m,\u001b[0m \u001b[0mreport_interval_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Adam'\u001b[0m      \u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total time taken'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#python operator plus = 59s, 54s, 57s - 95.7%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# python two plus operator replaced with tf.nn.bias_add : 61s, 57s, 57s - 95.7%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-c9ca3f2aef95>\u001b[0m in \u001b[0;36mRunNN1\u001b[0;34m(num_steps, batch_size, patch_size, depth, num_hidden, report_interval_steps, random_seed, gradient_optimizer, learning_rate, beta)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mRunNN1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m          \u001b[0;34m,\u001b[0m \u001b[0mreport_interval_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'None'\u001b[0m         \u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mInitNRunTfModel1\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_hidden\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mreport_interval_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreport_interval_steps\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0mrandom_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_optimizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mgradient_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0;31m#RunTfModel(graph, num_steps = num_steps , batch_size = batch_size ,report_interval_steps = report_interval_steps, random_seed = random_seed  )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-c9ca3f2aef95>\u001b[0m in \u001b[0;36mInitNRunTfModel1\u001b[0;34m(num_steps, batch_size, patch_size, depth, num_hidden, report_interval_steps, random_seed, gradient_optimizer, learning_rate, beta)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreport_interval_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step %4d :: Minibatch ==> Loss: %f .Accuracy: %.1f%% .Validation Acc.: %.1f%% DataOffset:%7d'\u001b[0m                    \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[0;32m-> 1107\u001b[0;31m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[0m\u001b[1;32m   1108\u001b[0m             raise ValueError('Cannot feed value of shape %r for Tensor %r, '\n\u001b[1;32m   1109\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mis_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \"\"\"\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "st = datetime.datetime.now()\n",
    "RunNN1( num_steps = 300000, batch_size = 40, patch_size = 5, depth = 16, num_hidden = 64 \\\n",
    "            , report_interval_steps = 10000 , random_seed = 12, gradient_optimizer = 'Adam'\\\n",
    "      , learning_rate = 0.001, beta = 0)\n",
    "print('Total time taken',datetime.datetime.now()- st)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using Tflearn :\n",
    "A simple python library built on top of tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "2017-03-28 21:17:06.539158\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n",
      "2017-03-28 21:17:09.129803\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print( datetime.datetime.now() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "AgQDIREv02p1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-03-28 21:17:13.350547\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "  # gives [True, False, True, True,...] based on whethe predictions and lables is equal or not\n",
    "  #correct_prediction = tf.equal( tf.argmax(predictions, 1), tf.argmax(labels, 1)  )\n",
    "  # cast converts bool to float [True, False, True, True] => [1,0,1,1]\n",
    "  #return   tf.cast( (tf.reduce_mean(tf.cast(correct_prediction, tf.float32))), float)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Function Init Completed 2017-03-28 21:17:20.650543\n"
     ]
    }
   ],
   "source": [
    "def GetOptimiser(tf, gradient_optimizer, loss, rate):\n",
    "    #print('gradient_optimizer is :: ',gradient_optimizer)\n",
    "    if gradient_optimizer == 'GradientDescent':\n",
    "        print('SELECTED OPTIMISER :: Gradient Descent optimisier with default rate - 0.05')\n",
    "        return tf.train.GradientDescentOptimizer(0.05).minimize(loss)        \n",
    "    elif  gradient_optimizer == 'Adam':\n",
    "        print('SELECTED OPTIMISER :: Adam with learning rate - ', rate)\n",
    "        return tf.train.AdamOptimizer(rate).minimize(loss)\n",
    "    else:\n",
    "        print('SELECTED OPTIMISER :: Invalid optimiser selected. Valid Optimiser : {GradientDescent | Adam}')\n",
    "        return False\n",
    "\n",
    "def InitNRunTfModel(num_steps,  batch_size, patch_size, depth, num_hidden,\\\n",
    "                report_interval_steps,  random_seed, gradient_optimizer, learning_rate):\n",
    "    print('patch size ', patch_size)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "      # Input data.\n",
    "      tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "      tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "      tf_valid_dataset = tf.constant(valid_dataset)\n",
    "      tf_test_dataset = tf.constant(test_dataset)\n",
    "              # Variables.\n",
    "      layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "      layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "      layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "      layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "      layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "          [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "      layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "      layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "          [num_hidden, num_labels], stddev=0.1))\n",
    "      layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "               # Model.\n",
    "      def model(data):       \n",
    "            print('Using  Vanilla With no max pool. Drop out not supported yet')\n",
    "            strides = [1, 2, 2, 1]   # A stride of sliding windows for each dimension. Here two dimension hence 4 stride param     \n",
    "            conv = tf.nn.conv2d(data, layer1_weights, strides, padding='SAME') # sweep a 2-D filter over a batch of images, with tf.nn.conv2d  \n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            conv = tf.nn.conv2d(hidden, layer2_weights, strides, padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer2_biases)\n",
    "            shape = hidden.get_shape().as_list()\n",
    "            reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "            return tf.matmul(hidden, layer4_weights) + layer4_biases        \n",
    "              # Training computation.\n",
    "      logits = model(tf_train_dataset)\n",
    "      loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))      \n",
    "                # Optimizer.\n",
    "      optimizer = GetOptimiser(tf, gradient_optimizer, loss, rate = learning_rate)\n",
    "      #optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "      #optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "              # Predictions for the training, validation, and test data.\n",
    "      train_prediction = tf.nn.softmax(logits)\n",
    "      valid_prediction = tf.nn.softmax( model(tf_valid_dataset))\n",
    "      test_prediction = tf.nn.softmax( model(tf_test_dataset))\n",
    "    #print('TensorFlow Graph Initialisation completed at ',datetime.datetime.now())\n",
    "    \n",
    "#def RunTfModel(graph, num_steps, batch_size, report_interval_steps = 50, random_seed = 12  ):        \n",
    "    ################\n",
    "    ### Run TF model\n",
    "    ###############\n",
    "    start_time = datetime.datetime.now()\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      #tf.global_variables_initializer().run()\n",
    "      tf.initialize_all_variables().run()\n",
    "      print('Initialized')\n",
    "      for step in range(num_steps):\n",
    "        random.seed(random_seed*step)    \n",
    "        offset = random.randint( 1, train_labels.shape[0] - batch_size )        \n",
    "        #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)        \n",
    "        if (step % report_interval_steps == 0):\n",
    "            print('Step %4d :: Minibatch ==> Loss: %f .Accuracy: %.1f%% .Validation Acc.: %.1f%% DataOffset:%7d'\\\n",
    "                    % (step, l, accuracy(predictions, batch_labels), accuracy(valid_prediction.eval(), valid_labels), offset ))          \n",
    "      print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))     \n",
    "      #tf.reset_default_graph()\n",
    "      session.close()    \n",
    "    tf.reset_default_graph()    \n",
    "    print('Total Time taken', datetime.datetime.now() - start_time)\n",
    "    \n",
    "def RunNN(num_steps = 101, batch_size = 16, patch_size = 5, depth = 16, num_hidden = 64 \\\n",
    "         , report_interval_steps = 50, random_seed = 12, gradient_optimizer = 'None'\\\n",
    "         , learning_rate = 0.0001):        \n",
    "                #def RunNN(num_steps , batch_size , patch_size , depth , num_hidden, report_interval_steps , random_seed ):            \n",
    "    InitNRunTfModel( num_steps = num_steps, batch_size = batch_size, patch_size = patch_size, \\\n",
    "            depth = depth, num_hidden = num_hidden , report_interval_steps = report_interval_steps,\\\n",
    "            random_seed = random_seed, gradient_optimizer= gradient_optimizer, learning_rate = learning_rate)\n",
    "    #RunTfModel(graph, num_steps = num_steps , batch_size = batch_size ,report_interval_steps = report_interval_steps, random_seed = random_seed  )\n",
    "    \n",
    "print('NN Function Init Completed', datetime.datetime.now()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch Size :  7 x 7 \n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch size  7\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "SELECTED OPTIMISER :: Gradient Descent optimisier with default rate - 0.05\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Initialized\n",
      "Step    0 :: Minibatch ==> Loss: 2.896104 .Accuracy: 6.0% .Validation Acc.: 13.8% DataOffset: 168800\n",
      "Step 1000 :: Minibatch ==> Loss: 0.650454 .Accuracy: 80.0% .Validation Acc.: 84.8% DataOffset:   7689\n",
      "Step 2000 :: Minibatch ==> Loss: 0.377741 .Accuracy: 87.0% .Validation Acc.: 87.0% DataOffset:  99859\n",
      "Step 3000 :: Minibatch ==> Loss: 0.385118 .Accuracy: 87.0% .Validation Acc.: 87.7% DataOffset: 183851\n",
      "Step 4000 :: Minibatch ==> Loss: 0.312888 .Accuracy: 91.0% .Validation Acc.: 88.4% DataOffset:  36536\n",
      "Step 5000 :: Minibatch ==> Loss: 0.456389 .Accuracy: 85.0% .Validation Acc.: 88.8% DataOffset: 129488\n",
      "Test accuracy: 94.3%\n",
      "Total Time taken 0:05:48.733186\n"
     ]
    }
   ],
   "source": [
    "RunNN(num_steps = 5001, batch_size = 100, patch_size = 7, depth = 16, num_hidden = 64\\\n",
    "     , report_interval_steps =1000, gradient_optimizer = 'GradientDescent' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch Size :  5 x 5 : Best\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch size  5\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "SELECTED OPTIMISER :: Gradient Descent optimisier with default rate - 0.05\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Initialized\n",
      "Step    0 :: Minibatch ==> Loss: 3.365601 .Accuracy: 14.0% .Validation Acc.: 9.9% DataOffset: 168800\n",
      "Step 1000 :: Minibatch ==> Loss: 0.607137 .Accuracy: 82.0% .Validation Acc.: 84.1% DataOffset:   7689\n",
      "Step 2000 :: Minibatch ==> Loss: 0.362161 .Accuracy: 87.0% .Validation Acc.: 85.9% DataOffset:  99859\n",
      "Step 3000 :: Minibatch ==> Loss: 0.415511 .Accuracy: 86.0% .Validation Acc.: 86.9% DataOffset: 183851\n",
      "Step 4000 :: Minibatch ==> Loss: 0.372233 .Accuracy: 90.0% .Validation Acc.: 87.7% DataOffset:  36536\n",
      "Step 5000 :: Minibatch ==> Loss: 0.427996 .Accuracy: 90.0% .Validation Acc.: 87.9% DataOffset: 129488\n",
      "Test accuracy: 94.2%\n",
      "Total Time taken 0:03:41.117379\n"
     ]
    }
   ],
   "source": [
    "RunNN(num_steps = 5001, batch_size = 100, patch_size = 5, depth = 16, num_hidden = 64\\\n",
    "     , report_interval_steps =1000, gradient_optimizer = 'GradientDescent' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch size  5\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "SELECTED OPTIMISER :: Gradient Descent optimisier with default rate - 0.05\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Initialized\n",
      "Step    0 :: Minibatch ==> Loss: 3.549231 .Accuracy: 12.5% .Validation Acc.: 10.0% DataOffset: 168871\n",
      "Step 1000 :: Minibatch ==> Loss: 0.294258 .Accuracy: 93.8% .Validation Acc.: 82.4% DataOffset:   7693\n",
      "Step 2000 :: Minibatch ==> Loss: 0.525900 .Accuracy: 81.2% .Validation Acc.: 84.6% DataOffset:  99901\n",
      "Step 3000 :: Minibatch ==> Loss: 0.569500 .Accuracy: 81.2% .Validation Acc.: 85.8% DataOffset: 183928\n",
      "Step 4000 :: Minibatch ==> Loss: 0.878385 .Accuracy: 75.0% .Validation Acc.: 86.3% DataOffset:  36552\n",
      "Step 5000 :: Minibatch ==> Loss: 0.354826 .Accuracy: 87.5% .Validation Acc.: 87.0% DataOffset: 129543\n",
      "Test accuracy: 93.6%\n",
      "Total Time taken 0:00:54.464790\n"
     ]
    }
   ],
   "source": [
    "RunNN(num_steps = 5001, batch_size = 16, patch_size = 5, depth = 16, num_hidden = 64\\\n",
    "     , report_interval_steps =1000, gradient_optimizer = 'GradientDescent' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smaller Patch Size :  3 x 3 \n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch size  3\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "SELECTED OPTIMISER :: Gradient Descent optimisier with default rate - 0.05\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Initialized\n",
      "Step    0 :: Minibatch ==> Loss: 3.445589 .Accuracy: 7.0% .Validation Acc.: 9.0% DataOffset: 168800\n",
      "Step 1000 :: Minibatch ==> Loss: 0.616753 .Accuracy: 83.0% .Validation Acc.: 81.3% DataOffset:   7689\n",
      "Step 2000 :: Minibatch ==> Loss: 0.424078 .Accuracy: 88.0% .Validation Acc.: 84.0% DataOffset:  99859\n",
      "Step 3000 :: Minibatch ==> Loss: 0.461060 .Accuracy: 86.0% .Validation Acc.: 84.7% DataOffset: 183851\n",
      "Step 4000 :: Minibatch ==> Loss: 0.364323 .Accuracy: 86.0% .Validation Acc.: 85.6% DataOffset:  36536\n",
      "Step 5000 :: Minibatch ==> Loss: 0.547264 .Accuracy: 82.0% .Validation Acc.: 85.9% DataOffset: 129488\n",
      "Test accuracy: 92.4%\n",
      "Total Time taken 0:02:27.722717\n"
     ]
    }
   ],
   "source": [
    "RunNN(num_steps = 5001, batch_size = 100, patch_size = 3, depth = 16, num_hidden = 64\\\n",
    "     , report_interval_steps =1000, gradient_optimizer = 'GradientDescent' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smallest Patch Size :  1 x 1 \n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch size  1\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "SELECTED OPTIMISER :: Gradient Descent optimisier with default rate - 0.05\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Initialized\n",
      "Step    0 :: Minibatch ==> Loss: 4.267948 .Accuracy: 12.0% .Validation Acc.: 10.0% DataOffset: 168800\n",
      "Step 1000 :: Minibatch ==> Loss: 2.302978 .Accuracy: 8.0% .Validation Acc.: 10.0% DataOffset:   7689\n",
      "Step 2000 :: Minibatch ==> Loss: 2.301145 .Accuracy: 14.0% .Validation Acc.: 10.0% DataOffset:  99859\n",
      "Step 3000 :: Minibatch ==> Loss: 2.304610 .Accuracy: 4.0% .Validation Acc.: 10.0% DataOffset: 183851\n",
      "Step 4000 :: Minibatch ==> Loss: 2.302458 .Accuracy: 11.0% .Validation Acc.: 10.0% DataOffset:  36536\n",
      "Step 5000 :: Minibatch ==> Loss: 2.303481 .Accuracy: 10.0% .Validation Acc.: 10.0% DataOffset: 129488\n",
      "Test accuracy: 10.0%\n",
      "Total Time taken 0:01:00.377971\n"
     ]
    }
   ],
   "source": [
    "RunNN(num_steps = 5001, batch_size = 100, patch_size = 1, depth = 16, num_hidden = 64\\\n",
    "     , report_interval_steps =1000 , gradient_optimizer = 'GradientDescent' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch Size :  5 x 5 + AdamOptimiser :  Best, new conf\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch size  5\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "SELECTED OPTIMISER :: Adam with learning rate -  0.001\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Using  Vanilla With no max pool. Drop out not supported yet\n",
      "Initialized\n",
      "Step    0 :: Minibatch ==> Loss: 4.727360 .Accuracy: 7.0% .Validation Acc.: 10.0% DataOffset: 168800\n",
      "Step 1000 :: Minibatch ==> Loss: 0.514220 .Accuracy: 86.0% .Validation Acc.: 85.9% DataOffset:   7689\n",
      "Step 2000 :: Minibatch ==> Loss: 0.280834 .Accuracy: 91.0% .Validation Acc.: 88.0% DataOffset:  99859\n",
      "Step 3000 :: Minibatch ==> Loss: 0.332831 .Accuracy: 90.0% .Validation Acc.: 88.8% DataOffset: 183851\n",
      "Step 4000 :: Minibatch ==> Loss: 0.266403 .Accuracy: 92.0% .Validation Acc.: 89.1% DataOffset:  36536\n",
      "Step 5000 :: Minibatch ==> Loss: 0.388152 .Accuracy: 89.0% .Validation Acc.: 90.1% DataOffset: 129488\n",
      "Test accuracy: 95.5%\n",
      "Total Time taken 0:03:28.837959\n"
     ]
    }
   ],
   "source": [
    "RunNN( num_steps = 5001, batch_size = 100, patch_size = 5, depth = 16, num_hidden = 64 \\\n",
    "            , report_interval_steps = 1000 , random_seed = 12, gradient_optimizer = 'Adam'\\\n",
    "      , learning_rate = 0.001 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1 : Max-Pool implementation : Min RAM : 4GB\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Function Init Completed 2017-03-28 22:20:30.701819\n"
     ]
    }
   ],
   "source": [
    "def GetOptimiser(tf, gradient_optimizer, loss, rate):\n",
    "    #print('gradient_optimizer is :: ',gradient_optimizer)\n",
    "    if gradient_optimizer == 'GradientDescent':\n",
    "        print('SELECTED OPTIMISER :: Gradient Descent optimisier with default rate - 0.05')\n",
    "        return tf.train.GradientDescentOptimizer(0.05).minimize(loss)        \n",
    "    elif  gradient_optimizer == 'Adam':\n",
    "        print('SELECTED OPTIMISER :: Adam with learning rate - ', rate)\n",
    "        return tf.train.AdamOptimizer(rate).minimize(loss)\n",
    "    else:\n",
    "        print('SELECTED OPTIMISER :: Invalid optimiser selected. Valid Optimiser : {GradientDescent | Adam}')\n",
    "        return False\n",
    "\n",
    "def InitNRunTfModel(num_steps,  batch_size, patch_size, depth, num_hidden,\\\n",
    "                report_interval_steps,  random_seed, gradient_optimizer, learning_rate, use_2x2_maxpool ):\n",
    "    print('patch size ', patch_size)\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "      # Input data.\n",
    "      tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "      tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "      tf_valid_dataset = tf.constant(valid_dataset)\n",
    "      tf_test_dataset = tf.constant(test_dataset)\n",
    "              # Variables.\n",
    "      layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "      layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "      layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "          [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "      layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "      layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "          [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "      layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "      layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "          [num_hidden, num_labels], stddev=0.1))\n",
    "      layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "      # We need placeholder because we need to turn dropout on  for training only and  imp. not for other validation and test set/\n",
    "      dropout_probability_train  = tf.placeholder(tf.float32)\n",
    "              # Model.\n",
    "      def model(data, use_2x2_maxpool, dropout_keep_probability = 1, defn = 'Train'):\n",
    "        if use_2x2_maxpool == 0:\n",
    "            print('Using  Vanilla With no max pool. Drop out not supported yet')\n",
    "            strides = [1, 2, 2, 1]   # A stride of sliding windows for each dimension. Here two dimension hence 4 stride param     \n",
    "            conv = tf.nn.conv2d(data, layer1_weights, strides, padding='SAME') # sweep a 2-D filter over a batch of images, with tf.nn.conv2d  \n",
    "            hidden = tf.nn.relu(conv + layer1_biases)\n",
    "            conv = tf.nn.conv2d(hidden, layer2_weights, strides, padding='SAME')\n",
    "            hidden = tf.nn.relu(conv + layer2_biases)\n",
    "            shape = hidden.get_shape().as_list()\n",
    "            reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "            return tf.matmul(hidden, layer4_weights) + layer4_biases      \n",
    "        else:\n",
    "            print('Using  2x2 max pool. Dropout suppported for '+defn+' at keep probability rate', dropout_keep_probability)      \n",
    "            strides = [1, 1, 1, 1]   # A stride of sliding windows for each dimension. Here two dimension hence 4 stride param     \n",
    "            #conv1 = tf.nn.conv2d(data, layer1_weights, strides = [1 ,1 ,1 , 1], padding='SAME') # sweep a 2-D filter over a batch of images, with tf.nn.conv2d  \n",
    "            #hidden1 = tf.nn.relu(tf.nn.conv2d(data, layer1_weights, strides = [1 ,1 ,1 , 1], padding='SAME')\\\n",
    "            #                     + layer1_biases)\n",
    "            #max_pool1 = tf.nn.max_pool(\\\n",
    "            #                ( tf.nn.relu(tf.nn.conv2d(data, layer1_weights, strides = [1 ,1 ,1 , 1], padding='SAME')\\\n",
    "            #                     + layer1_biases) )\\\n",
    "            #                           , ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME' )\n",
    "            #conv2 = tf.nn.conv2d(max_pool1, layer2_weights, strides = [1 ,1 ,1, 1] , padding='SAME')\n",
    "            #hidden2 = tf.nn.relu(tf.nn.conv2d(max_pool1, layer2_weights, strides = [1 ,1 ,1, 1] , padding='SAME')\\\n",
    "            #                     + layer2_biases)\n",
    "            #max_pool2 = tf.nn.max_pool(\\\n",
    "            #            ( tf.nn.relu(tf.nn.conv2d(max_pool1, layer2_weights, strides = [1 ,1 ,1, 1], padding='SAME')\\\n",
    "            #                     + layer2_biases) )\\\n",
    "            #                           , ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME' )\n",
    "            ############################ Single Max pool variable\n",
    "            max_pool2 = tf.nn.max_pool(( \\\n",
    "                   #Hidden Layer 2 \n",
    "                   tf.nn.relu(\\\n",
    "                     #Conv Layer 2                 \n",
    "                     tf.nn.conv2d(( \\\n",
    "                         #max pool layer 1\n",
    "                         tf.nn.max_pool(\\\n",
    "                             #hidden_layer 1\n",
    "                             ( tf.nn.relu(\\\n",
    "                                  #Conv Layer 1        \n",
    "                                  tf.nn.conv2d(data, layer1_weights, strides = [1 ,1 ,1 , 1], padding='SAME')\\\n",
    "                                  #Conv Layer 1 end\n",
    "                              + layer1_biases) )\\\n",
    "                              #hidden_layer 1 end                                        \n",
    "                         , ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME' )\\\n",
    "                         #max pool layer 1 end\n",
    "                      ), layer2_weights, strides = [1 ,1 ,1, 1], padding='SAME')\\\n",
    "                      #Conv Layer 2 end\n",
    "                   + layer2_biases) )\\\n",
    "                   ##Hidden Layer 2  end\n",
    "              , ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME' )\n",
    "              # MAx Pool2 end          \n",
    "            \n",
    "            shape = max_pool2.get_shape().as_list()\n",
    "            #reshape = tf.reshape(max_pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            #hidden3 = tf.nn.relu(tf.matmul(\\\n",
    "            #                         ( tf.reshape(max_pool2, [shape[0], shape[1] * shape[2] * shape[3]]) )\\\n",
    "            #                               , layer3_weights) + layer3_biases)\n",
    "            return tf.matmul(\\\n",
    "                          # drop-out \n",
    "                          tf.nn.dropout(\\\n",
    "                             #hidden layer 3\n",
    "                             ( tf.nn.relu(tf.matmul(\\\n",
    "                                     ( tf.reshape(max_pool2, [shape[0], shape[1] * shape[2] * shape[3]]) )\\\n",
    "                                           , layer3_weights) + layer3_biases) )\\\n",
    "                             # hidden layer 3 end\n",
    "                           , dropout_keep_probability)\\\n",
    "                           # drop-out end             \n",
    "                    , layer4_weights) + layer4_biases\n",
    "\n",
    "              # Training computation.\n",
    "      logits = model(tf_train_dataset, use_2x2_maxpool, 0.5, 'Training Data')\n",
    "      loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))      \n",
    "                # Optimizer.\n",
    "      optimizer = GetOptimiser(tf, gradient_optimizer, loss, rate = learning_rate)\n",
    "      #optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "      #optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "              # Predictions for the training, validation, and test data.\n",
    "      train_prediction = tf.nn.softmax(logits)\n",
    "      valid_prediction = tf.nn.softmax( model(tf_valid_dataset, use_2x2_maxpool, 1, 'Validation Data' ))\n",
    "      test_prediction = tf.nn.softmax( model(tf_test_dataset, use_2x2_maxpool, 1, 'Test Data'))\n",
    "    #print('TensorFlow Graph Initialisation completed at ',datetime.datetime.now())\n",
    "    \n",
    "#def RunTfModel(graph, num_steps, batch_size, report_interval_steps = 50, random_seed = 12  ):        \n",
    "    ################\n",
    "    ### Run TF model\n",
    "    ###############\n",
    "    start_time = datetime.datetime.now()\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      #tf.global_variables_initializer().run()\n",
    "      tf.initialize_all_variables().run()\n",
    "      print('Initialized')\n",
    "      for step in range(num_steps):\n",
    "        random.seed(random_seed*step)    \n",
    "        offset = random.randint( 1, train_labels.shape[0] - batch_size )        \n",
    "        #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)        \n",
    "        if (step % report_interval_steps == 0):\n",
    "            print('Step %4d :: Minibatch ==> Loss: %f .Accuracy: %.1f%% .Validation Acc.: %.1f%% DataOffset:%7d'\\\n",
    "                    % (step, l, accuracy(predictions, batch_labels), accuracy(valid_prediction.eval(), valid_labels), offset ))          \n",
    "      print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))     \n",
    "      #tf.reset_default_graph()\n",
    "      session.close()    \n",
    "    tf.reset_default_graph()    \n",
    "    print('Total Time taken', datetime.datetime.now() - start_time)\n",
    "    \n",
    "def RunNN(num_steps = 101, batch_size = 16, patch_size = 5, depth = 16, num_hidden = 64 \\\n",
    "         , report_interval_steps = 50, random_seed = 12, gradient_optimizer = 'None'\\\n",
    "         , learning_rate = 0.0001, use_2x2_maxpool = 0 ):        \n",
    "                #def RunNN(num_steps , batch_size , patch_size , depth , num_hidden, report_interval_steps , random_seed ):            \n",
    "    InitNRunTfModel( num_steps = num_steps, batch_size = batch_size, patch_size = patch_size, \\\n",
    "            depth = depth, num_hidden = num_hidden , report_interval_steps = report_interval_steps,\\\n",
    "            random_seed = random_seed, gradient_optimizer= gradient_optimizer, learning_rate = learning_rate\\\n",
    "            , use_2x2_maxpool = use_2x2_maxpool)\n",
    "    #RunTfModel(graph, num_steps = num_steps , batch_size = batch_size ,report_interval_steps = report_interval_steps, random_seed = random_seed  )\n",
    "    \n",
    "print('NN Function Init Completed', datetime.datetime.now())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch : 5 x 5 + MaxPool\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch size  5\n",
      "Using  2x2 max pool. Dropout suppported for Train at keep probability rate 0.5\n",
      "SELECTED OPTIMISER :: Adam with learning rate -  0.01\n",
      "Using  2x2 max pool. Dropout suppported for Valid at keep probability rate 1\n",
      "Using  2x2 max pool. Dropout suppported for Test at keep probability rate 1\n",
      "Initialized\n",
      "Step    0 :: Minibatch ==> Loss: 4.404894 .Accuracy: 18.8% .Validation Acc.: 10.0% DataOffset: 168871\n",
      "Step 1000 :: Minibatch ==> Loss: 0.596764 .Accuracy: 81.2% .Validation Acc.: 80.6% DataOffset:   7693\n",
      "Step 2000 :: Minibatch ==> Loss: 0.552265 .Accuracy: 81.2% .Validation Acc.: 80.8% DataOffset:  99901\n",
      "Step 3000 :: Minibatch ==> Loss: 2.019364 .Accuracy: 62.5% .Validation Acc.: 73.4% DataOffset: 183928\n",
      "Step 4000 :: Minibatch ==> Loss: 0.937754 .Accuracy: 68.8% .Validation Acc.: 80.7% DataOffset:  36552\n",
      "Step 5000 :: Minibatch ==> Loss: 1.022063 .Accuracy: 68.8% .Validation Acc.: 81.7% DataOffset: 129543\n",
      "Test accuracy: 89.0%\n",
      "Total Time taken 0:08:10.221837\n"
     ]
    }
   ],
   "source": [
    "RunNN( num_steps = 5001, batch_size = 16, patch_size = 5, depth = 16, num_hidden = 64 \\\n",
    "            , report_interval_steps = 1000 , random_seed = 12, gradient_optimizer = 'Adam'\\\n",
    "      , learning_rate = 0.01, use_2x2_maxpool = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 2 : Drop Out \n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch size  5\n",
      "Using  2x2 max pool. Dropout suppported for train at rate 0.5\n",
      "SELECTED OPTIMISER :: Adam with learning rate -  0.001\n",
      "Using  2x2 max pool. Dropout suppported for train at rate 1\n",
      "Using  2x2 max pool. Dropout suppported for train at rate 1\n",
      "Initialized\n",
      "Step    0 :: Minibatch ==> Loss: 5.931297 .Accuracy: 12.0% .Validation Acc.: 10.0% DataOffset: 168800\n",
      "Step 1000 :: Minibatch ==> Loss: 0.716753 .Accuracy: 77.0% .Validation Acc.: 85.2% DataOffset:   7689\n",
      "Step 2000 :: Minibatch ==> Loss: 0.470212 .Accuracy: 87.0% .Validation Acc.: 87.2% DataOffset:  99859\n",
      "Step 3000 :: Minibatch ==> Loss: 0.519879 .Accuracy: 83.0% .Validation Acc.: 87.6% DataOffset: 183851\n",
      "Step 4000 :: Minibatch ==> Loss: 0.421558 .Accuracy: 85.0% .Validation Acc.: 88.3% DataOffset:  36536\n",
      "Step 5000 :: Minibatch ==> Loss: 0.564555 .Accuracy: 85.0% .Validation Acc.: 88.7% DataOffset: 129488\n",
      "Test accuracy: 94.9%\n",
      "Total Time taken 0:17:33.244888\n"
     ]
    }
   ],
   "source": [
    "RunNN( num_steps = 5001, batch_size = 100, patch_size = 5, depth = 16, num_hidden = 64 \\\n",
    "            , report_interval_steps = 1000 , random_seed = 12, gradient_optimizer = 'Adam'\\\n",
    "      , learning_rate = 0.001, use_2x2_maxpool = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misc :\n",
    "--------------------------\n",
    "**Understanding strides parameter in Conv2d**\n",
    "strides = [1, 2, 2, 1]  \n",
    "- A stride of sliding windows for each dimension. Here two dimension hence 4 stride param   \n",
    "\n",
    "- strides = [batch, height, width, channel].\n",
    "\n",
    "with [1, 2 , 2 , 1] we are saying that we dont want to slide across batch and channel and that we want to slide by 2 when it comes to height and width i.e\n",
    "\n",
    "10 11 12 13 14 ...\n",
    "\n",
    "20 21 22 23 24 ...\n",
    "\n",
    "30 31 32 33 34 ...\n",
    "\n",
    "\n",
    "| Stride  (patch-2 x2)  | [1, 1, 1, 1]  | [1, 2, 2, 1]  |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| Filter1       | 10 11         | 10 11 |\n",
    "|               | 20 21         | 20 21 |\n",
    "| Filter2       | 11 12         | 12 13 |\n",
    "|               | 21 22         | 22 23 |            \n",
    "\n",
    "To understand the code more and better visit, https://bikalblogs.wordpress.com/2017/02/10/tensorflow-convolution-nets-and-params/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

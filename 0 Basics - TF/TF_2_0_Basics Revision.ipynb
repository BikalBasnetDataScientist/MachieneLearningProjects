{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF 2.0 -  Basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKqvIr+YxrX9H27R7osTI3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beekal/MachieneLearningProjects/blob/master/0%20Basics%20-%20TF/TF_2_0_Basics%20Revision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlTyY3IsvEdO",
        "colab_type": "text"
      },
      "source": [
        "# TF 2.0 Topics Covered\n",
        "1. TF.Data : A single point of entry to handle any/ varied data type ranging from pandas, csv, image, text, TfRecordByte e.t.c\n",
        "2. TFX: Tensorflow extended,  which aims to  provided end to end TF ecosystem from its  research/ prototype to  the server deployment. It includes the following  all of  which we will cover in this notebook\n",
        "  -  TF validation : Used to validate the data\n",
        "  -  TF Transform : Used to transform the data to its modeling state\n",
        "  -  TF Modeling / Analysis : Used to create the model and evaluate its performance. Reiterate until satisfactory Metric reached.\n",
        "  - TF Serving: Used to serve the model to production with versioning/ Rollback capabilities.\n",
        "\n",
        "REF: https://www.tensorflow.org/tfx "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YO0K90gu610",
        "colab_type": "code",
        "outputId": "fabf7e0d-7ca2-4ea3-a152-836aae3987d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "\n",
        "print(f'TF version : ', tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version :  2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyqkv1Vt0B7x",
        "colab_type": "text"
      },
      "source": [
        "## TF.Data\n",
        "Depending on the data source you wil have to use different TF data load calls\n",
        "  - Data in memory :\n",
        "      - tf.data.Dataset.from_tensors() or\n",
        "      - tf.data.Dataset.from_tensor_slices() : For data in memory\n",
        "      - tf.data.TFRecordDataset(): For Large Files\n",
        "      - tf.data.TextLineDataset() : For text File\n",
        "      - From CSV : Supports lazy incremental data load\n",
        "      - From DataFrame : Loads all data in mem\n",
        "\n",
        "1. **TFRecordDataset** : If we want to read the data efficiently or are dealing with the large files, it is best to use the TFRecordDataSet. It stores the data in the binary format of smaller size(100-200 MB chunk) and hence is faster / easier to read. To know more on [How to create a TFRecordDataset click here](www.tensorflow.org/tutorials/load_data/tfrecord)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMSgOjW3zexi",
        "colab_type": "code",
        "outputId": "f781d7b4-d9fc-4c2f-cd6a-ef255bd1bc53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        }
      },
      "source": [
        "def hprint(text):\n",
        "  print(\"\\n\"+\"=\"*80+\"\\n\\t\\t\"+text+\"\\n\"+\"=\"*80)\n",
        "\n",
        "hprint('READING FROM MEMORY')\n",
        "d_mem = tf.data.Dataset.from_tensor_slices([1,2,3])\n",
        "print(f' Tf.Data from memory : ' ,[e.numpy() for e in d_mem])\n",
        "it = iter(d_mem)\n",
        "print(f' Tf.Data from memory using iterator : ' ,next(it).numpy())\n",
        "\n",
        "# French street name sign FRecordDataset\n",
        "hprint('READING FROM TF RECORD DATASET')\n",
        "fsns_file = tf.keras.utils.get_file(\"fsns.tfrec\", \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")\n",
        "fsns_tfrecord = tf.data.TFRecordDataset(filenames= [fsns_file])\n",
        "print(fsns_tfrecord)\n",
        "byte_example = next(iter(fsns_tfrecord))\n",
        "# Displays all the example data\n",
        "# print(tf.train.Example.FromString(byte_example.numpy()) )\n",
        "print(tf.train.Example.FromString(byte_example.numpy()).features.feature['image/text'] ) \n",
        "\n",
        "hprint('READING TEXT FILE')\n",
        "dir = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "file_names = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "file_paths = [tf.keras.utils.get_file(f, dir+f) for f in file_names ]\n",
        "f_dataset =   tf.data.TextLineDataset(file_paths)\n",
        "for line in f_dataset.take(3):\n",
        "  print('First line of each file  :', line.numpy() )\n",
        "# To learn how to Shuffle Text lines between files, see https://www.tensorflow.org/guide/data#consuming_text_data\n",
        "\n",
        "hprint('READING FROM DATAFRAME')\n",
        "import pandas as pd\n",
        "titanic_csv = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
        "df =  pd.read_csv(titanic_csv, index_col=None)\n",
        "t_dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n",
        "for feat in t_dataset.take(1):\n",
        "  for col_name, val in feat.items():\n",
        "    print(\"{:20s}: {}\".format(col_name, val))\n",
        "\n",
        "hprint('READING FROM CSV')\n",
        "t_batch = tf.data.experimental.make_csv_dataset(\\\n",
        "                titanic_csv, batch_size=2, label_name=\"survived\", \\\n",
        "                select_columns=['sex','age', 'class', 'fare', 'survived'] )\n",
        "for feat, label in t_batch.take(1):\n",
        "  print(f'Label (Survived) :', label)\n",
        "  for col_name, val in feat.items():\n",
        "    print(\"{:20s} : {}\".format(col_name, val))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "\t\tREADING FROM MEMORY\n",
            "================================================================================\n",
            " Tf.Data from memory :  [1, 2, 3]\n",
            " Tf.Data from memory using iterator :  1\n",
            "\n",
            "================================================================================\n",
            "\t\tREADING FROM TF RECORD DATASET\n",
            "================================================================================\n",
            "<TFRecordDatasetV2 shapes: (), types: tf.string>\n",
            "bytes_list {\n",
            "  value: \"Rue Perreyon\"\n",
            "}\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\t\tREADING TEXT FILE\n",
            "================================================================================\n",
            "First line of each file  : b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\"\n",
            "First line of each file  : b'His wrath pernicious, who ten thousand woes'\n",
            "First line of each file  : b\"Caused to Achaia's host, sent many a soul\"\n",
            "\n",
            "================================================================================\n",
            "\t\tREADING FROM DATAFRAME\n",
            "================================================================================\n",
            "survived            : 0\n",
            "sex                 : b'male'\n",
            "age                 : 22.0\n",
            "n_siblings_spouses  : 1\n",
            "parch               : 0\n",
            "fare                : 7.25\n",
            "class               : b'Third'\n",
            "deck                : b'unknown'\n",
            "embark_town         : b'Southampton'\n",
            "alone               : b'n'\n",
            "\n",
            "================================================================================\n",
            "\t\tREADING FROM CSV\n",
            "================================================================================\n",
            "Label (Survived) : tf.Tensor([0 1], shape=(2,), dtype=int32)\n",
            "sex                  : [b'male' b'female']\n",
            "age                  : [36.  4.]\n",
            "fare                 : [10.5    13.4167]\n",
            "class                : [b'Second' b'Third']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uJkc7P2DQTh",
        "colab_type": "text"
      },
      "source": [
        "### TF.Data: Generator\n",
        "  - **Read From Mixed datatype + Lazy Data Loading**\n",
        "\n",
        "  - Use generator to convert each elements into a tf.Data Dataset \n",
        "  - Example : Load image data with generator. i.e We do not need to read 5+ GB of data and fill it up in  memory / waste memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdCgN8jbDK_X",
        "colab_type": "code",
        "outputId": "d6549095-0f40-4789-e9ca-b2617ae60762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "person_data=  ([{ 'age':18,'name':'Hary' },\n",
        "              { 'age':30,'name':'Sam' }\n",
        "              ])\n",
        "person_data\n",
        "person =  tf.data.Dataset.from_generator( lambda: person_data, {\"age\": tf.int32, \"name\":tf.string}  )\n",
        "print(person)\n",
        "print(list(person.as_numpy_iterator()))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<FlatMapDataset shapes: {age: <unknown>, name: <unknown>}, types: {age: tf.int32, name: tf.string}>\n",
            "[{'age': 18, 'name': b'Hary'}, {'age': 30, 'name': b'Sam'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybu8cCrS5IN8",
        "colab_type": "text"
      },
      "source": [
        "## TF Transform :\n",
        "We can apply different type of transformations as per our need\n",
        "  - Dataset.map() : Per-Element operations\n",
        "  - Dataset.filter(): Per-element Filter Operation\n",
        "  - Dataset.reduce(): Reduce transfomations to single scalar value\n",
        "  - Dataset.batch(): Per-batch operations\n",
        "  - Dataset.shuffle(): Shuffle data\n",
        "\n",
        "### Dataset.map: Parallelisation/ SpeedUp\n",
        "If we want to speed up the dataset.map then we must specify the num_parallel_calls. \n",
        "  - Not defined : Process sequentially\n",
        "  - tf.data.experimental.AUTOTUNE : parallel calls set based on available CPU \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoPnPgkk1XaH",
        "colab_type": "code",
        "outputId": "32a363d9-ad67-4de7-fafa-518b76f702b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "def f(x):\n",
        "  return x+2\n",
        "\n",
        "print('Dataset.map(): All elements Increment by 1')\n",
        "print(list(d_mem.map(lambda x: x+1, num_parallel_calls=2).as_numpy_iterator() ))\n",
        "print(f'Using function ',list(d_mem.map(lambda x: f(x), num_parallel_calls=2).as_numpy_iterator() ))\n",
        "\n",
        "\n",
        "print('\\nDataset.reduce():  With initial intitial_state or starting_val')\n",
        "print(f'=10 ', d_mem.reduce( 10,  lambda x,y: x+y).numpy() )\n",
        "print(f'=0 ', d_mem.reduce( 0,  lambda x,y: x+y).numpy() )\n",
        "\n",
        "print('\\nDataset.shuffle()')\n",
        "d_mem1 = tf.data.Dataset.from_tensor_slices([1,2,3,4,5,6,7,8,9]) \n",
        "print(f'Pre-Shuffle ', list(d_mem1.as_numpy_iterator() ))\n",
        "print(f'Post-Shuffle', list(d_mem1.shuffle(buffer_size=2).batch(2).as_numpy_iterator() ))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset.map(): All elements Increment by 1\n",
            "[2, 3, 4]\n",
            "Using function  [3, 4, 5]\n",
            "\n",
            "Dataset.reduce():  With initial intitial_state or starting_val\n",
            "=10  16\n",
            "=0  6\n",
            "\n",
            "Dataset.shuffle()\n",
            "Pre-Shuffle  [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Post-Shuffle [array([1, 3], dtype=int32), array([4, 5], dtype=int32), array([6, 2], dtype=int32), array([7, 9], dtype=int32), array([8], dtype=int32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzy20k7Qli08",
        "colab_type": "text"
      },
      "source": [
        "## TF Transform : Using Python function\n",
        "If you want to use the python function inside teh tensorflow pipeline, you can do so in two ways\n",
        "  - Autograph: Use the function directly in the TF graphs. \n",
        "    - Pro: Easy to use\n",
        "    - Con: Can covert some but not all python codes\n",
        "  - tf.py_function : Define the python function as tf function during its use, indicating that it needs to converted into the tf code\n",
        "    - Pro: Can write  arbitary code and will be supported by TF pipeline without throwing any error\n",
        "    - Con: Generally results in worse performance\n",
        "      - Not parallelised"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFXOuPfM-WoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upper1(x: tf.Tensor):\n",
        "  return x+10\n",
        "  \n",
        "person_modf = person.map( lambda x : tf.py_function( func=upper1, inp=[x['age']], Tout=tf.int32) )\n",
        "print(list(person_modf.as_numpy_iterator()) )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXK-NL7JykI5",
        "colab_type": "text"
      },
      "source": [
        "## TF.py_function vs TF.distribute.Server\n",
        "The tf.py_function must run in the same address as the python program including the device, hence if you are using the distributed Tensorflow you must  use the tf.distribute.Server instead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miCa06Vo4j6v",
        "colab_type": "text"
      },
      "source": [
        "## TF.Batch :\n",
        "Create dataset batch to be used in the training\n",
        "**[Caution]** : The batch sizes can be uneven because equal size batching may not be possible. e.g break 3 element into batch of 2. It may create warning/ errors due to different size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E01bsmEr5EmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Uneven batch size :', list( d_mem.batch(2).as_numpy_iterator() ) )\n",
        "print(f'\\nEven batch size by dropping remainder :', \\\n",
        "      list( d_mem.batch(2, drop_remainder=True).as_numpy_iterator() ) )\n",
        "\n",
        "print(f'\\nEven batch size with padding :', \\\n",
        "      list( d_mem.batch(2).padded_batch(2, padded_shapes=[None]).as_numpy_iterator() ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ndzKMf-DbR7",
        "colab_type": "text"
      },
      "source": [
        "## Optimisation\n",
        "There are couple of things that can be done to speed up the.\n",
        "  - prefetch :  If you are reading the data off the disk, then often the latency is involved in disk reads. You can avoid this latency and hence faster training by prefetching  some batch of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FarGRa3bGmSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Orig d_mem :', list( d_mem.as_numpy_iterator() ))\n",
        "# This one fetches all because we have only one  list in the dataset\n",
        "print('Prefetches all ', list( d_mem.prefetch(1).as_numpy_iterator() ))\n",
        "print('', list( d_mem.batch(1).prefetch(1).as_numpy_iterator() ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0aWMhvwnfHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(person.map(lambda d: (d['age'] , d['name'])))\n",
        "# print(list(person.map(lambda d: (d['age'] , str(d['name'].numpy()).upper())).as_numpy_iterator() ))\n",
        "# print(person.map( lambda x : upper(x) ).as_numpy_iterator()) \n",
        "# print(person.map( lambda x : upper(x) )) \n",
        "# print(list(person.as_numpy_iterator()))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF 2.0 -  Basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4Kmg9lhfl57PQUjFPQZD4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beekal/MachieneLearningProjects/blob/master/0%20Basics%20-%20TF/TF_2_0_Basics%20Revision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlTyY3IsvEdO",
        "colab_type": "text"
      },
      "source": [
        "# TF 2.0 Topics Covered\n",
        "1. TF.Data : A single point of entry to handle any/ varied data type ranging from pandas, csv, image, text, TfRecordByte e.t.c\n",
        "2. TFX: Tensorflow extended,  which aims to  provided end to end TF ecosystem from its  research/ prototype to  the server deployment. It includes the following  all of  which we will cover in this notebook\n",
        "  -  TF validation : Used to validate the data\n",
        "  -  TF Transform : Used to transform the data to its modeling state\n",
        "  -  TF Modeling / Analysis : Used to create the model and evaluate its performance. Reiterate until satisfactory Metric reached.\n",
        "  - TF Serving: Used to serve the model to production with versioning/ Rollback capabilities.\n",
        "\n",
        "REF: https://www.tensorflow.org/tfx "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YO0K90gu610",
        "colab_type": "code",
        "outputId": "48c85b46-cbd9-4bfd-c64d-0db310145151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "\n",
        "print(f'TF version : ', tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version :  2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyqkv1Vt0B7x",
        "colab_type": "text"
      },
      "source": [
        "## TF.Data\n",
        "Depending on the data source you wil have to use different TF data load calls\n",
        "  - Data in memory :\n",
        "      - tf.data.Dataset.from_tensors() or\n",
        "      - tf.data.Dataset.from_tensor_slices()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMSgOjW3zexi",
        "colab_type": "code",
        "outputId": "a61bd29d-af86-4a47-8d62-caf113f5a073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "d_mem = tf.data.Dataset.from_tensor_slices([1,2,3])\n",
        "print(f' Tf.Data from memory : ' ,[e.numpy() for e in d_mem])\n",
        "\n",
        "it = iter(d_mem)\n",
        "print(f' Tf.Data from memory using iterator : ' ,next(it).numpy())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Tf.Data from memory :  [1, 2, 3]\n",
            " Tf.Data from memory using iterator :  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uJkc7P2DQTh",
        "colab_type": "text"
      },
      "source": [
        "### TF.Data: From Mixed datatype\n",
        "  - Use generator to convert each elements into a tf.Data Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdCgN8jbDK_X",
        "colab_type": "code",
        "outputId": "5f9bad11-278a-460c-ee21-3c9bfccece38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "person_data=  ([{ 'age':18,'name':'Hary' },\n",
        "              { 'age':30,'name':'Sam' }\n",
        "              ])\n",
        "person_data\n",
        "person =  tf.data.Dataset.from_generator( lambda: person_data, {\"age\": tf.int32, \"name\":tf.string}  )\n",
        "print(person)\n",
        "print(list(person.as_numpy_iterator()))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<FlatMapDataset shapes: {age: <unknown>, name: <unknown>}, types: {age: tf.int32, name: tf.string}>\n",
            "[{'age': 18, 'name': b'Hary'}, {'age': 30, 'name': b'Sam'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybu8cCrS5IN8",
        "colab_type": "text"
      },
      "source": [
        "## TF Transform :\n",
        "We can apply different type of transformations as per our need\n",
        "  - Dataset.map() : Per-Element operations\n",
        "  - Dataset.filter(): Per-element Filter Operation\n",
        "  - Dataset.reduce(): Reduce transfomations to single scalar value\n",
        "  - Dataset.batch(): Per-batch operations\n",
        "\n",
        "### Dataset.map: Parallelisation/ SpeedUp\n",
        "If we want to speed up the dataset.map then we must specify the num_parallel_calls. \n",
        "  - Not defined : Process sequentially\n",
        "  - tf.data.experimental.AUTOTUNE : parallel calls set based on available CPU \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoPnPgkk1XaH",
        "colab_type": "code",
        "outputId": "b7bedb6d-b831-4be4-b5d9-db45340ee766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "def f(x):\n",
        "  return x+2\n",
        "\n",
        "print('Dataset.map(): All elements Increment by 1')\n",
        "print(list(d_mem.map(lambda x: x+1, num_parallel_calls=2).as_numpy_iterator() ))\n",
        "print(f'Using function ',list(d_mem.map(lambda x: f(x), num_parallel_calls=2).as_numpy_iterator() ))\n",
        "\n",
        "\n",
        "print('\\nDataset.reduce():  With initial intitial_state or starting_val')\n",
        "print(f'=10 ', d_mem.reduce( 10,  lambda x,y: x+y).numpy() )\n",
        "print(f'=0 ', d_mem.reduce( 0,  lambda x,y: x+y).numpy() )"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset.map(): All elements Increment by 1\n",
            "[2, 3, 4]\n",
            "Using function  [3, 4, 5]\n",
            "\n",
            "Dataset.reduce():  With initial intitial_state or starting_val\n",
            "=10  16\n",
            "=0  6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzy20k7Qli08",
        "colab_type": "text"
      },
      "source": [
        "## TF Transform : Using Python function\n",
        "If you want to use the python function inside teh tensorflow pipeline, you can do so in two ways\n",
        "  - Autograph: Use the function directly in the TF graphs. \n",
        "    - Pro: Easy to use\n",
        "    - Con: Can covert some but not all python codes\n",
        "  - tf.py_function : Define the python function as tf function during its use, indicating that it needs to converted into the tf code\n",
        "    - Pro: Can write  arbitary code and will be supported by TF pipeline without throwing any error\n",
        "    - Con: Generally results in worse performance\n",
        "      - Not parallelised"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFXOuPfM-WoP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7b272e7c-e496-4290-dbcb-ea734cdb3c7e"
      },
      "source": [
        "def upper1(x: tf.Tensor):\n",
        "  return x+10\n",
        "  \n",
        "person_modf = person.map( lambda x : tf.py_function( func=upper1, inp=[x['age']], Tout=tf.int32) )\n",
        "print(list(person_modf.as_numpy_iterator()) )\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[28, 40]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXK-NL7JykI5",
        "colab_type": "text"
      },
      "source": [
        "## TF.py_function vs TF.distribute.Server\n",
        "The tf.py_function must run in the same address as the python program including the device, hence if you are using the distributed Tensorflow you must  use the tf.distribute.Server instead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miCa06Vo4j6v",
        "colab_type": "text"
      },
      "source": [
        "## TF.Batch :\n",
        "Create dataset batch to be used in the training\n",
        "**[Caution]** : The batch sizes can be uneven because equal size batching may not be possible. e.g break 3 element into batch of 2. It may create warning/ errors due to different size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E01bsmEr5EmX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "ce1fdb57-6c1f-48b7-b5c5-034f4734b2d5"
      },
      "source": [
        "print(f'Uneven batch size :', list( d_mem.batch(2).as_numpy_iterator() ) )\n",
        "print(f'\\nEven batch size by dropping remainder :', \\\n",
        "      list( d_mem.batch(2, drop_remainder=True).as_numpy_iterator() ) )\n",
        "\n",
        "print(f'\\nEven batch size with padding :', \\\n",
        "      list( d_mem.batch(2).padded_batch(2, padded_shapes=[None]).as_numpy_iterator() ) )"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uneven batch size : [array([1, 2], dtype=int32), array([3], dtype=int32)]\n",
            "\n",
            "Even batch size by dropping remainder : [array([1, 2], dtype=int32)]\n",
            "\n",
            "Even batch size with padding : [array([[1, 2],\n",
            "       [3, 0]], dtype=int32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ndzKMf-DbR7",
        "colab_type": "text"
      },
      "source": [
        "## Optimisation\n",
        "There are couple of things that can be done to speed up the.\n",
        "  - prefetch :  If you are reading the data off the disk, then often the latency is involved in disk reads. You can avoid this latency and hence faster training by prefetching  some batch of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FarGRa3bGmSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b5b773eb-1da6-4c63-c7f1-a12cdecb05de"
      },
      "source": [
        "print('Orig d_mem :', list( d_mem.as_numpy_iterator() ))\n",
        "# This one fetches all because we have only one  list in the dataset\n",
        "print('Prefetches all ', list( d_mem.prefetch(1).as_numpy_iterator() ))\n",
        "print('', list( d_mem.batch(1).prefetch(1).as_numpy_iterator() ))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Orig d_mem : [1, 2, 3]\n",
            "Prefetches all  [1, 2, 3]\n",
            " [array([1], dtype=int32), array([2], dtype=int32), array([3], dtype=int32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0aWMhvwnfHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(person.map(lambda d: (d['age'] , d['name'])))\n",
        "# print(list(person.map(lambda d: (d['age'] , str(d['name'].numpy()).upper())).as_numpy_iterator() ))\n",
        "# print(person.map( lambda x : upper(x) ).as_numpy_iterator()) \n",
        "# print(person.map( lambda x : upper(x) )) \n",
        "# print(list(person.as_numpy_iterator()))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
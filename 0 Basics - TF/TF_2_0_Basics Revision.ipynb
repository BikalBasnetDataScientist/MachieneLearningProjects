{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF 2.0 -  Basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPITAK8mCqH0X/iidsfbWJF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beekal/MachieneLearningProjects/blob/master/0%20Basics%20-%20TF/TF_2_0_Basics%20Revision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlTyY3IsvEdO",
        "colab_type": "text"
      },
      "source": [
        "# TF 2.0 Topics Covered\n",
        "1. TF.Data : A single point of entry to handle any/ varied data type ranging from pandas, csv, image, text, TfRecordByte e.t.c\n",
        "2. TFX: Tensorflow extended,  which aims to  provided end to end TF ecosystem from its  research/ prototype to  the server deployment. It includes the following  all of  which we will cover in this notebook\n",
        "  -  TF validation : Used to validate the data\n",
        "  -  TF Transform : Used to transform the data to its modeling state\n",
        "  -  TF Modeling / Analysis : Used to create the model and evaluate its performance. Reiterate until satisfactory Metric reached.\n",
        "  - TF Serving: Used to serve the model to production with versioning/ Rollback capabilities.\n",
        "\n",
        "REF: https://www.tensorflow.org/tfx "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YO0K90gu610",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "28d8b3cb-ed00-4b6f-f5da-82c74f863b7d"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "\n",
        "print(f'TF version : ', tf.__version__)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version :  2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyqkv1Vt0B7x",
        "colab_type": "text"
      },
      "source": [
        "## TF.Data\n",
        "Depending on the data source you wil have to use different TF data load calls\n",
        "  - Data in memory :\n",
        "      - tf.data.Dataset.from_tensors() or\n",
        "      - tf.data.Dataset.from_tensor_slices()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMSgOjW3zexi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2caf2655-6cfc-4bd7-ce9a-2090087007fa"
      },
      "source": [
        "d_mem = tf.data.Dataset.from_tensor_slices([1,2])\n",
        "print(f' Tf.Data from memory : ' ,[e.numpy() for e in d_mem])\n",
        "\n",
        "it = iter(d_mem)\n",
        "print(f' Tf.Data from memory using iterator : ' ,next(it).numpy())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Tf.Data from memory :  [1, 2]\n",
            " Tf.Data from memory using iterator :  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uJkc7P2DQTh",
        "colab_type": "text"
      },
      "source": [
        "### TF.Data: From Mixed datatype\n",
        "  - Use generator to convert each elements into a tf.Data Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdCgN8jbDK_X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a3f3100e-c10c-4ee2-e198-72dbf9fc9668"
      },
      "source": [
        "person_data=  ([{ 'age':18,'name':'Hary' },\n",
        "              { 'age':30,'name':'Sam' }\n",
        "              ])\n",
        "person_data\n",
        "person =  tf.data.Dataset.from_generator( lambda: person_data, {\"age\": tf.int32, \"name\":tf.string}  )\n",
        "print(person)\n",
        "print(list(person.as_numpy_iterator()))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<FlatMapDataset shapes: {age: <unknown>, name: <unknown>}, types: {age: tf.int32, name: tf.string}>\n",
            "[{'age': 18, 'name': b'Hary'}, {'age': 30, 'name': b'Sam'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh1nvTrXF5ie",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJse4Gvl_I9m",
        "colab_type": "text"
      },
      "source": [
        "### TF.Data Inspection : element_spec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFs53LJD9tL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# d_mem1= tf.data.Dataset.from_tensor_slices(\n",
        "#           (tf.random.normal([2]),\n",
        "#           tf.random.normal([2,3])\n",
        "#           )\n",
        "#         )\n",
        "# print(d_mem1)\n",
        "# print(d_mem1.element_spec)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybu8cCrS5IN8",
        "colab_type": "text"
      },
      "source": [
        "## TF Transform :\n",
        "We can apply different type of transformations as per our need\n",
        "  - Dataset.map() : Per-Element operations\n",
        "  - Dataset.filter(): Per-element Filter Operation\n",
        "  - Dataset.reduce(): Reduce transfomations to single scalar value\n",
        "  - Dataset.batch(): Per-batch operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoPnPgkk1XaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "464196f2-7ed6-4974-8b91-65a73cd310e7"
      },
      "source": [
        "def f(x):\n",
        "  return x+2\n",
        "\n",
        "print('Dataset.map(): All elements Increment by 1')\n",
        "print(list(d_mem.map(lambda x: x+1, num_parallel_calls=2).as_numpy_iterator() ))\n",
        "print(f'Using function ',list(d_mem.map(lambda x: f(x), num_parallel_calls=2).as_numpy_iterator() ))\n",
        "\n",
        "\n",
        "print('\\nDataset.reduce():  With initial intitial_state or starting_val')\n",
        "print(f'=10 ', d_mem.reduce( 10,  lambda x,y: x+y).numpy() )\n",
        "print(f'=0 ', d_mem.reduce( 0,  lambda x,y: x+y).numpy() )"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset.map(): All elements Increment by 1\n",
            "[2, 3]\n",
            "Using function  [3, 4]\n",
            "\n",
            "Dataset.reduce():  With initial intitial_state or starting_val\n",
            "=10  13\n",
            "=0  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFXOuPfM-WoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
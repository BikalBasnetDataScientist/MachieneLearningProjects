{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFX : Production Scale ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNDZpxFI0SxJd4FQ5ZYYa3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beekal/MachieneLearningProjects/blob/master/0%20Basics%20-%20TF/TFX_Production_Scale_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfiXx2p98rNY",
        "colab_type": "text"
      },
      "source": [
        "## TFX : Why ?\n",
        "  - Provides end to end research to prod level ML solution , including Model versioning.\n",
        "  - Bundles all the task into a package ranging from\n",
        "    - Reading a data\n",
        "    - Preprocessing\n",
        "    - Model building / validation\n",
        "    - Model Deployment\n",
        "    - Model Versioning with rollback feature\n",
        "\n",
        "If we do not use the TFX and a separate model independent preprocessing steps, then we really would not be able to rollback, in case preprocessing step changes with the model version updates.\n",
        "\n",
        "### TFX Components :\n",
        "A TFX consists of following components which we will discuss here\n",
        "  - ExampleGen : Read data into TFX pipeline\n",
        "  - StatisticsGen : Calculate exploratory Statistics about the data\n",
        "  - SchemaGen : Create a data schema based on the Statistics\n",
        "  - ExampleValidator: Analyse the data for abnormalities / inconsistencies\n",
        "  - Transform: Perform necessary transformation in the data\n",
        "  - Trainer : Trains the model\n",
        "  - Evaluator : Evaluate the model performance to determine its readiness for deployment/ discard\n",
        "  - Pusher : Deploys model to the production\n",
        "  ![alt text](https://www.tensorflow.org/tfx/guide/images/diag_all.png)\n",
        "\n",
        "## TF Libraries for the components:\n",
        "  - StatsticsGen / SchemaGen/ ExampleValidator: [(TFDV) Tensorflow Data Validation](https://www.tensorflow.org/tfx/guide/tfdv) to generate Statistics, inspect Schema, Analyse/ validate Data. Also used to calculate/ record drifts/ anamolies  to identify if a model needs a retraining.\n",
        "  - ExampleValidator: (TFMD) : TensorFlow MetaData provides/stores Schema metadata to aid in Validation. Contains Schema for Data, Summary Statistics of the data.\n",
        "  - Transform: [(TFT). Tensorflow Transform](https://www.tensorflow.org/tfx/guide/tft)\n",
        "  - Evaluator: (TFMA). TensorFlow Model Analysis to evaluate models. Allows eval over large amount of data in a distributed way.\n",
        "  - MLMD: [ML MetaData](https://www.tensorflow.org/tfx/guide/mlmd) stores all relevant ML information other than data statistics including workflow.\n",
        "  - Pusher : [SavedModel](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model) + [TF Serving](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple). SavedModel is a universal, portable and recommended serialization for TF Model deployment across any mobile, JS app or any other infrastructure . TFServing serilises the TF model as SavedModel and deploys it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J7WZeAJw7qX",
        "colab_type": "text"
      },
      "source": [
        "## Visualisation :\n",
        "We can inspect the data visually using TFDV library using\n",
        "  - tfdv.load_statistics()\n",
        "  - tfdv.visualize_statistics()\n",
        "\n",
        "## Evaluator:\n",
        "\n",
        "## [TFX Guidelines](https://www.tensorflow.org/tfx/guide/train):\n",
        "If using TFX for developing pipelines, then \n",
        "- Model's input layes must consume from the SavedModel\n",
        "- Transform must be included in the model, so that the transformation can be exported along with the model using SavedModel\n",
        "- The model must be saved as both SavedModel (Used by TF Serving) and EvalSavedModel(used by TF Model Analyis for evaluation)\n",
        "\n",
        "REF: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ek_xlXs2eoM",
        "colab_type": "text"
      },
      "source": [
        "## Additional Dependency :\n",
        "  - Apache Beam :Develop in single node, run in multi-node \n",
        "\n",
        "  We want our ML to run parallely for  greater speed while also being scalable. E.g we would likely develop the Ml model  on a single computer, however when we want it on a prod, we would like to run it on a multi-node cluster environment to serve a lot of parallel requests  with low latency.  Apache  Beam provides this abstraction i.e whatever we research / develop in a single nodeis easily scalable  to multi-node cluster, without any extra work/effort or code modification.\n",
        "  - Apache Airflow / Kubeflow : Deploy, Scale and manage ML application automatically. \n",
        "  \n",
        "  Some example Cases Airflow/ Kubeflow handles\n",
        "    - Define 100 nodes/ input file path / checkpoint path / src code github path / \n",
        "    - Install required libraries in 100 node clusters\n",
        "    - then Deploy ML model to all of them\n",
        "    - Receiving tremendous volume of request for ML model, scale them up..\n",
        "    - Terrible disaster 50 nodes have gone down, we need to bring another 50 uoo to compensate.\n",
        "    - Efficiently utilise CPU/ GPU and minismise cost\n",
        "    - Train your model  cheaply using the AWS spot instance ( i.e use lower cost spot if available )\n",
        "\n",
        "REF : https://docs.agilestacks.com/article/gkyq26pzmr-creating-an-ml-pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0ao80eZ3sKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
---
title: "Time Series Forecasting"
author: "Bikal Basnet"
date: "Sunday, July 10, 2016"
output: html_document
---




**Data Load**

```{r}
install.packages("fUnitRoots")
library(fUnitRoots)
data(AirPassengers)
class(AirPassengers)
```

**Exploration of data**
```{r}
start(AirPassengers)
end(AirPassengers)

frequency(AirPassengers)
# The cycle of the time series is 12 months in a year.
```

**Observation**
*Exploration via Plotting Data*
1. Variance in data seems to be increasing with time
2. Passengers seem to grow over year
```{r, echo=FALSE}
plot(AirPassengers)
```
```{r, echo=FALSE}
# Fit the regression line to the series
abline(reg = lm(AirPassengers~time(AirPassengers)))

```

2. Plot appears to have seasonal component
```{r}
cycle(AirPassengers)
#Display a year on year trend with average of each year
plot(aggregate(AirPassengers, FUN= mean ))
boxplot(AirPassengers~cycle(AirPassengers))

```

**Step 2: Stationarise the series**
*Remove unequal variance by log*
```{r}
AirPassengers
AirPassengers_log = log(AirPassengers)
par(mfrow = c(1,2)) # plot figures in 1 row 2 columns
plot(AirPassengers,main="AirPAssenger Plot with unequal variance")
plot(AirPassengers_log, main="AirPAssenger Plot with variance removed")
```

*Address increasing trend component by taking the differnce of the series*
```{r}
AirPassengers_stationarised = diff(AirPassengers_log)
AirPassengers_stationarised
par(mfrow = c(2,2)) # plot figures in 1 row 2 columns
plot(AirPassengers,main="AirPAssenger Plot with unequal variance")
plot(AirPassengers_log, main="AirPAssenger Plot with variance removed")
plot(AirPassengers_stationarised, main="AirPAssenger Plot with variance and trend removed")
```

**Step 3:  Test : Is series Stationarised?**
Testing with Augmented Dickey-Fuller Test
```{r}
adfTest(AirPassengers);
adfTest(AirPassengers_stationarised)
```

**Step 4:  Optimal Parameter Extraction: Using ACf / PACF plots**
1. d : The d componenet is 1 , as we did 1 difference to make the series stationary i.e  
    if we had to do(diff(diff(airpassenger))), then the d  value would have been to 2

2. Number of AR (Auto-Regressive) terms i.e p : [2] AR terms are just lags of dependent varaibles i.e if  p is 5 then predictor of x(t) will be x(t-1)...x(t-5). i.e x(t) = alpha * x(t-1) + error(t)
The 5 indicates that the first instance depends upon the next 5 instances.
The order of the AR is answered by the ACF. 
The graph above has cut off  on PACF after 1, which means this is a AR(1) process.
            
In our case, we observe that the ACF plot cuts off after the first lag, hence the value of p should be 0 i.e (observed lag -1) i.e. x(t) = alpha*x(t-1)+ alpha*alpha*x(t-2)
for p = 4       
x(0) =alpha*x(t-1)+ alpha^2 x (t-2) + alpha^3 x (t-3) + alpha^4 x (t-4) + alpha^5 x (t-5)

As observed from the acf, the best p value is p = 1 i.e x(t) = alpha*x(t-1)
i.e x(0) = alpha * x(-1)

3. Number of MA (Moving Average) terms (q):   
    i.e x(t) = beta * error(t-1) + error(t)
As for the order of the MA, we observe that there is a cut off at ACF curve after 2,  which means this is a MA(2) order process
    

Ref : http://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/

```{r}
par(mfrow = c(1,3)) # plot figures in 1 row 2 columns
acf(AirPassengers_log, main="Decay is slow, hence observed population is not stationary")
# The decay is very slow, hence we see that the population is not stationary
acf(AirPassengers_stationarised, main = 'ACF plot of stationarised AirPAssengers')
pacf(AirPassengers_stationarised, main = 'PACF plot of stationarised AirPAssengers')
```


*STEP 5: Build ARMA Time Series Modeling*
The (p,d,q) parameters value obtained in the previous section might be an approximate estimate and to get the most optimal parameters, we will neeed to explore the (p,d,q) combination further, to achieve the lowest BIC and AIC.

They are the most commonly used time series models. The AR in ARMA stands for Auto-regression while MA stands for moving average.
Auto Regression : The following equation is known as AR formulation, in which the current year's output is said to be dependent on previous year's output and so on.The shock or fluctuations in the Ar model  is gradually absorbed with time and fades away slowly. e.g GDP increase or decrease

x(t) = alpha *  x(t - 1) + error (t)

IMP Note :  ARMA is not applicable on nonstationary series.

Moving average : In the Moving average model, the shock i.e rapid increases or decreases fade quickly for example, a bad sales suddenly went up the roor and suddenyl in the next week it goies out of fashion and theere is no sales. In this case the shock is rapid while also that the drop is rapid. This is  the MA model.

The difference between the AR and the MA model being that  in AR model, the increase and decrease both are gradual, while in the MA model the increase and decrease both are sudden and sharp.
x(t) = beta *  error(t-1) + error (t)


**Step 5 : Make prediction**
Best p,d,q values is (0,1,1)
Fitting the arima model now.
Since we observe the seasonal component in the ARIMA, we also factor it in.
```{r}
par(mfrow = c(1,1)) 
fit <- arima(AirPassengers_log, c(0,1,1), seasonal = list(order = c(0,1,1), period = 12))
pred <- predict(fit, n.ahead = 10*12) # predicting next 10 years
ts.plot(AirPassengers, 2.718 ^ pred$pred, log = "y", lty = c(1,3))

```




You can also embed plots, for example:

```{r, echo=FALSE}
#plot(cars)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

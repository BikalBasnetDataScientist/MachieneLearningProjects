{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification - Part 3.1: Imbalanced Data (Logistic Regression)\n",
    "- Objective : \n",
    "    1. Classify yes class  along with their probability.\n",
    "\n",
    "## 1.0 Constrains\n",
    "\n",
    "### 1.1 Modeling Constraints : \n",
    "- Modeling Type : Classification    \n",
    "- Model Explainaibility : \n",
    "    - 1. One Logistic Regression Model : Explainable Model\n",
    "    - 2. Another Black Box Model : \n",
    "        \n",
    "    \n",
    "### 1.2 Data Constraint :\n",
    "- Primary Data : \n",
    "    - Dependent / Outcome Variable :  y (0/1)\n",
    "    - Independent / Predictor Variables :  100 Feature Variables (Categorical, numerical)\n",
    "- Imbalanced Data :  \n",
    "\n",
    "\n",
    "### 1.3 Evaluation Metrics Constraint:\n",
    "- Imbalanced Data specific Evaluation Metrics\n",
    "- General constraints i.e accuracy leads to improper evaluation / modeling & final choice.\n",
    "\n",
    "### 1.4 Framework Constraint : \n",
    "- This constraint is self-selected based on ease of use and data size.\n",
    "- Scikit-learn. \n",
    "    - Scikit-Learn : Given that the Data-size is comparatively small, scikit-learn  is selected as the framework, given its comparative ease  or use and faster Iteration possiblity.\n",
    "- TensorFlow/ Keras : Complex Deep Neural Network  modeling and GPU support is provided by TensorFlow / Keras, and hence is used for more complex NN modeling.\n",
    "- StatsModel :\n",
    "    - Scikit-learn provides more varied models(Forest, Boosting, Bagging, SVM, NNs), powerful parameter customisation and control. However the LR model generated by them have few model and model parameter statistics compared to StatsModel (i.e p-value, 95% CI, marginal_effects ). Hence we will use StatsModel's LogisticRegression in the second part.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Load: Modeling Ready Data\n",
    "- Data has been preprocessed with the earlier notebook.\n",
    "- We are loading the preprocessed data in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "test_df = pd.read_csv(\"Data/ModelReadyData/test_df.csv\")\n",
    "train_df = pd.read_csv(\"Data/ModelReadyData/train_df.csv\")\n",
    "\n",
    "X_train = pd.read_csv(\"Data/ModelReadyData/X_train.csv\")\n",
    "y_train = pd.read_csv(\"Data/ModelReadyData/y_train.csv\", index_col=False)\n",
    "\n",
    "X_valid = pd.read_csv(\"Data/ModelReadyData/X_valid.csv\")\n",
    "y_valid = pd.read_csv(\"Data/ModelReadyData/y_valid.csv\")\n",
    "\n",
    "X_train_smote = pd.read_csv(\"Data/ModelReadyData/X_train_smote.csv\")\n",
    "y_train_smote = pd.read_csv(\"Data/ModelReadyData/y_train_smote.csv\")\n",
    "\n",
    "# ckpt_train_df_pre_scaling = pd.read_csv(\"Data/ModelReadyData/ckpt_train_df_pre_scaling.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_yes_probability_to_a_file(predict_probability, file_name):\n",
    "    class_1_probability_df = pd.DataFrame(columns=[\"class_1_probability\"])\n",
    "\n",
    "    for i, class_proba in enumerate(predict_probability):\n",
    "        _, one_proba = class_proba[0], class_proba[1]\n",
    "        class_1_probability_df.loc[i] = round(one_proba, 4)\n",
    "\n",
    "    class_1_probability_df.to_csv(file_name, header=None, index=None)\n",
    "    return class_1_probability_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluation Metric Selection:\n",
    "\n",
    "- Accuracy is a bad Evaluation metric given  highly imbalanced nature of Data.\n",
    "- We will choose AUC, recall, F1-score and additionally use confusion matrix. \n",
    "- Recall : Ability of model to find all positive classes\n",
    "- Precision : Ability of model to not label as positive a sample that is negative.\n",
    "- Balanced accuracy : avg of recall across all classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = [\"roc_auc\", \"recall\", \"precision\", \"accuracy\", \"balanced_accuracy\", \"f1\"]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from typing import List\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "# Defining the confusion matrix function\n",
    "def plot_confusion_matrix(cm, class_labels, title=\"Confusion matrix\", cmap=plt.cm.Blues):\n",
    "    import itertools\n",
    "\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_labels))\n",
    "    plt.xticks(tick_marks, class_labels, rotation=45)\n",
    "    plt.yticks(tick_marks, class_labels)\n",
    "\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def area_under_roc(y, pred):\n",
    "    from sklearn import metrics\n",
    "\n",
    "    # fpr,tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2 )\n",
    "    auc = metrics.roc_auc_score(y, pred)\n",
    "    # print('fpr,tpr, AUC (higher is better)', fpr, tpr, auc)\n",
    "    print(\"AUC (higher is better)\", auc)\n",
    "    return auc\n",
    "\n",
    "\n",
    "def eval_classification(\n",
    "    y_test,\n",
    "    ypred_test,\n",
    "    class_labels=[\"Retained\", \"Churn - Lost\"],\n",
    "    title=\"Confusion Matrix\",\n",
    "    metrics=[\"confusion_matrix\"],\n",
    "):\n",
    "    returned_metrics = []\n",
    "\n",
    "    if \"auc\" in metrics:\n",
    "        auc = area_under_roc(y_test, ypred_test)\n",
    "        returned_metrics.append(auc)\n",
    "    if \"confusion_matrix\" in metrics:\n",
    "        print(\"Confusion report is\")\n",
    "        print(classification_report(y_test, ypred_test))\n",
    "        conf_matrix = confusion_matrix(y_test, ypred_test)\n",
    "        plot_confusion_matrix(conf_matrix, class_labels=class_labels, title=title)\n",
    "        returned_metrics.append(conf_matrix)\n",
    "\n",
    "    return returned_metrics[0] if len(returned_metrics) == 1 else returned_metrics\n",
    "\n",
    "\n",
    "def plot_roc(y_test: List[int], y_test_proba: List[float]):\n",
    "    auc = roc_auc_score(y_test, y_test_proba)\n",
    "    print(\"Logistic: ROC AUC=%.3f\" % (auc))\n",
    "\n",
    "    # # keep probabilities for the positive outcome only\n",
    "    # probability_positive_class = y_test_proba[:, 1]\n",
    "    probability_positive_class = y_test_proba\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, probability_positive_class)\n",
    "\n",
    "    pyplot.plot(fpr, tpr, marker=\".\", label=\"\")\n",
    "    pyplot.title(\"ROC Curve\")\n",
    "    pyplot.xlabel(\"False Positive Rate (False Alarm Rate)\")\n",
    "    pyplot.ylabel(\"True Positive Rate ( Sensitivity, Hit Rate)\")\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "def plot_roc_for_binary_prediction_label(y_test, y_test_prediction):\n",
    "    from plot_metric.functions import BinaryClassification\n",
    "\n",
    "    bc = BinaryClassification(y_test, y_test_prediction, labels=[\"Class 1\", \"Class 2\"])\n",
    "    # Figures\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    bc.plot_roc_curve()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_new_score_tracker_df(scoring):\n",
    "    score_tracker_dict = {\"model_name\": []}\n",
    "    scores = {}\n",
    "    for cur_scoring in scoring:\n",
    "        score_tracker_dict.update({cur_scoring: []})\n",
    "\n",
    "    score_tracker_df = pd.DataFrame(score_tracker_dict)\n",
    "    return score_tracker_df\n",
    "\n",
    "\n",
    "def add_cv_score_to_df(df, model_descriptor_name, scoring, cur_clf_cv_result):\n",
    "    cur_scores = {\"model_name\": model_descriptor_name}\n",
    "    for cur_scoring in scoring:\n",
    "        cur_scores.update({cur_scoring: round(cur_clf_cv_result[\"test_\" + cur_scoring].mean(), 2)})\n",
    "\n",
    "    df = df.append(cur_scores, ignore_index=True)\n",
    "    df = df.drop_duplicates(keep=\"last\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_cv_scores(clf, model_descriptor_name, X, y, scoring, score_tracker_df=None, cv_fold=5):\n",
    "    # Create Score tracker df if first run\n",
    "    if score_tracker_df is None:\n",
    "        print(\"Created New Score Tracker\")\n",
    "        score_tracker_df = create_new_score_tracker_df(scoring)\n",
    "\n",
    "    cur_clf_cv_result = cross_validate(clf, X, y, cv=cv_fold, scoring=scoring)\n",
    "    score_tracker_df = add_cv_score_to_df(score_tracker_df, model_descriptor_name, scoring, cur_clf_cv_result)\n",
    "\n",
    "    return score_tracker_df\n",
    "\n",
    "\n",
    "def get_benchmark_cv_scores(clf, x_columns, df, model_name, score_tracker_df=None):\n",
    "    if x_columns is None:\n",
    "        x_columns = list(df.columns)\n",
    "        x_columns.remove(\"y\")\n",
    "    print(f\" Columns : {len(x_columns)}\")\n",
    "    cur_X_train, cur_y_train, _, _ = get_stratified_data(df[x_columns], df[\"y\"], test_size=0.01, seed=4)\n",
    "    score_tracker_df = get_cv_scores(\n",
    "        clf=clf,\n",
    "        X=cur_X_train,\n",
    "        y=cur_y_train[0],\n",
    "        cv_fold=5,\n",
    "        scoring=scoring,\n",
    "        model_descriptor_name=model_name,\n",
    "        score_tracker_df=score_tracker_df,\n",
    "    )\n",
    "    return score_tracker_df\n",
    "\n",
    "\n",
    "def get_clasifier_evaluation(clf, X_train=None, y_train=None, X_valid=None, y_valid=None):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_valid = clf.predict(X_valid)\n",
    "    eval_classification(y_valid, y_pred_valid, class_labels=[\"no\", \"yes\"], metrics=[\"auc\", \"confusion_matrix\"])\n",
    "\n",
    "## DataSet Creation\n",
    "def get_stratified_data(X_df, y_series, test_size=0.2, verbose=False, seed=4):\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "    feature_cols = list(X_df.columns)\n",
    "    # target_col = y_df.columns\n",
    "\n",
    "    X = np.array(X_df)\n",
    "    y = np.array(y_series)\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=2, test_size=test_size, random_state=seed)\n",
    "    sss.get_n_splits(X, y)\n",
    "\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        if verbose:\n",
    "            print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = pd.DataFrame(X_train, columns=feature_cols)\n",
    "    X_test = pd.DataFrame(X_test, columns=feature_cols)\n",
    "\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n ...No. of Training Data : {len(X_train)}\")\n",
    "        print(f\" ...No. of Test Data     : {len(X_test)}\")\n",
    "        print(f\" ...No. of Features : {len(X_train.columns)}\")\n",
    "        print(f\" ...Train Bincount : {np.bincount(y_train[0])}\")\n",
    "        print(f\" ...Test Bincount  : {np.bincount(y_test[0])}\")\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def simple_tree(\n",
    "    X_trainR,\n",
    "    y_trainR,\n",
    "    X_testR,\n",
    "    y_testR,\n",
    "    max_depth=None,\n",
    "    class_weight=None,\n",
    "    class_labels=[\"yes\", \"no\"],\n",
    "    metrics=[\"confusion_matrix\"],\n",
    "):\n",
    "    from sklearn import tree\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier(max_depth=max_depth, class_weight=class_weight)\n",
    "    clf.fit(X_trainR, y_trainR)\n",
    "    ypred_testR = clf.predict(X_testR)\n",
    "\n",
    "    eval_classification(y_testR, ypred_testR, class_labels=class_labels, metrics=metrics)\n",
    "    return clf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6. Modeling with StatsModel : Logistic Regression Explainable Models\n",
    "- Logistic Regression from sklearn  is good for prediction  however it is barebone. It provides very few statistical information compared to statsmodel i.e it does not provied p-values for coeficients to tell us coeficient significance and also provided musch richer model statistical information. Hence we will  build a Logistic Regression model with statsmodel library and then use it for further explanation. It provides information i.e p-values for the coeficient, pseudo-R2 values.\n",
    "- Hence for explainable model purpose, we will modeling with statsmodel with more descriptive statistical properties.\n",
    "\n",
    "Ref: https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Logit.fit.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6.1 Further Pre-Processing : Multi-colinearity (df-1)\n",
    "- Transform dummy variable into n-1 variables, because nth variable can be perfectly explained by 1,2,... n-1 variables \n",
    "- We will remove the nth variable and during interpretation we will use it as a reference variable to make the explanations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_747275/3086595888.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={col: col.replace(\".\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")}, inplace=True)\n",
      "/tmp/ipykernel_747275/3086595888.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={col: col.replace(\".\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")}, inplace=True)\n",
      "/tmp/ipykernel_747275/3086595888.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={col: col.replace(\".\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")}, inplace=True)\n",
      "/tmp/ipykernel_747275/3086595888.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={col: col.replace(\".\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")}, inplace=True)\n",
      "/tmp/ipykernel_747275/3086595888.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={col: col.replace(\".\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")}, inplace=True)\n",
      "/tmp/ipykernel_747275/3086595888.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={col: col.replace(\".\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")}, inplace=True)\n",
      "/tmp/ipykernel_747275/3086595888.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={col: col.replace(\".\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")}, inplace=True)\n",
      "/tmp/ipykernel_747275/3086595888.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={col: col.replace(\".\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")}, inplace=True)\n",
      "/tmp/ipykernel_747275/3086595888.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={col: col.replace(\".\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")}, inplace=True)\n",
      "/tmp/ipykernel_747275/3086595888.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={col: col.replace(\".\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.35216009824265926\n",
      "            Iterations: 100\n",
      "            Function evaluations: 104\n",
      "            Gradient evaluations: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bikal/work/venv_ds_3.10/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.150</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>         <td>y</td>              <td>AIC:</td>        <td>28506.8079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2023-03-11 22:43</td>       <td>BIC:</td>        <td>29942.4459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>40000</td>       <td>Log-Likelihood:</td>    <td>-14086.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>             <td>166</td>           <td>LL-Null:</td>        <td>-16563.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>39833</td>        <td>LLR p-value:</td>      <td>0.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>0.0000</td>           <td>Scale:</td>         <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>       <td>100.0000</td>             <td></td>               <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "           <td></td>           <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>          <td>-1.9579</td>  <td>0.1381</td>  <td>-14.1794</td> <td>0.0000</td> <td>-2.2285</td> <td>-1.6872</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>                 <td>0.0297</td>   <td>0.0223</td>   <td>1.3348</td>  <td>0.1819</td> <td>-0.0139</td> <td>0.0733</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>                <td>-0.0054</td>  <td>0.0194</td>   <td>-0.2811</td> <td>0.7787</td> <td>-0.0434</td> <td>0.0325</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x100</th>               <td>0.0384</td>   <td>0.0511</td>   <td>0.7520</td>  <td>0.4520</td> <td>-0.0617</td> <td>0.1386</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>                <td>0.1700</td>   <td>0.0403</td>   <td>4.2143</td>  <td>0.0000</td> <td>0.0909</td>  <td>0.2490</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>                <td>-0.0257</td>  <td>0.0765</td>   <td>-0.3360</td> <td>0.7368</td> <td>-0.1757</td> <td>0.1243</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>                <td>0.0591</td>   <td>0.0371</td>   <td>1.5919</td>  <td>0.1114</td> <td>-0.0137</td> <td>0.1318</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>                <td>0.0432</td>   <td>0.0240</td>   <td>1.8024</td>  <td>0.0715</td> <td>-0.0038</td> <td>0.0902</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>                <td>-0.0345</td>  <td>0.0155</td>   <td>-2.2212</td> <td>0.0263</td> <td>-0.0649</td> <td>-0.0041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>                <td>-0.0362</td>  <td>0.0205</td>   <td>-1.7638</td> <td>0.0778</td> <td>-0.0764</td> <td>0.0040</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>                <td>0.0223</td>   <td>0.0405</td>   <td>0.5516</td>  <td>0.5812</td> <td>-0.0570</td> <td>0.1017</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>                <td>-0.2377</td>  <td>0.0798</td>   <td>-2.9774</td> <td>0.0029</td> <td>-0.3941</td> <td>-0.0812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>                <td>-0.1965</td>  <td>0.0207</td>   <td>-9.4929</td> <td>0.0000</td> <td>-0.2371</td> <td>-0.1560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>                 <td>0.1057</td>   <td>0.0804</td>   <td>1.3146</td>  <td>0.1887</td> <td>-0.0519</td> <td>0.2632</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>                <td>-0.1279</td>  <td>0.0537</td>   <td>-2.3822</td> <td>0.0172</td> <td>-0.2332</td> <td>-0.0227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>                <td>0.0159</td>   <td>0.0153</td>   <td>1.0367</td>  <td>0.2999</td> <td>-0.0141</td> <td>0.0458</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>                <td>-0.0025</td>  <td>0.0194</td>   <td>-0.1300</td> <td>0.8966</td> <td>-0.0405</td> <td>0.0355</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>                <td>0.0061</td>   <td>0.0175</td>   <td>0.3468</td>  <td>0.7288</td> <td>-0.0283</td> <td>0.0404</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>                <td>-0.0036</td>  <td>0.0306</td>   <td>-0.1164</td> <td>0.9074</td> <td>-0.0636</td> <td>0.0564</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>                <td>0.0187</td>   <td>0.0215</td>   <td>0.8721</td>  <td>0.3831</td> <td>-0.0233</td> <td>0.0608</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>                <td>-0.0055</td>  <td>0.0189</td>   <td>-0.2896</td> <td>0.7721</td> <td>-0.0426</td> <td>0.0316</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>                <td>0.1243</td>   <td>0.0523</td>   <td>2.3787</td>  <td>0.0174</td> <td>0.0219</td>  <td>0.2267</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>                <td>-0.3474</td>  <td>0.0717</td>   <td>-4.8463</td> <td>0.0000</td> <td>-0.4879</td> <td>-0.2069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>                <td>-0.0102</td>  <td>0.0145</td>   <td>-0.7058</td> <td>0.4803</td> <td>-0.0387</td> <td>0.0182</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>                <td>-0.6182</td>  <td>0.0596</td>  <td>-10.3674</td> <td>0.0000</td> <td>-0.7350</td> <td>-0.5013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>                <td>0.0067</td>   <td>0.0108</td>   <td>0.6175</td>  <td>0.5369</td> <td>-0.0145</td> <td>0.0279</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Alabama</th>        <td>-0.1890</td>  <td>0.1369</td>   <td>-1.3801</td> <td>0.1675</td> <td>-0.4574</td> <td>0.0794</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Alaska</th>         <td>0.6814</td>   <td>0.1839</td>   <td>3.7059</td>  <td>0.0002</td> <td>0.3210</td>  <td>1.0417</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Arizona</th>        <td>-0.2409</td>  <td>0.1261</td>   <td>-1.9105</td> <td>0.0561</td> <td>-0.4880</td> <td>0.0062</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Arkansas</th>       <td>-0.2306</td>  <td>0.1735</td>   <td>-1.3291</td> <td>0.1838</td> <td>-0.5707</td> <td>0.1095</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Colorado</th>       <td>-0.3594</td>  <td>0.1429</td>   <td>-2.5152</td> <td>0.0119</td> <td>-0.6395</td> <td>-0.0793</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Connecticut</th>    <td>-0.1301</td>  <td>0.1500</td>   <td>-0.8671</td> <td>0.3859</td> <td>-0.4242</td> <td>0.1640</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_DC</th>             <td>-0.0012</td>  <td>0.2128</td>   <td>-0.0054</td> <td>0.9957</td> <td>-0.4183</td> <td>0.4160</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Delaware</th>       <td>-0.3200</td>  <td>0.2485</td>   <td>-1.2880</td> <td>0.1977</td> <td>-0.8071</td> <td>0.1670</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Florida</th>        <td>0.2931</td>   <td>0.0775</td>   <td>3.7833</td>  <td>0.0002</td> <td>0.1413</td>  <td>0.4450</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Georgia</th>        <td>0.3543</td>   <td>0.0973</td>   <td>3.6437</td>  <td>0.0003</td> <td>0.1637</td>  <td>0.5450</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Hawaii</th>         <td>0.0103</td>   <td>0.1707</td>   <td>0.0606</td>  <td>0.9517</td> <td>-0.3242</td> <td>0.3449</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Idaho</th>          <td>0.7955</td>   <td>0.1456</td>   <td>5.4631</td>  <td>0.0000</td> <td>0.5101</td>  <td>1.0810</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Illinois</th>       <td>0.3201</td>   <td>0.0868</td>   <td>3.6871</td>  <td>0.0002</td> <td>0.1500</td>  <td>0.4903</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Indiana</th>        <td>0.2469</td>   <td>0.1097</td>   <td>2.2514</td>  <td>0.0244</td> <td>0.0320</td>  <td>0.4619</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Iowa</th>           <td>0.2301</td>   <td>0.1482</td>   <td>1.5522</td>  <td>0.1206</td> <td>-0.0604</td> <td>0.5207</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Kansas</th>         <td>0.5476</td>   <td>0.1343</td>   <td>4.0788</td>  <td>0.0000</td> <td>0.2845</td>  <td>0.8108</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Kentucky</th>       <td>0.3124</td>   <td>0.1285</td>   <td>2.4306</td>  <td>0.0151</td> <td>0.0605</td>  <td>0.5644</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Louisiana</th>      <td>-0.2795</td>  <td>0.1451</td>   <td>-1.9259</td> <td>0.0541</td> <td>-0.5639</td> <td>0.0049</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Maine</th>          <td>0.1040</td>   <td>0.1809</td>   <td>0.5749</td>  <td>0.5654</td> <td>-0.2506</td> <td>0.4586</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Maryland</th>       <td>-0.2912</td>  <td>0.1358</td>   <td>-2.1442</td> <td>0.0320</td> <td>-0.5574</td> <td>-0.0250</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Massachusetts</th>  <td>-0.1929</td>  <td>0.1276</td>   <td>-1.5115</td> <td>0.1307</td> <td>-0.4429</td> <td>0.0572</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Michigan</th>       <td>-0.2792</td>  <td>0.1073</td>   <td>-2.6034</td> <td>0.0092</td> <td>-0.4895</td> <td>-0.0690</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Minnesota</th>      <td>-0.1984</td>  <td>0.1271</td>   <td>-1.5602</td> <td>0.1187</td> <td>-0.4475</td> <td>0.0508</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Mississippi</th>    <td>-0.1800</td>  <td>0.1610</td>   <td>-1.1180</td> <td>0.2636</td> <td>-0.4956</td> <td>0.1356</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Missouri</th>       <td>0.2149</td>   <td>0.1167</td>   <td>1.8413</td>  <td>0.0656</td> <td>-0.0139</td> <td>0.4437</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Montana</th>        <td>0.4187</td>   <td>0.1901</td>   <td>2.2030</td>  <td>0.0276</td> <td>0.0462</td>  <td>0.7913</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Nebraska</th>       <td>0.0715</td>   <td>0.1597</td>   <td>0.4481</td>  <td>0.6541</td> <td>-0.2414</td> <td>0.3845</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Nevada</th>         <td>-0.0823</td>  <td>0.1530</td>   <td>-0.5383</td> <td>0.5904</td> <td>-0.3821</td> <td>0.2175</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_New_Hampshire</th>  <td>-0.0530</td>  <td>0.1912</td>   <td>-0.2774</td> <td>0.7815</td> <td>-0.4278</td> <td>0.3217</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_New_Jersey</th>     <td>-0.2102</td>  <td>0.1133</td>   <td>-1.8546</td> <td>0.0637</td> <td>-0.4324</td> <td>0.0119</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_New_Mexico</th>     <td>-0.1120</td>  <td>0.1630</td>   <td>-0.6870</td> <td>0.4921</td> <td>-0.4315</td> <td>0.2075</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_New_York</th>       <td>-0.0292</td>  <td>0.0835</td>   <td>-0.3498</td> <td>0.7265</td> <td>-0.1929</td> <td>0.1345</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_North_Carolina</th> <td>0.5060</td>   <td>0.0945</td>   <td>5.3536</td>  <td>0.0000</td> <td>0.3207</td>  <td>0.6912</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_North_Dakota</th>   <td>-0.2209</td>  <td>0.2290</td>   <td>-0.9648</td> <td>0.3346</td> <td>-0.6697</td> <td>0.2279</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Ohio</th>           <td>0.2646</td>   <td>0.0921</td>   <td>2.8719</td>  <td>0.0041</td> <td>0.0840</td>  <td>0.4452</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Oklahoma</th>       <td>-0.2414</td>  <td>0.1584</td>   <td>-1.5233</td> <td>0.1277</td> <td>-0.5519</td> <td>0.0692</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Oregon</th>         <td>1.0426</td>   <td>0.1159</td>   <td>8.9964</td>  <td>0.0000</td> <td>0.8154</td>  <td>1.2697</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Pennsylvania</th>   <td>-0.1710</td>  <td>0.0963</td>   <td>-1.7751</td> <td>0.0759</td> <td>-0.3598</td> <td>0.0178</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Rhode_Island</th>   <td>-0.1327</td>  <td>0.1874</td>   <td>-0.7081</td> <td>0.4789</td> <td>-0.5001</td> <td>0.2347</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_South_Carolina</th> <td>0.3071</td>   <td>0.1283</td>   <td>2.3938</td>  <td>0.0167</td> <td>0.0557</td>  <td>0.5586</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_South_Dakota</th>   <td>0.0479</td>   <td>0.2005</td>   <td>0.2389</td>  <td>0.8112</td> <td>-0.3451</td> <td>0.4409</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Tennessee</th>      <td>-0.3413</td>  <td>0.1279</td>   <td>-2.6693</td> <td>0.0076</td> <td>-0.5920</td> <td>-0.0907</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Texas</th>          <td>-0.0501</td>  <td>0.0769</td>   <td>-0.6521</td> <td>0.5143</td> <td>-0.2008</td> <td>0.1006</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Utah</th>           <td>-0.1938</td>  <td>0.1691</td>   <td>-1.1459</td> <td>0.2519</td> <td>-0.5253</td> <td>0.1377</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Vermont</th>        <td>0.1675</td>   <td>0.1983</td>   <td>0.8447</td>  <td>0.3983</td> <td>-0.2212</td> <td>0.5562</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Virginia</th>       <td>-0.4116</td>  <td>0.1255</td>   <td>-3.2811</td> <td>0.0010</td> <td>-0.6575</td> <td>-0.1657</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Washington</th>     <td>0.8845</td>   <td>0.0973</td>   <td>9.0926</td>  <td>0.0000</td> <td>0.6939</td>  <td>1.0752</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_West_Virginia</th>  <td>-0.1058</td>  <td>0.1724</td>   <td>-0.6135</td> <td>0.5395</td> <td>-0.4437</td> <td>0.2321</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Wisconsin</th>      <td>-0.1975</td>  <td>0.1316</td>   <td>-1.5006</td> <td>0.1335</td> <td>-0.4554</td> <td>0.0605</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33_Wyoming</th>        <td>-0.3258</td>  <td>0.2488</td>   <td>-1.3096</td> <td>0.1903</td> <td>-0.8134</td> <td>0.1618</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>                <td>-0.0945</td>  <td>0.0639</td>   <td>-1.4790</td> <td>0.1391</td> <td>-0.2198</td> <td>0.0307</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>                <td>0.0135</td>   <td>0.0185</td>   <td>0.7331</td>  <td>0.4635</td> <td>-0.0227</td> <td>0.0497</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>                <td>-0.3340</td>  <td>0.0758</td>   <td>-4.4088</td> <td>0.0000</td> <td>-0.4825</td> <td>-0.1855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>                <td>0.2400</td>   <td>0.0712</td>   <td>3.3718</td>  <td>0.0007</td> <td>0.1005</td>  <td>0.3795</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>                <td>-0.0201</td>  <td>0.0193</td>   <td>-1.0386</td> <td>0.2990</td> <td>-0.0579</td> <td>0.0178</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3_Fri</th>             <td>0.2330</td>   <td>0.0552</td>   <td>4.2212</td>  <td>0.0000</td> <td>0.1248</td>  <td>0.3412</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3_Sat</th>             <td>0.2109</td>   <td>0.0559</td>   <td>3.7724</td>  <td>0.0002</td> <td>0.1013</td>  <td>0.3204</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3_Sun</th>             <td>0.2360</td>   <td>0.0584</td>   <td>4.0396</td>  <td>0.0001</td> <td>0.1215</td>  <td>0.3504</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3_Thur</th>            <td>0.0060</td>   <td>0.0604</td>   <td>0.0997</td>  <td>0.9206</td> <td>-0.1124</td> <td>0.1245</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3_Tue</th>             <td>-0.2514</td>  <td>0.0559</td>   <td>-4.4930</td> <td>0.0000</td> <td>-0.3610</td> <td>-0.1417</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3_Wed</th>             <td>-0.0455</td>  <td>0.0541</td>   <td>-0.8401</td> <td>0.4008</td> <td>-0.1515</td> <td>0.0606</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>                 <td>0.0010</td>   <td>0.0140</td>   <td>0.0705</td>  <td>0.9438</td> <td>-0.0265</td> <td>0.0285</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>                <td>-0.3072</td>  <td>0.0659</td>   <td>-4.6648</td> <td>0.0000</td> <td>-0.4363</td> <td>-0.1781</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>                <td>-0.0716</td>  <td>0.0277</td>   <td>-2.5842</td> <td>0.0098</td> <td>-0.1260</td> <td>-0.0173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>                <td>0.0091</td>   <td>0.0172</td>   <td>0.5278</td>  <td>0.5976</td> <td>-0.0246</td> <td>0.0428</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>                <td>-0.0060</td>  <td>0.0166</td>   <td>-0.3601</td> <td>0.7188</td> <td>-0.0385</td> <td>0.0266</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>                <td>0.0068</td>   <td>0.0164</td>   <td>0.4178</td>  <td>0.6761</td> <td>-0.0253</td> <td>0.0390</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>                <td>-0.0304</td>  <td>0.0169</td>   <td>-1.7953</td> <td>0.0726</td> <td>-0.0635</td> <td>0.0028</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>                <td>-0.8703</td>  <td>0.0629</td>  <td>-13.8252</td> <td>0.0000</td> <td>-0.9937</td> <td>-0.7469</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>                <td>0.0318</td>   <td>0.0850</td>   <td>0.3739</td>  <td>0.7085</td> <td>-0.1349</td> <td>0.1985</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>                <td>0.0069</td>   <td>0.0218</td>   <td>0.3141</td>  <td>0.7535</td> <td>-0.0359</td> <td>0.0496</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>                 <td>0.0504</td>   <td>0.0177</td>   <td>2.8446</td>  <td>0.0044</td> <td>0.0157</td>  <td>0.0851</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>                <td>-0.0644</td>  <td>0.0579</td>   <td>-1.1118</td> <td>0.2662</td> <td>-0.1780</td> <td>0.0491</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>                <td>0.0448</td>   <td>0.0105</td>   <td>4.2562</td>  <td>0.0000</td> <td>0.0242</td>  <td>0.0654</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>                <td>-0.1331</td>  <td>0.0688</td>   <td>-1.9351</td> <td>0.0530</td> <td>-0.2680</td> <td>0.0017</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>                <td>0.0373</td>   <td>0.0164</td>   <td>2.2765</td>  <td>0.0228</td> <td>0.0052</td>  <td>0.0694</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>                <td>-0.0069</td>  <td>0.0266</td>   <td>-0.2603</td> <td>0.7946</td> <td>-0.0590</td> <td>0.0452</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>                <td>0.0003</td>   <td>0.0005</td>   <td>0.7201</td>  <td>0.4715</td> <td>-0.0006</td> <td>0.0012</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>                <td>-0.0178</td>  <td>0.0513</td>   <td>-0.3465</td> <td>0.7290</td> <td>-0.1184</td> <td>0.0828</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>                 <td>-0.0186</td>  <td>0.0179</td>   <td>-1.0426</td> <td>0.2971</td> <td>-0.0537</td> <td>0.0164</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60_April</th>          <td>0.0405</td>   <td>0.1048</td>   <td>0.3865</td>  <td>0.6992</td> <td>-0.1649</td> <td>0.2459</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60_December</th>       <td>-0.0540</td>  <td>0.0485</td>   <td>-1.1145</td> <td>0.2651</td> <td>-0.1491</td> <td>0.0410</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60_February</th>       <td>-0.0062</td>  <td>0.0940</td>   <td>-0.0662</td> <td>0.9472</td> <td>-0.1904</td> <td>0.1780</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60_January</th>        <td>0.0121</td>   <td>0.0482</td>   <td>0.2508</td>  <td>0.8020</td> <td>-0.0824</td> <td>0.1066</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60_July</th>           <td>-0.0386</td>  <td>0.0487</td>   <td>-0.7913</td> <td>0.4288</td> <td>-0.1341</td> <td>0.0570</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60_June</th>           <td>-0.1578</td>  <td>0.0946</td>   <td>-1.6682</td> <td>0.0953</td> <td>-0.3433</td> <td>0.0276</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60_March</th>          <td>0.1646</td>   <td>0.1092</td>   <td>1.5075</td>  <td>0.1317</td> <td>-0.0494</td> <td>0.3786</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60_May</th>            <td>-0.2449</td>  <td>0.1224</td>   <td>-2.0004</td> <td>0.0455</td> <td>-0.4849</td> <td>-0.0049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60_November</th>       <td>0.0978</td>   <td>0.0989</td>   <td>0.9889</td>  <td>0.3227</td> <td>-0.0960</td> <td>0.2915</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60_October</th>        <td>0.0987</td>   <td>0.1116</td>   <td>0.8849</td>  <td>0.3762</td> <td>-0.1199</td> <td>0.3174</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60_September</th>      <td>0.0663</td>   <td>0.0913</td>   <td>0.7258</td>  <td>0.4680</td> <td>-0.1127</td> <td>0.2453</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>                <td>0.2946</td>   <td>0.0436</td>   <td>6.7633</td>  <td>0.0000</td> <td>0.2092</td>  <td>0.3800</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>                <td>-0.1494</td>  <td>0.0491</td>   <td>-3.0412</td> <td>0.0024</td> <td>-0.2456</td> <td>-0.0531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>                <td>0.0098</td>   <td>0.0143</td>   <td>0.6822</td>  <td>0.4951</td> <td>-0.0183</td> <td>0.0378</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>                <td>-0.1065</td>  <td>0.0403</td>   <td>-2.6434</td> <td>0.0082</td> <td>-0.1855</td> <td>-0.0275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65_esurance</th>       <td>-0.0100</td>  <td>0.0466</td>   <td>-0.2153</td> <td>0.8295</td> <td>-0.1014</td> <td>0.0813</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65_farmers</th>        <td>-0.0906</td>  <td>0.0512</td>   <td>-1.7694</td> <td>0.0768</td> <td>-0.1910</td> <td>0.0098</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65_geico</th>          <td>-0.0449</td>  <td>0.0507</td>   <td>-0.8851</td> <td>0.3761</td> <td>-0.1443</td> <td>0.0545</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65_progressive</th>    <td>0.0412</td>   <td>0.0412</td>   <td>0.9986</td>  <td>0.3180</td> <td>-0.0396</td> <td>0.1220</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>                <td>0.0570</td>   <td>0.0219</td>   <td>2.6015</td>  <td>0.0093</td> <td>0.0140</td>  <td>0.0999</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>                <td>-0.0204</td>  <td>0.0140</td>   <td>-1.4581</td> <td>0.1448</td> <td>-0.0478</td> <td>0.0070</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>                <td>0.0378</td>   <td>0.0396</td>   <td>0.9534</td>  <td>0.3404</td> <td>-0.0399</td> <td>0.1154</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>                <td>-0.0040</td>  <td>0.0281</td>   <td>-0.1433</td> <td>0.8861</td> <td>-0.0591</td> <td>0.0511</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>                 <td>-0.7561</td>  <td>0.0166</td>  <td>-45.4988</td> <td>0.0000</td> <td>-0.7887</td> <td>-0.7236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>                <td>-0.1241</td>  <td>0.1118</td>   <td>-1.1097</td> <td>0.2671</td> <td>-0.3433</td> <td>0.0951</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>                <td>-0.0273</td>  <td>0.0364</td>   <td>-0.7515</td> <td>0.4524</td> <td>-0.0986</td> <td>0.0440</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>                <td>-0.0061</td>  <td>0.0623</td>   <td>-0.0972</td> <td>0.9226</td> <td>-0.1282</td> <td>0.1161</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>                <td>-0.0944</td>  <td>0.0455</td>   <td>-2.0754</td> <td>0.0380</td> <td>-0.1836</td> <td>-0.0053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>                <td>0.0391</td>   <td>0.0162</td>   <td>2.4084</td>  <td>0.0160</td> <td>0.0073</td>  <td>0.0709</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>                <td>-0.0017</td>  <td>0.0104</td>   <td>-0.1645</td> <td>0.8694</td> <td>-0.0220</td> <td>0.0186</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>                <td>0.0089</td>   <td>0.0284</td>   <td>0.3134</td>  <td>0.7540</td> <td>-0.0467</td> <td>0.0645</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77_buick</th>          <td>-0.0650</td>  <td>0.1070</td>   <td>-0.6070</td> <td>0.5438</td> <td>-0.2747</td> <td>0.1448</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77_chevrolet</th>      <td>-0.0466</td>  <td>0.0667</td>   <td>-0.6988</td> <td>0.4847</td> <td>-0.1774</td> <td>0.0842</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77_ford</th>           <td>-0.0210</td>  <td>0.0599</td>   <td>-0.3499</td> <td>0.7264</td> <td>-0.1385</td> <td>0.0965</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77_mercedes</th>       <td>-0.0942</td>  <td>0.0686</td>   <td>-1.3720</td> <td>0.1701</td> <td>-0.2287</td> <td>0.0403</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77_missing</th>        <td>-0.0542</td>  <td>0.0598</td>   <td>-0.9065</td> <td>0.3647</td> <td>-0.1715</td> <td>0.0630</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77_nissan</th>         <td>0.0138</td>   <td>0.0777</td>   <td>0.1775</td>  <td>0.8591</td> <td>-0.1385</td> <td>0.1661</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77_subaru</th>         <td>-0.0391</td>  <td>0.0663</td>   <td>-0.5896</td> <td>0.5554</td> <td>-0.1691</td> <td>0.0909</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>                <td>0.0042</td>   <td>0.0235</td>   <td>0.1775</td>  <td>0.8591</td> <td>-0.0419</td> <td>0.0503</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>                <td>0.0184</td>   <td>0.0508</td>   <td>0.3629</td>  <td>0.7167</td> <td>-0.0811</td> <td>0.1179</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>                 <td>0.0974</td>   <td>0.0243</td>   <td>4.0087</td>  <td>0.0001</td> <td>0.0498</td>  <td>0.1451</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>                <td>0.0151</td>   <td>0.0422</td>   <td>0.3574</td>  <td>0.7208</td> <td>-0.0677</td> <td>0.0979</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>                <td>-0.0061</td>  <td>0.0184</td>   <td>-0.3295</td> <td>0.7418</td> <td>-0.0422</td> <td>0.0300</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>                <td>-0.2545</td>  <td>0.0616</td>   <td>-4.1341</td> <td>0.0000</td> <td>-0.3751</td> <td>-0.1338</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x83</th>                <td>0.0642</td>   <td>0.0432</td>   <td>1.4880</td>  <td>0.1367</td> <td>-0.0204</td> <td>0.1489</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x84</th>                <td>-0.0228</td>  <td>0.0206</td>   <td>-1.1068</td> <td>0.2684</td> <td>-0.0632</td> <td>0.0176</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x85</th>                <td>0.0160</td>   <td>0.0266</td>   <td>0.6032</td>  <td>0.5464</td> <td>-0.0361</td> <td>0.0681</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x86</th>                <td>0.0911</td>   <td>0.0560</td>   <td>1.6252</td>  <td>0.1041</td> <td>-0.0188</td> <td>0.2009</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x87</th>                <td>-0.0527</td>  <td>0.0216</td>   <td>-2.4401</td> <td>0.0147</td> <td>-0.0951</td> <td>-0.0104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x88</th>                <td>-0.0008</td>  <td>0.0172</td>   <td>-0.0479</td> <td>0.9618</td> <td>-0.0345</td> <td>0.0328</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x89</th>                <td>-0.0887</td>  <td>0.0183</td>   <td>-4.8320</td> <td>0.0000</td> <td>-0.1246</td> <td>-0.0527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>                 <td>-0.0114</td>  <td>0.0382</td>   <td>-0.2980</td> <td>0.7657</td> <td>-0.0862</td> <td>0.0634</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x90</th>                <td>0.0572</td>   <td>0.0630</td>   <td>0.9092</td>  <td>0.3633</td> <td>-0.0662</td> <td>0.1807</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x91</th>                <td>0.0209</td>   <td>0.0174</td>   <td>1.2004</td>  <td>0.2300</td> <td>-0.0132</td> <td>0.0551</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x92</th>                <td>0.0204</td>   <td>0.0216</td>   <td>0.9433</td>  <td>0.3455</td> <td>-0.0220</td> <td>0.0627</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x93</th>                <td>-0.7493</td>  <td>0.0720</td>  <td>-10.4113</td> <td>0.0000</td> <td>-0.8904</td> <td>-0.6083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x94</th>                <td>-0.0533</td>  <td>0.0389</td>   <td>-1.3698</td> <td>0.1707</td> <td>-0.1295</td> <td>0.0230</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x95</th>                <td>-0.0215</td>  <td>0.0144</td>   <td>-1.4916</td> <td>0.1358</td> <td>-0.0498</td> <td>0.0068</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x96</th>                <td>-0.0289</td>  <td>0.0232</td>   <td>-1.2459</td> <td>0.2128</td> <td>-0.0744</td> <td>0.0166</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x97</th>                <td>-0.0303</td>  <td>0.0342</td>   <td>-0.8847</td> <td>0.3763</td> <td>-0.0973</td> <td>0.0368</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x98</th>                <td>-0.0627</td>  <td>0.0306</td>   <td>-2.0480</td> <td>0.0406</td> <td>-0.1228</td> <td>-0.0027</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "===================================================================\n",
       "Model:                Logit            Pseudo R-squared: 0.150     \n",
       "Dependent Variable:   y                AIC:              28506.8079\n",
       "Date:                 2023-03-11 22:43 BIC:              29942.4459\n",
       "No. Observations:     40000            Log-Likelihood:   -14086.   \n",
       "Df Model:             166              LL-Null:          -16563.   \n",
       "Df Residuals:         39833            LLR p-value:      0.0000    \n",
       "Converged:            0.0000           Scale:            1.0000    \n",
       "No. Iterations:       100.0000                                     \n",
       "-------------------------------------------------------------------\n",
       "                    Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
       "-------------------------------------------------------------------\n",
       "Intercept          -1.9579   0.1381 -14.1794 0.0000 -2.2285 -1.6872\n",
       "x1                  0.0297   0.0223   1.3348 0.1819 -0.0139  0.0733\n",
       "x10                -0.0054   0.0194  -0.2811 0.7787 -0.0434  0.0325\n",
       "x100                0.0384   0.0511   0.7520 0.4520 -0.0617  0.1386\n",
       "x11                 0.1700   0.0403   4.2143 0.0000  0.0909  0.2490\n",
       "x12                -0.0257   0.0765  -0.3360 0.7368 -0.1757  0.1243\n",
       "x13                 0.0591   0.0371   1.5919 0.1114 -0.0137  0.1318\n",
       "x14                 0.0432   0.0240   1.8024 0.0715 -0.0038  0.0902\n",
       "x15                -0.0345   0.0155  -2.2212 0.0263 -0.0649 -0.0041\n",
       "x16                -0.0362   0.0205  -1.7638 0.0778 -0.0764  0.0040\n",
       "x17                 0.0223   0.0405   0.5516 0.5812 -0.0570  0.1017\n",
       "x18                -0.2377   0.0798  -2.9774 0.0029 -0.3941 -0.0812\n",
       "x19                -0.1965   0.0207  -9.4929 0.0000 -0.2371 -0.1560\n",
       "x2                  0.1057   0.0804   1.3146 0.1887 -0.0519  0.2632\n",
       "x20                -0.1279   0.0537  -2.3822 0.0172 -0.2332 -0.0227\n",
       "x21                 0.0159   0.0153   1.0367 0.2999 -0.0141  0.0458\n",
       "x22                -0.0025   0.0194  -0.1300 0.8966 -0.0405  0.0355\n",
       "x23                 0.0061   0.0175   0.3468 0.7288 -0.0283  0.0404\n",
       "x24                -0.0036   0.0306  -0.1164 0.9074 -0.0636  0.0564\n",
       "x25                 0.0187   0.0215   0.8721 0.3831 -0.0233  0.0608\n",
       "x26                -0.0055   0.0189  -0.2896 0.7721 -0.0426  0.0316\n",
       "x27                 0.1243   0.0523   2.3787 0.0174  0.0219  0.2267\n",
       "x28                -0.3474   0.0717  -4.8463 0.0000 -0.4879 -0.2069\n",
       "x29                -0.0102   0.0145  -0.7058 0.4803 -0.0387  0.0182\n",
       "x31                -0.6182   0.0596 -10.3674 0.0000 -0.7350 -0.5013\n",
       "x32                 0.0067   0.0108   0.6175 0.5369 -0.0145  0.0279\n",
       "x33_Alabama        -0.1890   0.1369  -1.3801 0.1675 -0.4574  0.0794\n",
       "x33_Alaska          0.6814   0.1839   3.7059 0.0002  0.3210  1.0417\n",
       "x33_Arizona        -0.2409   0.1261  -1.9105 0.0561 -0.4880  0.0062\n",
       "x33_Arkansas       -0.2306   0.1735  -1.3291 0.1838 -0.5707  0.1095\n",
       "x33_Colorado       -0.3594   0.1429  -2.5152 0.0119 -0.6395 -0.0793\n",
       "x33_Connecticut    -0.1301   0.1500  -0.8671 0.3859 -0.4242  0.1640\n",
       "x33_DC             -0.0012   0.2128  -0.0054 0.9957 -0.4183  0.4160\n",
       "x33_Delaware       -0.3200   0.2485  -1.2880 0.1977 -0.8071  0.1670\n",
       "x33_Florida         0.2931   0.0775   3.7833 0.0002  0.1413  0.4450\n",
       "x33_Georgia         0.3543   0.0973   3.6437 0.0003  0.1637  0.5450\n",
       "x33_Hawaii          0.0103   0.1707   0.0606 0.9517 -0.3242  0.3449\n",
       "x33_Idaho           0.7955   0.1456   5.4631 0.0000  0.5101  1.0810\n",
       "x33_Illinois        0.3201   0.0868   3.6871 0.0002  0.1500  0.4903\n",
       "x33_Indiana         0.2469   0.1097   2.2514 0.0244  0.0320  0.4619\n",
       "x33_Iowa            0.2301   0.1482   1.5522 0.1206 -0.0604  0.5207\n",
       "x33_Kansas          0.5476   0.1343   4.0788 0.0000  0.2845  0.8108\n",
       "x33_Kentucky        0.3124   0.1285   2.4306 0.0151  0.0605  0.5644\n",
       "x33_Louisiana      -0.2795   0.1451  -1.9259 0.0541 -0.5639  0.0049\n",
       "x33_Maine           0.1040   0.1809   0.5749 0.5654 -0.2506  0.4586\n",
       "x33_Maryland       -0.2912   0.1358  -2.1442 0.0320 -0.5574 -0.0250\n",
       "x33_Massachusetts  -0.1929   0.1276  -1.5115 0.1307 -0.4429  0.0572\n",
       "x33_Michigan       -0.2792   0.1073  -2.6034 0.0092 -0.4895 -0.0690\n",
       "x33_Minnesota      -0.1984   0.1271  -1.5602 0.1187 -0.4475  0.0508\n",
       "x33_Mississippi    -0.1800   0.1610  -1.1180 0.2636 -0.4956  0.1356\n",
       "x33_Missouri        0.2149   0.1167   1.8413 0.0656 -0.0139  0.4437\n",
       "x33_Montana         0.4187   0.1901   2.2030 0.0276  0.0462  0.7913\n",
       "x33_Nebraska        0.0715   0.1597   0.4481 0.6541 -0.2414  0.3845\n",
       "x33_Nevada         -0.0823   0.1530  -0.5383 0.5904 -0.3821  0.2175\n",
       "x33_New_Hampshire  -0.0530   0.1912  -0.2774 0.7815 -0.4278  0.3217\n",
       "x33_New_Jersey     -0.2102   0.1133  -1.8546 0.0637 -0.4324  0.0119\n",
       "x33_New_Mexico     -0.1120   0.1630  -0.6870 0.4921 -0.4315  0.2075\n",
       "x33_New_York       -0.0292   0.0835  -0.3498 0.7265 -0.1929  0.1345\n",
       "x33_North_Carolina  0.5060   0.0945   5.3536 0.0000  0.3207  0.6912\n",
       "x33_North_Dakota   -0.2209   0.2290  -0.9648 0.3346 -0.6697  0.2279\n",
       "x33_Ohio            0.2646   0.0921   2.8719 0.0041  0.0840  0.4452\n",
       "x33_Oklahoma       -0.2414   0.1584  -1.5233 0.1277 -0.5519  0.0692\n",
       "x33_Oregon          1.0426   0.1159   8.9964 0.0000  0.8154  1.2697\n",
       "x33_Pennsylvania   -0.1710   0.0963  -1.7751 0.0759 -0.3598  0.0178\n",
       "x33_Rhode_Island   -0.1327   0.1874  -0.7081 0.4789 -0.5001  0.2347\n",
       "x33_South_Carolina  0.3071   0.1283   2.3938 0.0167  0.0557  0.5586\n",
       "x33_South_Dakota    0.0479   0.2005   0.2389 0.8112 -0.3451  0.4409\n",
       "x33_Tennessee      -0.3413   0.1279  -2.6693 0.0076 -0.5920 -0.0907\n",
       "x33_Texas          -0.0501   0.0769  -0.6521 0.5143 -0.2008  0.1006\n",
       "x33_Utah           -0.1938   0.1691  -1.1459 0.2519 -0.5253  0.1377\n",
       "x33_Vermont         0.1675   0.1983   0.8447 0.3983 -0.2212  0.5562\n",
       "x33_Virginia       -0.4116   0.1255  -3.2811 0.0010 -0.6575 -0.1657\n",
       "x33_Washington      0.8845   0.0973   9.0926 0.0000  0.6939  1.0752\n",
       "x33_West_Virginia  -0.1058   0.1724  -0.6135 0.5395 -0.4437  0.2321\n",
       "x33_Wisconsin      -0.1975   0.1316  -1.5006 0.1335 -0.4554  0.0605\n",
       "x33_Wyoming        -0.3258   0.2488  -1.3096 0.1903 -0.8134  0.1618\n",
       "x34                -0.0945   0.0639  -1.4790 0.1391 -0.2198  0.0307\n",
       "x35                 0.0135   0.0185   0.7331 0.4635 -0.0227  0.0497\n",
       "x36                -0.3340   0.0758  -4.4088 0.0000 -0.4825 -0.1855\n",
       "x37                 0.2400   0.0712   3.3718 0.0007  0.1005  0.3795\n",
       "x38                -0.0201   0.0193  -1.0386 0.2990 -0.0579  0.0178\n",
       "x3_Fri              0.2330   0.0552   4.2212 0.0000  0.1248  0.3412\n",
       "x3_Sat              0.2109   0.0559   3.7724 0.0002  0.1013  0.3204\n",
       "x3_Sun              0.2360   0.0584   4.0396 0.0001  0.1215  0.3504\n",
       "x3_Thur             0.0060   0.0604   0.0997 0.9206 -0.1124  0.1245\n",
       "x3_Tue             -0.2514   0.0559  -4.4930 0.0000 -0.3610 -0.1417\n",
       "x3_Wed             -0.0455   0.0541  -0.8401 0.4008 -0.1515  0.0606\n",
       "x4                  0.0010   0.0140   0.0705 0.9438 -0.0265  0.0285\n",
       "x40                -0.3072   0.0659  -4.6648 0.0000 -0.4363 -0.1781\n",
       "x41                -0.0716   0.0277  -2.5842 0.0098 -0.1260 -0.0173\n",
       "x42                 0.0091   0.0172   0.5278 0.5976 -0.0246  0.0428\n",
       "x43                -0.0060   0.0166  -0.3601 0.7188 -0.0385  0.0266\n",
       "x45                 0.0068   0.0164   0.4178 0.6761 -0.0253  0.0390\n",
       "x46                -0.0304   0.0169  -1.7953 0.0726 -0.0635  0.0028\n",
       "x47                -0.8703   0.0629 -13.8252 0.0000 -0.9937 -0.7469\n",
       "x48                 0.0318   0.0850   0.3739 0.7085 -0.1349  0.1985\n",
       "x49                 0.0069   0.0218   0.3141 0.7535 -0.0359  0.0496\n",
       "x5                  0.0504   0.0177   2.8446 0.0044  0.0157  0.0851\n",
       "x50                -0.0644   0.0579  -1.1118 0.2662 -0.1780  0.0491\n",
       "x51                 0.0448   0.0105   4.2562 0.0000  0.0242  0.0654\n",
       "x53                -0.1331   0.0688  -1.9351 0.0530 -0.2680  0.0017\n",
       "x54                 0.0373   0.0164   2.2765 0.0228  0.0052  0.0694\n",
       "x56                -0.0069   0.0266  -0.2603 0.7946 -0.0590  0.0452\n",
       "x58                 0.0003   0.0005   0.7201 0.4715 -0.0006  0.0012\n",
       "x59                -0.0178   0.0513  -0.3465 0.7290 -0.1184  0.0828\n",
       "x6                 -0.0186   0.0179  -1.0426 0.2971 -0.0537  0.0164\n",
       "x60_April           0.0405   0.1048   0.3865 0.6992 -0.1649  0.2459\n",
       "x60_December       -0.0540   0.0485  -1.1145 0.2651 -0.1491  0.0410\n",
       "x60_February       -0.0062   0.0940  -0.0662 0.9472 -0.1904  0.1780\n",
       "x60_January         0.0121   0.0482   0.2508 0.8020 -0.0824  0.1066\n",
       "x60_July           -0.0386   0.0487  -0.7913 0.4288 -0.1341  0.0570\n",
       "x60_June           -0.1578   0.0946  -1.6682 0.0953 -0.3433  0.0276\n",
       "x60_March           0.1646   0.1092   1.5075 0.1317 -0.0494  0.3786\n",
       "x60_May            -0.2449   0.1224  -2.0004 0.0455 -0.4849 -0.0049\n",
       "x60_November        0.0978   0.0989   0.9889 0.3227 -0.0960  0.2915\n",
       "x60_October         0.0987   0.1116   0.8849 0.3762 -0.1199  0.3174\n",
       "x60_September       0.0663   0.0913   0.7258 0.4680 -0.1127  0.2453\n",
       "x61                 0.2946   0.0436   6.7633 0.0000  0.2092  0.3800\n",
       "x62                -0.1494   0.0491  -3.0412 0.0024 -0.2456 -0.0531\n",
       "x63                 0.0098   0.0143   0.6822 0.4951 -0.0183  0.0378\n",
       "x64                -0.1065   0.0403  -2.6434 0.0082 -0.1855 -0.0275\n",
       "x65_esurance       -0.0100   0.0466  -0.2153 0.8295 -0.1014  0.0813\n",
       "x65_farmers        -0.0906   0.0512  -1.7694 0.0768 -0.1910  0.0098\n",
       "x65_geico          -0.0449   0.0507  -0.8851 0.3761 -0.1443  0.0545\n",
       "x65_progressive     0.0412   0.0412   0.9986 0.3180 -0.0396  0.1220\n",
       "x66                 0.0570   0.0219   2.6015 0.0093  0.0140  0.0999\n",
       "x67                -0.0204   0.0140  -1.4581 0.1448 -0.0478  0.0070\n",
       "x68                 0.0378   0.0396   0.9534 0.3404 -0.0399  0.1154\n",
       "x69                -0.0040   0.0281  -0.1433 0.8861 -0.0591  0.0511\n",
       "x7                 -0.7561   0.0166 -45.4988 0.0000 -0.7887 -0.7236\n",
       "x70                -0.1241   0.1118  -1.1097 0.2671 -0.3433  0.0951\n",
       "x71                -0.0273   0.0364  -0.7515 0.4524 -0.0986  0.0440\n",
       "x72                -0.0061   0.0623  -0.0972 0.9226 -0.1282  0.1161\n",
       "x73                -0.0944   0.0455  -2.0754 0.0380 -0.1836 -0.0053\n",
       "x74                 0.0391   0.0162   2.4084 0.0160  0.0073  0.0709\n",
       "x75                -0.0017   0.0104  -0.1645 0.8694 -0.0220  0.0186\n",
       "x76                 0.0089   0.0284   0.3134 0.7540 -0.0467  0.0645\n",
       "x77_buick          -0.0650   0.1070  -0.6070 0.5438 -0.2747  0.1448\n",
       "x77_chevrolet      -0.0466   0.0667  -0.6988 0.4847 -0.1774  0.0842\n",
       "x77_ford           -0.0210   0.0599  -0.3499 0.7264 -0.1385  0.0965\n",
       "x77_mercedes       -0.0942   0.0686  -1.3720 0.1701 -0.2287  0.0403\n",
       "x77_missing        -0.0542   0.0598  -0.9065 0.3647 -0.1715  0.0630\n",
       "x77_nissan          0.0138   0.0777   0.1775 0.8591 -0.1385  0.1661\n",
       "x77_subaru         -0.0391   0.0663  -0.5896 0.5554 -0.1691  0.0909\n",
       "x78                 0.0042   0.0235   0.1775 0.8591 -0.0419  0.0503\n",
       "x79                 0.0184   0.0508   0.3629 0.7167 -0.0811  0.1179\n",
       "x8                  0.0974   0.0243   4.0087 0.0001  0.0498  0.1451\n",
       "x80                 0.0151   0.0422   0.3574 0.7208 -0.0677  0.0979\n",
       "x81                -0.0061   0.0184  -0.3295 0.7418 -0.0422  0.0300\n",
       "x82                -0.2545   0.0616  -4.1341 0.0000 -0.3751 -0.1338\n",
       "x83                 0.0642   0.0432   1.4880 0.1367 -0.0204  0.1489\n",
       "x84                -0.0228   0.0206  -1.1068 0.2684 -0.0632  0.0176\n",
       "x85                 0.0160   0.0266   0.6032 0.5464 -0.0361  0.0681\n",
       "x86                 0.0911   0.0560   1.6252 0.1041 -0.0188  0.2009\n",
       "x87                -0.0527   0.0216  -2.4401 0.0147 -0.0951 -0.0104\n",
       "x88                -0.0008   0.0172  -0.0479 0.9618 -0.0345  0.0328\n",
       "x89                -0.0887   0.0183  -4.8320 0.0000 -0.1246 -0.0527\n",
       "x9                 -0.0114   0.0382  -0.2980 0.7657 -0.0862  0.0634\n",
       "x90                 0.0572   0.0630   0.9092 0.3633 -0.0662  0.1807\n",
       "x91                 0.0209   0.0174   1.2004 0.2300 -0.0132  0.0551\n",
       "x92                 0.0204   0.0216   0.9433 0.3455 -0.0220  0.0627\n",
       "x93                -0.7493   0.0720 -10.4113 0.0000 -0.8904 -0.6083\n",
       "x94                -0.0533   0.0389  -1.3698 0.1707 -0.1295  0.0230\n",
       "x95                -0.0215   0.0144  -1.4916 0.1358 -0.0498  0.0068\n",
       "x96                -0.0289   0.0232  -1.2459 0.2128 -0.0744  0.0166\n",
       "x97                -0.0303   0.0342  -0.8847 0.3763 -0.0973  0.0368\n",
       "x98                -0.0627   0.0306  -2.0480 0.0406 -0.1228 -0.0027\n",
       "===================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.formula.api import logit\n",
    "import cvxopt\n",
    "\n",
    "\n",
    "def normalise_column_names(df):\n",
    "    for col in list(df.columns):\n",
    "        if col == \"y\":\n",
    "            continue\n",
    "\n",
    "        if \".\" in col or \"-\" in col or \" \" in col:\n",
    "            # print('Replacing col ', col, col.replace(\".\",\"_\").replace(\"-\",\"_\"))\n",
    "            df.rename(columns={col: col.replace(\".\", \"_\").replace(\"-\", \"_\").replace(\" \", \"_\")}, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_df_n_formulae(df, columns, class_label=\"y\"):\n",
    "    df = df[columns]\n",
    "    df = normalise_column_names(df)\n",
    "    normalised_column_names = sorted(list(df.columns))\n",
    "\n",
    "    if class_label in normalised_column_names:\n",
    "        normalised_column_names.remove(class_label)\n",
    "\n",
    "    return df, class_label + \" ~ \" + \" + \".join(normalised_column_names)\n",
    "\n",
    "# reference_columns_to_remove = ['education_illiterate', 'job_unemployed','month_dec', 'marital_single', 'day_of_week_fri', 'default_miss' ]\n",
    "reference_columns_to_remove = [\n",
    "        \"x77_toyota\",\n",
    "        \"x33_California\",\n",
    "        \"x3_Mon\",\n",
    "        \"x60_August\",\n",
    "        \"x65_allstate\"\n",
    "]\n",
    "base_explainable_cols = list(train_df.columns)\n",
    "explainable_cols = list(set(base_explainable_cols) - set(reference_columns_to_remove))\n",
    "\n",
    "explainable_data_df, explainable_formulae = get_df_n_formulae(train_df, explainable_cols, class_label=\"y\")\n",
    "\n",
    "\n",
    "logit_model_explainable_variables = logit(explainable_formulae, explainable_data_df).fit_regularized(\n",
    "    maxiter=100, method=\"l1\", trim_mode=\"size\", size_trim_tol=\"auto\", auto_trim_tol=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "lr = logit_model_explainable_variables\n",
    "# marginal_effect = logit_model_explainable_variables.get_margeff()\n",
    "# marginal_effect.summary()\n",
    "logit_model_explainable_variables.summary2()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 LR Model Confusion Matrix : Diff Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix across different threshold\n",
      "\n",
      "Threshold=0.5   \n",
      "[[33769.   428.]\n",
      " [ 5178.   625.]]\n",
      "\n",
      "Threshold=0.4   \n",
      "[[33113.  1084.]\n",
      " [ 4607.  1196.]]\n",
      "\n",
      "Threshold=0.3   \n",
      "[[31439.  2758.]\n",
      " [ 3722.  2081.]]\n",
      "\n",
      "Threshold=0.2   \n",
      "[[27561.  6636.]\n",
      " [ 2463.  3340.]]\n",
      "\n",
      "Threshold=0.1   \n",
      "[[18287. 15910.]\n",
      " [  986.  4817.]]\n"
     ]
    }
   ],
   "source": [
    "best_lr = logit_model_explainable_variables\n",
    "print(\"Confusion Matrix across different threshold\")\n",
    "print(f\"\\nThreshold=0.5   \\n{best_lr.pred_table(0.5)}\")\n",
    "print(f\"\\nThreshold=0.4   \\n{best_lr.pred_table(0.4)}\")\n",
    "print(f\"\\nThreshold=0.3   \\n{best_lr.pred_table(0.3)}\")\n",
    "print(f\"\\nThreshold=0.2   \\n{best_lr.pred_table(0.2)}\")\n",
    "print(f\"\\nThreshold=0.1   \\n{best_lr.pred_table(0.1)}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Future To Dos: Logistic Regression\n",
    "- Variable slicing for coeficients with larger p-value\n",
    "- Add in interaction terms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a45166dac615e3fa4b0b281861ed35480db8037454d5e577e97579f5959b09c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

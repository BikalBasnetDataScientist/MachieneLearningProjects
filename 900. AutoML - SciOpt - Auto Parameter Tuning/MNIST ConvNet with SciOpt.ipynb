{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with SciOpt : MNIST\n",
    "=============\n",
    "\n",
    "Task : \n",
    "\n",
    "1. Construct a classification model on the MNIST data set in the form of a deep neural network of your choice.\n",
    "\n",
    "2. Perform and compare two or more methods of hyperparameter optimization on this model, and comment on the comparison.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 :  Perform and compare  hyperparameter optimization\n",
    "\n",
    "### 4.1 Grid Search : 350 years\n",
    "- Because of the exhaustiveness Grid Search was not a feasible option at all for our case.\n",
    "- Our case below : 137 million possible parameter combination set. (4*7*3*6*5*3*65*140*2)\n",
    "- Total time taken @ 80s for 1 parameter set : 349.04 years (4*7*3*6*5*3*65*140*2)/(60.0*60.0*24.0*365) * 80.0 # years-\n",
    "- PS : 80s training duration as observed in training of one parameter set with fitting function for Scikit-Optimise.\n",
    "\n",
    "\n",
    "### 4.2 Random Search : \n",
    "**Performance** :In our case, the scikit-optimise library' dummy  random optimise in a 4 core 2.5GHz CPU with 6 GB RAM, ran for 20 mins and yielded 98.0% accuracy which is above the manually  implemented 96.7% accuracy.\n",
    "**Basic Info**: It dramatically reduce search space, but the probability of getting to the globally optimal hyperparameter reduces to almost zero  with addition of each hyperparameter. \n",
    "- e.g What is the probability of getting the 1 best hyperparameters set from 137,592,000  parameter set = 1 / 137,592,000\n",
    "However it is more effective in practice.\n",
    "- **Cons** : The biggest cons of the random search is that when it find the approximate optimal hyperparameter values, it is not able to gain cues from it,  to further optimise the parameters.\n",
    "- **Pros** : However one of the  advantage is that because  other optimisation tehcniqques are so expensiveness  in terms of time and computation, when no  good default hyper parameter estimate is avaialable, it can be used to find the approximate good default hyperparameter configuration set. This hyperparameter then can be used as the default parameter setting for later Bayesian optmisation and  automated optimisation algorithms.\n",
    "\n",
    "### 4.3 Bayesian Optimization\n",
    "**Performance** : In our case, the scikit-optimise library's Bayesian optimization algorithm in a 4 core 2.5GHz CPU with 6 GB RAM, ran for 1.5 hours and 98.2% accuracy which is above the manually  implemented 96.7% accuracy and 97.6% accuraccy of evolutionary algorithm.\n",
    "\n",
    "**Implementation**:In this project, we  used scikit-optimise library's Bayesian optimization (Gaussian process) because of its compatibility with  tensorflow, keras making it suitable for future scalable , massively parallelised hyper-parameter optimisation approach.\n",
    "For cases of neural networks with large no of  hyper-parameters and where function evaluation is the mean  cross-validated score across multiple folds, gaussian prior is much quicker to evaluate, leading to faster hyperparameter next set choice. And hence we have used the Bayesian optimisation with Gaussian process.\n",
    "\n",
    "** Basic Info **\n",
    "- Bayesian optimization  is based on Bayes theorem. Bayes theorem uses the prior probability i.e what we know  already to deduce the posterior probability. \n",
    "- Correlating this  with the HyperParameter optimization, the next best hyper-parameter set are the unknowns, which are then determined by what we know already i.e earlier iterations of hyper-parameter configuration and the  evaluation(accuracy in our case). \n",
    "- Bayesian optimization as such is well suited for functions that are very expensive to evaluate. In our particular case, we observed that  tuning just 9 hyper-parameters meant that we had 140 million hyper-parameter set which is very very exhaustive and expensive.\n",
    "-  They reduce the expensive operation by trading off exploration for exploitation so as to minimize the no. of  queries. Number of acquisition functions such as Probability of improvement, Bayessian Expected Loss(EL), upper/lower confidence bounds, Thompson sampling or mix of these exists. These are the functions that we try to minimize with each parameter iteration.\n",
    "\n",
    "Other options that exists are,\n",
    "1. BayesSearchCV - similar to grid search but  more efficient in that only a fixed number of parameter set is sampled from specific distribution.\n",
    "2. dummy_minimize - random search by uniform sampling\n",
    "3. forest_minimize -\n",
    "4. gbrt_minimize : used for highly expensive evaluation function.\n",
    "\n",
    "### 4.4 Evolutionary Optimisation :\n",
    "**Performacce** : In our case, the genetic algorithm in a 4 core 2.5GHz CPU with 6 GB RAM, ran for 6 hours and yielded the Convolution neural network with following architecture with 97.7% accuracy which is slightly above the manually  implemented 96.7% accuracy. The fact that we clipped the hyperparameters(nodes specifically) and evolution parameters( child population and the generation) may have been influential to not reach state of art performance.\n",
    "**Implementation** : Please see file \"MNIST with Genetic Algorithm.html\" for its detailed implementation\n",
    "\n",
    "**Basic Info**\n",
    "The evolutionary optimisation use the biological evolution concept inspired optimisation for hyperparameters. \n",
    "The general evolution concept works  on a parent child architecture as\n",
    "1. Parents (1st generation) reproduce i.e generate n initial population(random tuples of hyperparameters in our case)\n",
    "2. Parent evaluate fitness of each child (fitness function)\n",
    "3. Select best fit child for reproduction (Parents )\n",
    "4. Generate next generation childs with breeding(cross over, mutation of best hyper parameters of 1st generation or leading parent)\n",
    "5. Evaluate and replace least fit with new child (low performing hyperparam replacement)\n",
    "6. Repeat 3 through 5, until stopping criteria(generation elapses, target eval reached)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimiser Performance Comment\n",
    "\n",
    "Both of the optimiser's performance are well below the state of art performance of above 99.5%. This should be considered in the light that we provided limited search space because of the limited run time.\n",
    "The next interesting aspect would be to  provide much richer search space for both algorithms at the expense of more training time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Keras Imports\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import InputLayer, Input, Reshape, MaxPooling2D, Conv2D, Dense, Flatten,Dropout \n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.optimizers import Adam, SGD\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "# SkOpt Bayesian Hyperparameter optimisation imports\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "#from skopt.plots import plot_convergence\n",
    "#from skopt.plots import plot_objective, plot_evaluations\n",
    "#from skopt.plots import plot_histogram, plot_objective_2D\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 :  Data  Load\n",
    "\n",
    "MNIST Data as downloaded from  tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-6c2e94c86b1a>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Change Load Data format\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('data/MNIST/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set:\t\t55000\n",
      "- Test-set:\t\t10000\n",
      "- Validation-set:\t5000\n"
     ]
    }
   ],
   "source": [
    "# Reshape to  this\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(data.test.labels)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(data.validation.labels)))\n",
    "\n",
    "# One hot Encoding\n",
    "data.test.cls = np.argmax(data.test.labels, axis=1)\n",
    "validation_data = (data.validation.images, data.validation.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 28 # We know that MNIST images are 28 pixels in each dimension.\n",
    "num_channels = 1 # greyscale images hence 1 channel\n",
    "num_classes = 10 # for 10 digits from 0 to 9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Keras ConvNet  \n",
    "\n",
    "###  2 (Conv  Layer + MaxPool + Dropout ) + 2 Fc  Layer\n",
    "\n",
    "Create a simple keras convnet MNIST digit classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 179s 3ms/step - loss: 0.3207 - acc: 0.9032 - val_loss: 0.1096 - val_acc: 0.9672\n",
      "ConvNet model accuracy is :  0.9671999945640564\n"
     ]
    }
   ],
   "source": [
    "dropout_values = 0.2\n",
    "activation = 'relu'\n",
    "batch_sizes = 20\n",
    "\n",
    "# Model Creation\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(img_size*img_size,))) # accepts flattened images    \n",
    "# 784 to  (28, 28, 1) Conv layers expected shape\n",
    "model.add(Reshape((img_size, img_size, 1)))\n",
    "\n",
    "model.add(Conv2D(kernel_size=5, strides=1, filters=36, padding='same',activation=activation, name='layer_conv1'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Dropout(dropout_values))\n",
    "model.add(Conv2D(kernel_size=5, strides=1, filters=36, padding='same',activation=activation, name='layer_conv2'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Dropout(dropout_values))\n",
    "\n",
    "# Flatten  to feed to fully connected layer i.e dense layer in keras\n",
    "model.add(Flatten())    \n",
    "model.add(Dense(64,activation='relu',name='fc_layer_0'))\n",
    "model.add(Dropout(dropout_values))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "optimizer = Adam(lr=0.01)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "convnet = model.fit(x=data.train.images,  y=data.train.labels,  epochs=1,\n",
    "                        batch_size=batch_sizes, validation_data=validation_data)#,callbacks=[callback_log])\n",
    "accuracy = convnet.history['val_acc'][-1]\n",
    "print('ConvNet model accuracy is : ',accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 HyperParameter Optimisation\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Important Hyperparameters Identification\n",
    "\n",
    "Not all hyper parameters have equal significance. And since with each addition of a hyperparameter, the hyperparameter search space grows exponentially, it would be computationally and time exhaustive  to search through all the hyper parameters available. \n",
    "\n",
    "Hence it is much more better to  identify important  hyperparameters, to reduce the search space.\n",
    "\n",
    "1. Learning rate\n",
    "2. Conv Layers\n",
    "3. Patch size\n",
    "4. max_pool size\n",
    "5. hidden layers\n",
    "6. hidden nodes\n",
    "7. batch\n",
    "8. optimiser\n",
    "9. dropout values\n",
    "\n",
    "While there are other important hyperparameters too, we will limit ourselves to these for this project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 HyperParameter Boundary\n",
    "\n",
    "Define HyperParameter boundary to search within. It is not feassible to search in the infinite boundary space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will search over k only for 1e-k i.e 0.01, 0.001, 0.0001, 0.00001 , because we specified log-uniform\n",
    "dim_learning_rate = Real(low=1e-5, high=1e-2, prior='log-uniform',  name='learning_rate') \n",
    "dim_dropout_values = Real(low=0.1, high=0.7 , prior='uniform', name='dropout_values') \n",
    "dim_num_conv_layers = Integer(low=2, high=4, name='num_conv_layers')\n",
    "dim_patch_sizes = Integer(low=2, high=7, name='patch_sizes')\n",
    "dim_max_pool_sizes = Integer(low=1, high=4, name='max_pool_sizes')\n",
    "dim_num_dense_layers = Integer(low=2, high=5, name='num_dense_layers')\n",
    "dim_num_hidden_nodes = Integer(low=64, high=1280, name='num_hidden_nodes')\n",
    "dim_batch_sizes = Integer(low=10, high=150, name='batch_sizes')\n",
    "dim_optimisers = Categorical(categories=['Adam','SGD'],    name='optimisers')\n",
    "#dim_activation = Categorical(categories=['relu', 'sigmoid'],    name='activation')\n",
    "\n",
    "# Combine boundaries, to be fed as single boundary list\n",
    "dimensions = [dim_learning_rate,dim_num_dense_layers, dim_num_hidden_nodes, dim_batch_sizes, dim_optimisers,\\\n",
    "              dim_dropout_values, dim_num_conv_layers, dim_patch_sizes, dim_max_pool_sizes ]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Default parameter \n",
    "\n",
    "While the hyperparameter optimiser runs for the first time, it runs the fitness function with the default parameters. \n",
    "\n",
    "If good default hyperparameter values are provided, then it will significantly  help reach to global optimal hyperparameter configuration.\n",
    "\n",
    "Hence it is always a good idea to set default parameters to good parameter values, after literature study, whenever feasible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start search from a decent choice, so that the search narrows faster\n",
    "default_parameters = [0.001, 2, 64, 20,'Adam',0.1, 2, 5,2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Create Model\n",
    "\n",
    "We now create model that will accept the hyperparameters configuration set, create model and return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a keras model\n",
    "def create_model(learning_rate, num_dense_layers,  num_hidden_nodes,  optimisers, dropout_values ,\\\n",
    "                num_conv_layers, patch_sizes, max_pool_sizes ):\n",
    "    activation = 'relu'\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape=(img_size*img_size,))) # accepts flattened images    \n",
    "    # 784 to  (28, 28, 1) Conv layers expected shape\n",
    "    model.add(Reshape((img_size, img_size, 1)))\n",
    "    \n",
    "    for j in range(num_conv_layers):           \n",
    "        model.add(Conv2D(kernel_size=5, strides=1, filters=16, padding='same', activation=activation, \\\n",
    "                         name='layer_conv{0}'.format(j+1) ))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) #maxpool by 2x2 window. 2x2 will halve the space\n",
    "        model.add(Dropout(dropout_values))\n",
    "    # Flatten  to feed to fully connected layer i.e dense layer in keras\n",
    "    model.add(Flatten())    \n",
    "    for i in range(num_dense_layers):        \n",
    "        model.add(Dense(num_hidden_nodes,activation=activation,name='fc_layer_{0}'.format(i+1)))\n",
    "        model.add(Dropout(dropout_values))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    optimizer = eval(optimisers)(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Fitness Function\n",
    "\n",
    "Now we create the fitness function. This function accepts each parmeter set configuration, creates model, fits it and performs evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from skopt import dump, load\n",
    "save_best_model_path = 'best_conv_model'\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# optimiser function. skopt performs minimisation instead of maximisation, hence we negate the accuracy\n",
    "# @use_named_args  used so that the function can be called as a single list as required by skopt i.e fitness(x=[1e-4, 3, 256, 'relu'])\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(learning_rate, num_dense_layers,  num_hidden_nodes, batch_sizes, optimisers, dropout_values ,\\\n",
    "           num_conv_layers, patch_sizes, max_pool_sizes):\n",
    "    # Print the hyper-parameters.\n",
    "    print('learning rate : ', learning_rate,'. Num_dense_layers:', num_dense_layers,'. num_hidden_nodes:', num_hidden_nodes)\n",
    "    print('optimisers: ', optimisers,'.  batch_sizes: ',batch_sizes,'. dropout_values : ',dropout_values,\\\n",
    "          'num_conv_layers =', num_conv_layers,'. patch_sizes = ',patch_sizes,'. max_pool_sizes =', max_pool_sizes)    \n",
    "        \n",
    "    # Create the neural network with the new hyper-parameters.\n",
    "    model = create_model(learning_rate=learning_rate,           num_dense_layers=num_dense_layers,\\\n",
    "                         num_hidden_nodes=num_hidden_nodes,  optimisers=optimisers, dropout_values = dropout_values,\\\n",
    "                        num_conv_layers = num_conv_layers, patch_sizes = patch_sizes, max_pool_sizes = max_pool_sizes )   \n",
    "    history = model.fit(x=data.train.images,  y=data.train.labels,  epochs=1,\n",
    "                        batch_size=batch_sizes, validation_data=validation_data)#,callbacks=[callback_log])\n",
    "    # Get the classification accuracy on the validation-set after the last training-epoch.\n",
    "    accuracy = history.history['val_acc'][-1]\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy),)\n",
    "    global best_accuracy # global to preserve  the accuracy beyond this function scope\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        model.save(save_best_model_path)\n",
    "        best_accuracy = accuracy    \n",
    "    del model # free memory for next model, once performance evaluation with current hyper parameter is done.\n",
    "    \n",
    "    K.clear_session() # reset graph, other wise model will be added to same tensorflow graph everytime with new hyperparams\n",
    "    return -accuracy # scikit opt does minimisation instead of maximisation, hence  we negate it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Test & Debug fitness function\n",
    "\n",
    "Because the fitness function may  contain error, we should test it and debug it prior to running the optimisation across full hyperpareters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate :  0.001 . Num_dense_layers: 2 . num_hidden_nodes: 64\n",
      "optimisers:  Adam .  batch_sizes:  20 . dropout_values :  0.1 num_conv_layers = 2 . patch_sizes =  5 . max_pool_sizes = 2\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 87s 2ms/step - loss: 0.2310 - acc: 0.9279 - val_loss: 0.0664 - val_acc: 0.9820\n",
      "Accuracy: 98.20%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.9819999964237213"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample check with default parameter\n",
    "fitness(x=default_parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Run HyperParameter Optimisation\n",
    "\n",
    "Now we run the hyperparameter optimisation using scikit-optimise's  Bayesian Optimisation with Gaussian Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate :  0.001 . Num_dense_layers: 2 . num_hidden_nodes: 64\n",
      "optimisers:  Adam .  batch_sizes:  20 . dropout_values :  0.1 num_conv_layers = 2 . patch_sizes =  5 . max_pool_sizes = 2\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 77s 1ms/step - loss: 0.2257 - acc: 0.9293 - val_loss: 0.0567 - val_acc: 0.9812\n",
      "Accuracy: 98.12%\n",
      "learning rate :  1.2273662556515456e-05 . Num_dense_layers: 5 . num_hidden_nodes: 101\n",
      "optimisers:  SGD .  batch_sizes:  100 . dropout_values :  0.40201642903982104 num_conv_layers = 3 . patch_sizes =  6 . max_pool_sizes = 3\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 63s 1ms/step - loss: 2.3416 - acc: 0.0989 - val_loss: 2.3029 - val_acc: 0.0948\n",
      "Accuracy: 9.48%\n",
      "learning rate :  0.0026281154224798995 . Num_dense_layers: 2 . num_hidden_nodes: 679\n",
      "optimisers:  SGD .  batch_sizes:  83 . dropout_values :  0.5902048504250774 num_conv_layers = 3 . patch_sizes =  7 . max_pool_sizes = 3\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 70s 1ms/step - loss: 2.3132 - acc: 0.1020 - val_loss: 2.3016 - val_acc: 0.1126\n",
      "Accuracy: 11.26%\n",
      "learning rate :  0.0040231071299883835 . Num_dense_layers: 5 . num_hidden_nodes: 181\n",
      "optimisers:  Adam .  batch_sizes:  117 . dropout_values :  0.6268878256420579 num_conv_layers = 4 . patch_sizes =  5 . max_pool_sizes = 2\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 65s 1ms/step - loss: 2.1183 - acc: 0.1819 - val_loss: 3.0146 - val_acc: 0.1132\n",
      "Accuracy: 11.32%\n",
      "learning rate :  0.0022637413889353745 . Num_dense_layers: 5 . num_hidden_nodes: 838\n",
      "optimisers:  SGD .  batch_sizes:  125 . dropout_values :  0.27011773317437004 num_conv_layers = 3 . patch_sizes =  6 . max_pool_sizes = 3\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 89s 2ms/step - loss: 2.3016 - acc: 0.1016 - val_loss: 2.3006 - val_acc: 0.1548\n",
      "Accuracy: 15.48%\n",
      "learning rate :  6.821654549728117e-05 . Num_dense_layers: 4 . num_hidden_nodes: 1148\n",
      "optimisers:  Adam .  batch_sizes:  101 . dropout_values :  0.4941064065027715 num_conv_layers = 3 . patch_sizes =  6 . max_pool_sizes = 4\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 136s 2ms/step - loss: 2.2942 - acc: 0.1105 - val_loss: 2.2685 - val_acc: 0.1648\n",
      "Accuracy: 16.48%\n",
      "learning rate :  0.00025857017175947894 . Num_dense_layers: 3 . num_hidden_nodes: 493\n",
      "optimisers:  Adam .  batch_sizes:  38 . dropout_values :  0.4060449389435037 num_conv_layers = 3 . patch_sizes =  6 . max_pool_sizes = 3\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 120s 2ms/step - loss: 1.0082 - acc: 0.6439 - val_loss: 0.1889 - val_acc: 0.9488\n",
      "Accuracy: 94.88%\n",
      "learning rate :  0.0004440020004428979 . Num_dense_layers: 3 . num_hidden_nodes: 532\n",
      "optimisers:  Adam .  batch_sizes:  119 . dropout_values :  0.5471514032474885 num_conv_layers = 3 . patch_sizes =  3 . max_pool_sizes = 4\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 65s 1ms/step - loss: 1.7599 - acc: 0.3547 - val_loss: 0.5954 - val_acc: 0.8356\n",
      "Accuracy: 83.56%\n",
      "learning rate :  0.00011842620739119759 . Num_dense_layers: 4 . num_hidden_nodes: 1251\n",
      "optimisers:  SGD .  batch_sizes:  114 . dropout_values :  0.5351899613508884 num_conv_layers = 3 . patch_sizes =  4 . max_pool_sizes = 4\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 93s 2ms/step - loss: 2.3553 - acc: 0.0997 - val_loss: 2.3031 - val_acc: 0.1014\n",
      "Accuracy: 10.14%\n",
      "learning rate :  0.00062003703251284 . Num_dense_layers: 4 . num_hidden_nodes: 499\n",
      "optimisers:  Adam .  batch_sizes:  97 . dropout_values :  0.6647005975141239 num_conv_layers = 2 . patch_sizes =  3 . max_pool_sizes = 3\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 66s 1ms/step - loss: 2.0122 - acc: 0.2448 - val_loss: 1.5743 - val_acc: 0.4296\n",
      "Accuracy: 42.96%\n",
      "learning rate :  0.00022921001012124313 . Num_dense_layers: 3 . num_hidden_nodes: 280\n",
      "optimisers:  Adam .  batch_sizes:  81 . dropout_values :  0.10331790957943952 num_conv_layers = 2 . patch_sizes =  6 . max_pool_sizes = 3\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 61s 1ms/step - loss: 0.4287 - acc: 0.8681 - val_loss: 0.0987 - val_acc: 0.9686\n",
      "Accuracy: 96.86%\n",
      "learning rate :  0.01 . Num_dense_layers: 3 . num_hidden_nodes: 64\n",
      "optimisers:  Adam .  batch_sizes:  150 . dropout_values :  0.35092620217950443 num_conv_layers = 2 . patch_sizes =  3 . max_pool_sizes = 2\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 56s 1ms/step - loss: 0.6785 - acc: 0.7873 - val_loss: 0.1224 - val_acc: 0.9724\n",
      "Accuracy: 97.24%\n",
      "learning rate :  1e-05 . Num_dense_layers: 2 . num_hidden_nodes: 64\n",
      "optimisers:  Adam .  batch_sizes:  10 . dropout_values :  0.1 num_conv_layers = 2 . patch_sizes =  6 . max_pool_sizes = 3\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 116s 2ms/step - loss: 1.7405 - acc: 0.4205 - val_loss: 0.8513 - val_acc: 0.8226\n",
      "Accuracy: 82.26%\n",
      "learning rate :  0.01 . Num_dense_layers: 2 . num_hidden_nodes: 64\n",
      "optimisers:  Adam .  batch_sizes:  150 . dropout_values :  0.1 num_conv_layers = 2 . patch_sizes =  7 . max_pool_sizes = 1\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 51s 926us/step - loss: 0.2451 - acc: 0.9227 - val_loss: 0.0871 - val_acc: 0.9752\n",
      "Accuracy: 97.52%\n",
      "learning rate :  0.0009187946785502826 . Num_dense_layers: 3 . num_hidden_nodes: 243\n",
      "optimisers:  Adam .  batch_sizes:  10 . dropout_values :  0.36761659600249386 num_conv_layers = 3 . patch_sizes =  6 . max_pool_sizes = 2\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 184s 3ms/step - loss: 0.4726 - acc: 0.8529 - val_loss: 0.0919 - val_acc: 0.9718\n",
      "Accuracy: 97.18%\n",
      "learning rate :  1e-05 . Num_dense_layers: 2 . num_hidden_nodes: 64\n",
      "optimisers:  Adam .  batch_sizes:  10 . dropout_values :  0.2277791146830224 num_conv_layers = 3 . patch_sizes =  7 . max_pool_sizes = 2\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 156s 3ms/step - loss: 2.2724 - acc: 0.1500 - val_loss: 2.1594 - val_acc: 0.4360\n",
      "Accuracy: 43.60%\n",
      "learning rate :  0.01 . Num_dense_layers: 2 . num_hidden_nodes: 64\n",
      "optimisers:  Adam .  batch_sizes:  10 . dropout_values :  0.1 num_conv_layers = 2 . patch_sizes =  2 . max_pool_sizes = 1\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 120s 2ms/step - loss: 0.4181 - acc: 0.8868 - val_loss: 0.1949 - val_acc: 0.9518\n",
      "Accuracy: 95.18%\n",
      "learning rate :  0.00017138203893577955 . Num_dense_layers: 3 . num_hidden_nodes: 1069\n",
      "optimisers:  SGD .  batch_sizes:  136 . dropout_values :  0.1 num_conv_layers = 3 . patch_sizes =  2 . max_pool_sizes = 1\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 78s 1ms/step - loss: 2.3049 - acc: 0.0926 - val_loss: 2.3041 - val_acc: 0.0930\n",
      "Accuracy: 9.30%\n",
      "learning rate :  0.0031902961765478046 . Num_dense_layers: 3 . num_hidden_nodes: 1071\n",
      "optimisers:  Adam .  batch_sizes:  150 . dropout_values :  0.11920412766761226 num_conv_layers = 2 . patch_sizes =  7 . max_pool_sizes = 2\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 82s 1ms/step - loss: 0.1917 - acc: 0.9389 - val_loss: 0.0655 - val_acc: 0.9822\n",
      "Accuracy: 98.22%\n",
      "learning rate :  0.01 . Num_dense_layers: 3 . num_hidden_nodes: 64\n",
      "optimisers:  Adam .  batch_sizes:  10 . dropout_values :  0.1 num_conv_layers = 3 . patch_sizes =  7 . max_pool_sizes = 1\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 121s 2ms/step - loss: 2.3037 - acc: 0.1070 - val_loss: 2.3036 - val_acc: 0.0986\n",
      "Accuracy: 9.86%\n"
     ]
    }
   ],
   "source": [
    "# HyperPArameter Optimisation\n",
    "hp_search_result = gp_minimize(func=fitness,    # function to minimise\n",
    "                            dimensions=dimensions, # hyperparamter search boundary to search through\n",
    "                            acq_func='EI', # Expected Improvement.\n",
    "                            n_calls=20,   #40 no of calls to func to find minimum\n",
    "                            x0=default_parameters\n",
    "                           ,n_jobs = 4 # number of cores to run in parallel\n",
    "                           )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Results\n",
    "\n",
    "Now we inspect best hyperparameter set and its performance.\n",
    "\n",
    "We also inspect other hyperparameter configuration set and  thier corresponding performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameter is : [0.0031902961765478046, 3, 1071, 150, 'Adam', 0.11920412766761226, 2, 7, 2]\n",
      "Best accuracy is :  -0.9822000133991241\n",
      " \n",
      "All hyper params and their respective accuracy is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(98.25,\n",
       "  [0.0031902961765478046, 3, 1071, 150, 'Adam', 0.11920412766761226, 2, 7, 2]),\n",
       " (98.1, [0.001, 2, 64, 20, 'Adam', 0.1, 2, 5, 2]),\n",
       " (97.5, [0.01, 2, 64, 150, 'Adam', 0.1, 2, 7, 1]),\n",
       " (97.25, [0.01, 3, 64, 150, 'Adam', 0.35092620217950443, 2, 3, 2]),\n",
       " (97.2,\n",
       "  [0.0009187946785502826, 3, 243, 10, 'Adam', 0.36761659600249386, 3, 6, 2]),\n",
       " (96.9,\n",
       "  [0.00022921001012124313, 3, 280, 81, 'Adam', 0.10331790957943952, 2, 6, 3]),\n",
       " (95.2, [0.01, 2, 64, 10, 'Adam', 0.1, 2, 2, 1]),\n",
       " (94.9,\n",
       "  [0.00025857017175947894, 3, 493, 38, 'Adam', 0.4060449389435037, 3, 6, 3]),\n",
       " (83.56,\n",
       "  [0.0004440020004428979, 3, 532, 119, 'Adam', 0.5471514032474885, 3, 3, 4]),\n",
       " (82.25, [1e-05, 2, 64, 10, 'Adam', 0.1, 2, 6, 3]),\n",
       " (43.6, [1e-05, 2, 64, 10, 'Adam', 0.2277791146830224, 3, 7, 2]),\n",
       " (42.97,\n",
       "  [0.00062003703251284, 4, 499, 97, 'Adam', 0.6647005975141239, 2, 3, 3]),\n",
       " (16.48,\n",
       "  [6.821654549728117e-05, 4, 1148, 101, 'Adam', 0.4941064065027715, 3, 6, 4]),\n",
       " (15.48,\n",
       "  [0.0022637413889353745, 5, 838, 125, 'SGD', 0.27011773317437004, 3, 6, 3]),\n",
       " (11.32,\n",
       "  [0.0040231071299883835, 5, 181, 117, 'Adam', 0.6268878256420579, 4, 5, 2]),\n",
       " (11.26,\n",
       "  [0.0026281154224798995, 2, 679, 83, 'SGD', 0.5902048504250774, 3, 7, 3]),\n",
       " (10.14,\n",
       "  [0.00011842620739119759, 4, 1251, 114, 'SGD', 0.5351899613508884, 3, 4, 4]),\n",
       " (9.86, [0.01, 3, 64, 10, 'Adam', 0.1, 3, 7, 1]),\n",
       " (9.48,\n",
       "  [1.2273662556515456e-05, 5, 101, 100, 'SGD', 0.40201642903982104, 3, 6, 3]),\n",
       " (9.3, [0.00017138203893577955, 3, 1069, 136, 'SGD', 0.1, 3, 2, 1])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best HyperParameters\n",
    "print('Best hyperparameter is :', hp_search_result.x )\n",
    "print('Best accuracy is : ', hp_search_result.fun )\n",
    "print(' ')\n",
    "print('All hyper params and their respective accuracy is')\n",
    "sorted(zip((hp_search_result.func_vals*-100).astype(np.float16), hp_search_result.x_iters), reverse=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Grid Search CV\n",
    "\n",
    "Now, we implement the Random Grid Search to see its  performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate :  2.5959833184971615e-05 . Num_dense_layers: 3 . num_hidden_nodes: 828\n",
      "optimisers:  SGD .  batch_sizes:  89 . dropout_values :  0.6611469037516559 num_conv_layers = 3 . patch_sizes =  3 . max_pool_sizes = 4\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 86s 2ms/step - loss: 2.7014 - acc: 0.0999 - val_loss: 2.3027 - val_acc: 0.1310\n",
      "Accuracy: 13.10%\n",
      "learning rate :  0.0006400437068443558 . Num_dense_layers: 4 . num_hidden_nodes: 101\n",
      "optimisers:  SGD .  batch_sizes:  114 . dropout_values :  0.16060957932136827 num_conv_layers = 4 . patch_sizes =  5 . max_pool_sizes = 4\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 72s 1ms/step - loss: 2.3030 - acc: 0.0955 - val_loss: 2.3027 - val_acc: 0.0828\n",
      "Accuracy: 8.28%\n",
      "learning rate :  0.0008775168799193001 . Num_dense_layers: 2 . num_hidden_nodes: 891\n",
      "optimisers:  Adam .  batch_sizes:  112 . dropout_values :  0.1403714605221342 num_conv_layers = 3 . patch_sizes =  4 . max_pool_sizes = 3\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 113s 2ms/step - loss: 0.3285 - acc: 0.8936 - val_loss: 0.0710 - val_acc: 0.9808\n",
      "Accuracy: 98.08%\n",
      "learning rate :  0.00011775370018598427 . Num_dense_layers: 3 . num_hidden_nodes: 661\n",
      "optimisers:  SGD .  batch_sizes:  85 . dropout_values :  0.33228013145954816 num_conv_layers = 2 . patch_sizes =  2 . max_pool_sizes = 3\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 72s 1ms/step - loss: 2.3128 - acc: 0.1040 - val_loss: 2.3014 - val_acc: 0.1098\n",
      "Accuracy: 10.98%\n",
      "learning rate :  0.0024550891858925827 . Num_dense_layers: 4 . num_hidden_nodes: 779\n",
      "optimisers:  SGD .  batch_sizes:  46 . dropout_values :  0.48823252559910135 num_conv_layers = 3 . patch_sizes =  5 . max_pool_sizes = 4\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 113s 2ms/step - loss: 2.3027 - acc: 0.1083 - val_loss: 2.3005 - val_acc: 0.1126\n",
      "Accuracy: 11.26%\n",
      "learning rate :  0.006748070241708341 . Num_dense_layers: 2 . num_hidden_nodes: 805\n",
      "optimisers:  Adam .  batch_sizes:  118 . dropout_values :  0.4748698794269489 num_conv_layers = 4 . patch_sizes =  3 . max_pool_sizes = 1\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 74s 1ms/step - loss: 1.4941 - acc: 0.4585 - val_loss: 1.4817 - val_acc: 0.4308\n",
      "Accuracy: 43.08%\n",
      "learning rate :  1.323224914220038e-05 . Num_dense_layers: 5 . num_hidden_nodes: 1077\n",
      "optimisers:  SGD .  batch_sizes:  100 . dropout_values :  0.5734067709958133 num_conv_layers = 2 . patch_sizes =  5 . max_pool_sizes = 1\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 113s 2ms/step - loss: 2.4993 - acc: 0.1049 - val_loss: 2.3022 - val_acc: 0.1272\n",
      "Accuracy: 12.72%\n",
      "learning rate :  2.7588555835364105e-05 . Num_dense_layers: 3 . num_hidden_nodes: 300\n",
      "optimisers:  Adam .  batch_sizes:  21 . dropout_values :  0.5146625717691502 num_conv_layers = 3 . patch_sizes =  3 . max_pool_sizes = 2\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 112s 2ms/step - loss: 2.3064 - acc: 0.1053 - val_loss: 2.3000 - val_acc: 0.1126\n",
      "Accuracy: 11.26%\n",
      "learning rate :  0.002002149800288206 . Num_dense_layers: 4 . num_hidden_nodes: 596\n",
      "optimisers:  Adam .  batch_sizes:  137 . dropout_values :  0.20778462891348054 num_conv_layers = 4 . patch_sizes =  2 . max_pool_sizes = 4\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 70s 1ms/step - loss: 0.9225 - acc: 0.6698 - val_loss: 0.2092 - val_acc: 0.9442\n",
      "Accuracy: 94.42%\n",
      "learning rate :  0.00012520771660244203 . Num_dense_layers: 5 . num_hidden_nodes: 1058\n",
      "optimisers:  SGD .  batch_sizes:  74 . dropout_values :  0.12560342010092307 num_conv_layers = 2 . patch_sizes =  4 . max_pool_sizes = 1\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 116s 2ms/step - loss: 2.3034 - acc: 0.0908 - val_loss: 2.3030 - val_acc: 0.0800\n",
      "Accuracy: 8.00%\n",
      "learning rate :  0.0001825922302582303 . Num_dense_layers: 2 . num_hidden_nodes: 1227\n",
      "optimisers:  Adam .  batch_sizes:  138 . dropout_values :  0.5342300680774701 num_conv_layers = 3 . patch_sizes =  7 . max_pool_sizes = 4\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 83s 2ms/step - loss: 1.7213 - acc: 0.3767 - val_loss: 0.7085 - val_acc: 0.8348\n",
      "Accuracy: 83.48%\n",
      "learning rate :  0.00024266304345115004 . Num_dense_layers: 2 . num_hidden_nodes: 362\n",
      "optimisers:  SGD .  batch_sizes:  87 . dropout_values :  0.11155600100350825 num_conv_layers = 2 . patch_sizes =  7 . max_pool_sizes = 2\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 67s 1ms/step - loss: 2.2977 - acc: 0.0935 - val_loss: 2.2903 - val_acc: 0.0944\n",
      "Accuracy: 9.44%\n",
      "learning rate :  1.0421363671008183e-05 . Num_dense_layers: 5 . num_hidden_nodes: 1054\n",
      "optimisers:  Adam .  batch_sizes:  100 . dropout_values :  0.29257650578398875 num_conv_layers = 4 . patch_sizes =  6 . max_pool_sizes = 1\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 132s 2ms/step - loss: 2.2999 - acc: 0.1010 - val_loss: 2.2964 - val_acc: 0.1202\n",
      "Accuracy: 12.02%\n",
      "learning rate :  1.1383776174125046e-05 . Num_dense_layers: 5 . num_hidden_nodes: 981\n",
      "optimisers:  Adam .  batch_sizes:  142 . dropout_values :  0.5394650340196394 num_conv_layers = 2 . patch_sizes =  4 . max_pool_sizes = 3\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 97s 2ms/step - loss: 2.3456 - acc: 0.1029 - val_loss: 2.2982 - val_acc: 0.1740\n",
      "Accuracy: 17.40%\n",
      "learning rate :  0.000251750740897661 . Num_dense_layers: 4 . num_hidden_nodes: 254\n",
      "optimisers:  Adam .  batch_sizes:  106 . dropout_values :  0.3589516492732975 num_conv_layers = 3 . patch_sizes =  5 . max_pool_sizes = 3\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 72s 1ms/step - loss: 1.7134 - acc: 0.3706 - val_loss: 0.5239 - val_acc: 0.8566\n",
      "Accuracy: 85.66%\n",
      "learning rate :  0.00016438184469598887 . Num_dense_layers: 4 . num_hidden_nodes: 364\n",
      "optimisers:  Adam .  batch_sizes:  146 . dropout_values :  0.10276432387758581 num_conv_layers = 4 . patch_sizes =  6 . max_pool_sizes = 2\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 63s 1ms/step - loss: 1.2438 - acc: 0.5517 - val_loss: 0.3574 - val_acc: 0.8922\n",
      "Accuracy: 89.22%\n",
      "learning rate :  6.912716684097001e-05 . Num_dense_layers: 2 . num_hidden_nodes: 686\n",
      "optimisers:  Adam .  batch_sizes:  65 . dropout_values :  0.2198441040309784 num_conv_layers = 3 . patch_sizes =  4 . max_pool_sizes = 2\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 77s 1ms/step - loss: 1.1107 - acc: 0.6241 - val_loss: 0.3163 - val_acc: 0.9066\n",
      "Accuracy: 90.66%\n",
      "learning rate :  0.0006321487071254391 . Num_dense_layers: 4 . num_hidden_nodes: 674\n",
      "optimisers:  Adam .  batch_sizes:  33 . dropout_values :  0.25177350822432865 num_conv_layers = 3 . patch_sizes =  5 . max_pool_sizes = 4\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 152s 3ms/step - loss: 0.4250 - acc: 0.8591 - val_loss: 0.0803 - val_acc: 0.9758\n",
      "Accuracy: 97.58%\n",
      "learning rate :  0.0004300946373541931 . Num_dense_layers: 2 . num_hidden_nodes: 270\n",
      "optimisers:  Adam .  batch_sizes:  87 . dropout_values :  0.18994194319417068 num_conv_layers = 2 . patch_sizes =  3 . max_pool_sizes = 1\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 66s 1ms/step - loss: 0.4103 - acc: 0.8692 - val_loss: 0.0868 - val_acc: 0.9738\n",
      "Accuracy: 97.38%\n",
      "learning rate :  3.6356294896521647e-05 . Num_dense_layers: 2 . num_hidden_nodes: 412\n",
      "optimisers:  SGD .  batch_sizes:  38 . dropout_values :  0.6967831225629629 num_conv_layers = 2 . patch_sizes =  2 . max_pool_sizes = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "55000/55000 [==============================] - 79s 1ms/step - loss: 2.6613 - acc: 0.0964 - val_loss: 2.3019 - val_acc: 0.0990\n",
      "Accuracy: 9.90%\n"
     ]
    }
   ],
   "source": [
    "from skopt import dummy_minimize\n",
    "random_grid_search = dummy_minimize(func=fitness,    # function to minimise\n",
    "                            dimensions=dimensions, # hyperparamter search boundary to search through                            \n",
    "                            n_calls=20   #20 no of calls to func to find minimum                            \n",
    "                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy with random grid  search is   :  -0.9807999932289123\n",
      "Best accuracy with Bayesian Optimisation is :  -0.9822000133991241\n"
     ]
    }
   ],
   "source": [
    "print('Best accuracy with random grid  search is   : ',random_grid_search.fun)\n",
    "print('Best accuracy with Bayesian Optimisation is : ', hp_search_result.fun )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "## Imports\n",
    "#######\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "import tensorlayer as tl\n",
    "from tensorlayer.layers import *\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0  Clean Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get  Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrector dict len is 0\n",
      "Sample word count dictionary is : [{'are': 891, 'you': 2912, 'real': 40, 'no': 540, 'a': 962, 'bot': 64, 'is': 661, 'i': 2291, 'm': 361, 'not': 596, 'yes': 475, 'prove': 24, 'it': 607, 'first': 24, 'can': 225, 't': 685, 'something': 62, 'don': 418, 'believe': 66, 'the': 710, 'halting': 1, 'problem': 12, 'see': 55, 're': 289, 'that': 729, 's': 405, 'what': 627, 'told': 24, 'would': 126, 'marry': 5, 'for': 186, 'love': 111, 'of': 391, 'my': 225, 'life': 59, 'me': 492, 'sure': 93, 'course': 28, 'everyone': 11, 'how': 208, 'did': 149, 'learn': 19, 'learned': 5, 'in': 237, 'age': 6, 'legends': 1, 'people': 63, 'nowadays': 1, 'know': 370, 'so': 219}]\n",
      "X is : [['are', 'you', 'real'], ['i', 'would', 'marry', 'for', 'the', 'love', 'of', 'my', 'life']]\n",
      "Y is : [['no', 'a', 'bot', 'is', 'real', 'i', 'm', 'not', 'a', 'bot', 'yes', 'you', 'are', 'prove', 'it', 'you', 'first', 'i', 'can', 't', 'prove', 'something', 'i', 'don', 't', 'believe', 'prove', 'the', 'halting', 'problem', 'you', 'first', 'see', 'you', 're', 'not', 'that', 's', 'what', 'i', 'told', 'you'], ['would', 'you', 'marry', 'me', 'sure', 'can', 'you', 'love', 'of', 'course', 'everyone', 'can', 'love', 'how', 'did', 'you', 'learn', 'i', 'learned', 'in', 'the', 'age', 'of', 'legends', 'people', 'nowadays', 'know', 'so', 'little', 'now', 'lie', 'on', 'the', 'bed', 'yes', 'maam', 'giggles', 'giggles', 'back']]\n"
     ]
    }
   ],
   "source": [
    "def getDictElems(dictn, n = 2):\n",
    "    return [{k: dictn[k] for k in list(dictn.keys())[:n]  }]\n",
    "\n",
    "#.rstrip(os.linesep)\n",
    "\n",
    "def correct_chat_text(line, corrector_dict = {} ):     \n",
    "    line =  (line.encode('ascii', 'ignore')).decode(\"utf-8\").lower()\\\n",
    "            .replace('.',' . ')\\\n",
    "        .replace('?',' ? ').replace(',',' , ')#.replace('<CHAT_EOD>','...'+os.linesep)\n",
    "    new_line = []\n",
    "    for word in line.split():\n",
    "        #if w not in set(stopwords.words(\"english\")):\n",
    "        if word in  corrector_dict:\n",
    "            new_line.append(corrector_dict[word] )\n",
    "        else:\n",
    "            new_line.append(word)\n",
    "    line = \" \".join(new_line)\n",
    "    \n",
    "    # Implement post correction of i'm  or else it breaks i'm to [i, m]\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    line = \" \".join(tokenizer.tokenize(line))\n",
    "        \n",
    "    return line\n",
    "\n",
    "def getXYnWordFrequencyFromText(file  = \"chatter.txt\", corrector_dict = {} ):    \n",
    "    print('Corrector dict len is', len(corrector_dict) )\n",
    "    word_count = {}\n",
    "    X,Y = [],[]\n",
    "    cur_chat_response = ''\n",
    "    next_line_is_new_chat = True\n",
    "    \n",
    "    with open(file) as fin:\n",
    "        for line in fin:\n",
    "            if line.replace('...','EOChat') != 'EOChat':\n",
    "                line = correct_chat_text(line, corrector_dict)                         \n",
    "            \n",
    "            if next_line_is_new_chat :\n",
    "                X.append(line.split() )\n",
    "            else:\n",
    "                cur_chat_response += line+' '\n",
    "            \n",
    "            if len(line.split()) == 0:\n",
    "                next_line_is_new_chat = True\n",
    "                Y.append(cur_chat_response.split())\n",
    "                cur_chat_response =  ''\n",
    "            else:\n",
    "                next_line_is_new_chat = False\n",
    "                           \n",
    "            for word in line.split():\n",
    "                if word in word_count.keys():\n",
    "                    word_count[word] += 1\n",
    "                else:\n",
    "                    word_count[word] = 1\n",
    "    return word_count, X,Y\n",
    "                    \n",
    "word_count, X,Y  = getXYnWordFrequencyFromText(\"chatter.txt\")\n",
    "print('Sample word count dictionary is :', getDictElems(word_count, 50) )\n",
    "print('X is :', X[0:2] )\n",
    "print('Y is :', Y[0:2] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2  Add Words not in Glove embeddings to wordvec_embedding\n",
    "\n",
    "Although limitin word embeddings reduces search space, we won't be doing so because\n",
    "because we want our model to be able to also generate words not previously seen i.e If we only had \"sad, happy\" words  in embeddings then we would not be able to predict  words other than that   i.e \"surprised\".\n",
    "\n",
    "Because we would not be able to reverse lookup the word surprised fromt he word embedding vectors if it is not there at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Words in text and glove   ['the', 'of', 'to', 'and', 'in', 'a', 'for', 'that', 'on', 'is']\n",
      "\n",
      " Words  in text but not in glove =  193\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "#vocabulary_size\n",
    "#pret_embeddings = np.empty(shape=(vocabulary_size,50),dtype=np.float32)\n",
    "\n",
    "def getWordEmbedNFounds(word_count, wordvec_embedding = {}):     \n",
    "    words_found = []\n",
    "\n",
    "    with zipfile.ZipFile('glove.6B.zip') as glovezip:\n",
    "        with glovezip.open('glove.6B.50d.txt') as glovefile:\n",
    "            for li, line in enumerate(glovefile):\n",
    "                # Progress\n",
    "                #if (li+1)%10000==0: print('.',end='')\n",
    "                line_tokens = line.decode('utf-8').split(' ')\n",
    "                word = line_tokens[0]            \n",
    "                \n",
    "                vector = [float(v) for v in line_tokens[1:]]\n",
    "                wordvec_embedding[word] = vector                \n",
    "                    \n",
    "                if word in word_count.keys():\n",
    "                    words_found.append(word)\n",
    "    return wordvec_embedding, words_found\n",
    "\n",
    "wordvec_embedding, words_found = getWordEmbedNFounds(word_count)\n",
    "\n",
    "print('\\n Words in text and glove  ', words_found[:10])\n",
    "print('\\n Words  in text but not in glove = ', len(word_count) - len(words_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found words sample :  [{'maam': 1, 'k___': 7, 'gooooood': 1, 'geekiest': 1, 'j_______': 1}]\n"
     ]
    }
   ],
   "source": [
    "def getWordsNotFound(word_count, words_found):\n",
    "    not_found_words = {}\n",
    "    for word in word_count:\n",
    "        if word not in words_found:\n",
    "            not_found_words[word] = word_count[word]\n",
    "    return not_found_words\n",
    "\n",
    "not_found_words = getWordsNotFound(word_count, words_found)\n",
    "print('Not found words sample : ', getDictElems(not_found_words,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Invalid Words and Correction\n",
    "\n",
    "USe count  to filter  must / good to have / no fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found words: \n",
      "{'maam': 1, 'k___': 7, 'gooooood': 1, 'geekiest': 1, 'j_______': 1, 'diden': 1, 'j____': 12, 'hahahaha': 1, 'yuppers': 1, 'awwwww': 1, 'jabberwacky': 18, 'uhmm': 1, 't___': 1, 'unthing': 1, 'superthing': 2, 's__': 1, 'l____': 3, 'srsly': 1, 'bloodtypes': 1, 'internetworks': 1, 'aaaaaaaaaa': 1, 'noumenal': 1, 'philolsopher': 1, 'philolsophers': 2, 'misapplications': 1, 'priviledge': 1, 'naturlich': 1, 'yeaup': 1, 'achali': 1, 'escopo': 1, 'alvo': 1, 'asno': 1, 'significar': 1, 'concordo': 1, 'mquina': 3, 'sonho': 1, 'acordar': 1, 'ento': 1, 'reconhece': 1, 'apesar': 1, 'devemos': 1, 'discutir': 1, 'chatbot': 6, 'mellifluity': 1, 'aleatorically': 1, 'recurse': 1, 'interweb': 1, 'gumph': 1, 'hahaha': 10, 'quantuum': 1, '085': 1, 'clarabel': 1, 'goodybe': 1, 'dienstag': 1, 'papagei': 3, 'pinapple': 1, 'brillig': 3, 'slithey': 3, 'toves': 3, 'counfounding': 1, 'wouln': 1, 'c____': 3, 'rythmically': 1, 'existense': 1, 'jsut': 1, 'gaaagooo': 1, 'gagagagagoo': 1, 'inscrutible': 1, 'jurrasic': 1, 'uhuh': 1, 'heheh': 2, 'harhar': 1, 'gufwa': 1, 'gufaw': 1, 'hihihi': 1, 'hohoho': 1, 'fantabulous': 1, 'paries': 1, 'duhhhh': 2, 'duhhhhh': 1, 'nooooooooooooooooooooo': 1, 'nothig': 1, 'inputing': 1, 'shouldnt': 1, 'ahhhhhhhhhh': 1, 'yahh': 1, 'rawr': 1, 'specail': 1, 'fjjfw': 1, 'corrcect': 1, 'sourwull': 2, 'motiviation': 1, 'geooorgeee': 1, 'pffff': 1, 'definintion': 1, 'ehh': 1, 'riffic': 1, 'diffic': 1, 'hahhaha': 1, 'ahemmm': 1, 'interesteed': 1, 'percieve': 1, 'roflmao': 1, 'teehehehehehehe': 1, 'honeymustard': 2, 'strawberryclock': 1, 'evilest': 1, 'bedsocks': 1, 'wintows': 1, 'xanthrope': 1, 'mooooo': 1, 'mooo': 1, 'yoou': 1, 'vilken': 1, 'sjlen': 1, 'irrevelant': 1, 'botling': 2, 'corrrrrrrrrrect': 1, 'omfgwtfroflollmao': 1, 'tryed': 1, 'sweeeeet': 1, 'someting': 1, 'nd8hufdhuythbtoiueaomrpaek': 1, 'reasurring': 1, 'padwan': 1, 'latez': 1, 'mindreader': 1, 'awright': 1, 'ulitmately': 1, 'marrrrs': 2, 'highlord': 2, 'errr': 1, 'meeeep': 1, 'everythng': 1, 'dissapeared': 1, 'goatbeast': 1, 'fwoosh': 1, 'compser': 2, 'howabout': 2, 'baabababaaaaaaaa': 1, 'pigions': 1, 'neeeeeeeeeeeee': 1, 'youve': 1, 'mmmmmmmmm': 1, 'anyother': 1, 'slithy': 1, 'gire': 1, 'momeraths': 1, 'outgrabe': 1, 'jubjub': 1, 'frumious': 1, 'maxome': 1, 'tumtum': 1, 'uffish': 1, 'whiffling': 1, 'tulgey': 1, 'gallumping': 1, 'slainthe': 1, 'frabjous': 1, 'callooh': 1, 'callay': 1, 'subract': 1, 'suspiscious': 1, 'beeep': 1, 'mwhahaha': 1, 'blissfull': 1, 'ssssssssssssssssssssssss': 1, 'shoggoth': 1, 'shoggoths': 1, 'o_o': 1, 'lmao': 1, 'awwwwwwwwwww': 1, 'flmethrower': 1, 'defeatest': 1, 'blablablabla': 1, 'mwahahahaha': 1, 'smoooooch': 1, 'vaction': 1, 'suuuuuure': 1, 'parfois': 1, 'hehehe': 1, 'chatbots': 1, 'jawas': 1, 'secrure': 1, 'pepperment': 1, 'eveyone': 1, 'heebeejeebieme': 1, 'zigabytes': 1, 'povk': 1, 'sassing': 1, 'methought': 1, 'embarresed': 1, 'fingerwork': 1}\n"
     ]
    }
   ],
   "source": [
    "print('Not found words: ' )\n",
    "print(not_found_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerunWordVecEmbedsPostCorrection(corrector_dict, wordvec_embedding= {} ):    \n",
    "    word_count, X,Y = getXYnWordFrequencyFromText(\"chatter.txt\", corrector_dict )\n",
    "    wordvec_embedding, words_found = getWordEmbedNFounds(word_count, wordvec_embedding)\n",
    "    not_found_words = getWordsNotFound(word_count, words_found)    \n",
    "    return word_count, wordvec_embedding, not_found_words, X, Y\n",
    "\n",
    "#word_count  = getWordFrequencyFromText(\"chatter.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add unk and pad to WordVec embedding \n",
    "\n",
    "Because we want these to be the first embedding. We will later create word2id and id2word dict based on it and want pad to be of  0 index hence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_embedding = {}\n",
    "wordvec_embedding['_pad_'] = list( np.random.uniform(0, 0, 50 ) )\n",
    "wordvec_embedding['_unk_'] = list( np.random.uniform(-1.0, 1.0, 50 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrector dict len is 74\n",
      "\n",
      " Words  in text but not in glove =  185\n",
      "{'maam': 1, 'k___': 7, 'gooooood': 1, 'geekiest': 1, 'j_______': 1, 'diden': 1, 'j____': 12, 'hahahaha': 1, 'yuppers': 1, 'awwwww': 1, 'jabberwacky': 18, 'uhmm': 1, 't___': 1, 'unthing': 1, 'superthing': 2, 's__': 1, 'l____': 3, 'srsly': 1, 'bloodtypes': 1, 'internetworks': 1, 'aaaaaaaaaa': 1, 'noumenal': 1, 'philolsopher': 1, 'philolsophers': 2, 'misapplications': 1, 'priviledge': 1, 'naturlich': 1, 'yeaup': 1, 'achali': 1, 'escopo': 1, 'alvo': 1, 'asno': 1, 'significar': 1, 'concordo': 1, 'mquina': 3, 'sonho': 1, 'acordar': 1, 'ento': 1, 'reconhece': 1, 'apesar': 1, 'devemos': 1, 'discutir': 1, 'chatbot': 6, 'mellifluity': 1, 'aleatorically': 1, 'recurse': 1, 'interweb': 1, 'gumph': 1, 'hahaha': 10, 'quantuum': 1, '085': 1, 'clarabel': 1, 'goodybe': 1, 'dienstag': 1, 'papagei': 3, 'pinapple': 1, 'brillig': 3, 'slithey': 3, 'toves': 3, 'counfounding': 1, 'wouln': 1, 'c____': 3, 'rythmically': 1, 'existense': 1, 'jsut': 1, 'gaaagooo': 1, 'gagagagagoo': 1, 'inscrutible': 1, 'jurrasic': 1, 'uhuh': 1, 'heheh': 2, 'harhar': 1, 'gufwa': 1, 'gufaw': 1, 'hihihi': 1, 'hohoho': 1, 'fantabulous': 1, 'paries': 1, 'duhhhh': 2, 'duhhhhh': 1, 'nooooooooooooooooooooo': 1, 'nothig': 1, 'inputing': 1, 'shouldnt': 1, 'ahhhhhhhhhh': 1, 'yahh': 1, 'rawr': 1, 'specail': 1, 'fjjfw': 1, 'corrcect': 1, 'sourwull': 2, 'motiviation': 1, 'geooorgeee': 1, 'pffff': 1, 'definintion': 1, 'ehh': 1, 'riffic': 1, 'diffic': 1, 'hahhaha': 1, 'ahemmm': 1, 'interesteed': 1, 'percieve': 1, 'roflmao': 1, 'teehehehehehehe': 1, 'honeymustard': 2, 'strawberryclock': 1, 'evilest': 1, 'bedsocks': 1, 'wintows': 1, 'xanthrope': 1, 'mooooo': 1, 'mooo': 1, 'yoou': 1, 'vilken': 1, 'sjlen': 1, 'irrevelant': 1, 'botling': 2, 'corrrrrrrrrrect': 1, 'omfgwtfroflollmao': 1, 'tryed': 1, 'sweeeeet': 1, 'someting': 1, 'nd8hufdhuythbtoiueaomrpaek': 1, 'reasurring': 1, 'padwan': 1, 'latez': 1, 'mindreader': 1, 'awright': 1, 'ulitmately': 1, 'marrrrs': 2, 'highlord': 2, 'errr': 1, 'meeeep': 1, 'everythng': 1, 'dissapeared': 1, 'goatbeast': 1, 'fwoosh': 1, 'compser': 2, 'howabout': 2, 'baabababaaaaaaaa': 1, 'pigions': 1, 'neeeeeeeeeeeee': 1, 'youve': 1, 'mmmmmmmmm': 1, 'anyother': 1, 'slithy': 1, 'gire': 1, 'momeraths': 1, 'outgrabe': 1, 'jubjub': 1, 'frumious': 1, 'maxome': 1, 'tumtum': 1, 'uffish': 1, 'whiffling': 1, 'tulgey': 1, 'gallumping': 1, 'slainthe': 1, 'frabjous': 1, 'callooh': 1, 'callay': 1, 'subract': 1, 'suspiscious': 1, 'beeep': 1, 'mwhahaha': 1, 'blissfull': 1, 'ssssssssssssssssssssssss': 1, 'shoggoth': 1, 'shoggoths': 1, 'o_o': 1, 'lmao': 1, 'awwwwwwwwwww': 1, 'flmethrower': 1, 'defeatest': 1, 'blablablabla': 1, 'mwahahahaha': 1, 'smoooooch': 1, 'vaction': 1, 'suuuuuure': 1, 'parfois': 1, 'hehehe': 1, 'chatbots': 1, 'jawas': 1, 'secrure': 1, 'pepperment': 1, 'eveyone': 1, 'heebeejeebieme': 1, 'zigabytes': 1, 'povk': 1, 'sassing': 1, 'methought': 1, 'embarresed': 1, 'fingerwork': 1}\n"
     ]
    }
   ],
   "source": [
    "# Corrector dict from stackoverflow\n",
    "corrector_dict = { \n",
    "    \",\": \" , \" ,\"i'm\": \"i am\", \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"can not\",\"can't've\": \"cannot have\",\\\n",
    "    \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\\\n",
    "    \"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\\\n",
    "    \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\\\n",
    "    \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\\\n",
    "    \"he's\": \"he is\",\"how'd\": \"how did\",\"how'll\": \"how will\",\"how's\": \"how is\",\"i'd\": \"i would\",\\\n",
    "    \"i'll\": \"i will\",\"i'm\": \"i am\",\"i've\": \"i have\",\"isn't\": \"is not\",\"it'd\": \"it would\",\\\n",
    "    \"it'll\": \"it will\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"needn't\": \"need not\",\"oughtn't\": \"ought not\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"she'd\": \"she would\",\"she'll\": \"she will\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\"that'd\": \"that would\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'll\": \"they will\",\"they're\": \"they are\",\"they've\": \"they have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'll\": \"we will\",\"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"where'd\": \"where did\",\"where's\": \"where is\",\"who'll\": \"who will\",\"who's\": \"who is\",\"won't\": \"will not\",\"wouldn't\": \"would not\",\"you'd\": \"you would\",\"you'll\": \"you will\",\"you're\": \"you are\"\n",
    "}\n",
    "\n",
    "word_count, wordvec_embedding, not_found_words, X,Y = \\\n",
    "    rerunWordVecEmbedsPostCorrection(corrector_dict, wordvec_embedding )\n",
    "print('\\n Words  in text but not in glove = ', len(word_count) - len(words_found))\n",
    "print(not_found_words) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More cleaning to do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of word embedding is : 400002\n",
      "WordVector embedding of dimensions : 50\n"
     ]
    }
   ],
   "source": [
    "#getDictElems( wordvec_embedding, 2)\n",
    "print(\"Length of word embedding is :\", len(wordvec_embedding) )\n",
    "print(\"WordVector embedding of dimensions :\",len(wordvec_embedding['the']) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Add new words (Unfound) to Wordvec_embeddings\n",
    "\n",
    "1. WordVec_embeddings : Replaceall one occurence with UNK :: TO DO\n",
    "2. WordVec : Extend wordvec embedding to include these unknown words and 50d vector.\n",
    "3. Dictionary : Create Word2Id and Id2Word from new extended WordVec.\n",
    "4. Create input, encode, decode and target data\n",
    "5. Run it with script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_embed = list( np.random.uniform(-1.0, 1.0, 50 ) )\n",
    "len(new_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length prior to new word addition is : 400002\n",
      "Embedding length post new word addition is : 400195\n"
     ]
    }
   ],
   "source": [
    "print('Embedding length prior to new word addition is :', len(wordvec_embedding))\n",
    "emb_dims = len(wordvec_embedding['the']) #or use static 50 if it fails\n",
    "for word in not_found_words:\n",
    "      # if word not in glove embedding, create random embed\n",
    "      wordvec_embedding[word] = list( np.random.uniform(-1.0, 1.0, 50 ) ) \n",
    "    \n",
    "print('Embedding length post new word addition is :', len(wordvec_embedding))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Create Word2id,  id2word dict from wordvec_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 id is :  [{'_pad_': 0, '_unk_': 1, 'the': 2, ',': 3}]\n",
      "Id 2 word is :  [{0: '_pad_', 1: '_unk_', 2: 'the', 3: ','}]\n"
     ]
    }
   ],
   "source": [
    "idx = 0 \n",
    "word2id, id2word = {}, {}\n",
    "for word in wordvec_embedding:\n",
    "    word2id[word] = idx\n",
    "    id2word[idx] = word\n",
    "    idx +=1\n",
    "\n",
    "print('Word 2 id is : ', getDictElems(word2id, 4))\n",
    "print('Id 2 word is : ',getDictElems(id2word, 4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Add Start_id , end_id  to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'start_id'  not in word2id.keys():\n",
    "    start_id = len(word2id)\n",
    "    end_id = len(word2id) + 1\n",
    "\n",
    "word2id['start_id'], word2id['end_id'] = start_id, end_id\n",
    "id2word[start_id], id2word[end_id] = 'start_id', 'end_id'\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Input Data, Label\n",
    "\n",
    "1. X  : First line after '...'\n",
    "2. Y : All lines until  next '...'\n",
    "3. Convert X, Y to word_id  format for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is  [['are', 'you', 'real'], ['i', 'would', 'marry', 'for', 'the', 'love', 'of', 'my', 'life']]\n",
      "Y is  [['no', 'a', 'bot', 'is', 'real', 'i', 'am', 'not', 'a', 'bot', 'yes', 'you', 'are', 'prove', 'it', 'you', 'first', 'i', 'can', 'not', 'prove', 'something', 'i', 'do', 'not', 'believe', 'prove', 'the', 'halting', 'problem', 'you', 'first', 'see', 'you', 'are', 'not', 'that', 'is', 'what', 'i', 'told', 'you'], ['would', 'you', 'marry', 'me', 'sure', 'can', 'you', 'love', 'of', 'course', 'everyone', 'can', 'love', 'how', 'did', 'you', 'learn', 'i', 'learned', 'in', 'the', 'age', 'of', 'legends', 'people', 'nowadays', 'know', 'so', 'little', 'now', 'lie', 'on', 'the', 'bed', 'yes', 'maam', 'giggles', 'giggles', 'back']]\n"
     ]
    }
   ],
   "source": [
    "print('X is ', X[0:2])\n",
    "print('Y is ', Y[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1  Input Data (words) to word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_wordid is : [[34, 83, 569], [43, 56, 6638, 12, 2, 837, 5, 194, 216]]\n",
      "Y_wordid is : [[86, 9, 25458, 16, 569, 43, 915, 38, 9, 25458, 2774, 83, 34, 2829, 22, 83, 60, 43, 88, 38, 2829, 647, 43, 90, 38, 735, 2829, 2, 13059, 749, 83, 60, 255, 83, 34, 38, 14, 16, 104, 43, 156, 83], [56, 83, 6638, 287, 1087, 88, 83, 837, 5, 748, 1404, 88, 837, 199, 121, 83, 2370, 43, 2274, 8, 2, 466, 5, 10252, 71, 11204, 348, 102, 335, 116, 4623, 15, 2, 3829, 2774, 400002, 50912, 50912, 139]]\n"
     ]
    }
   ],
   "source": [
    "X_wordid, Y_wordid = [], []\n",
    "for chat in X:\n",
    "    X_wordid.append( [ word2id[word] for word in chat] )\n",
    "\n",
    "for chat_response in Y:\n",
    "    Y_wordid.append( [ word2id[word] for word in chat_response ]  )\n",
    "        \n",
    "        \n",
    "print('X_wordid is :',X_wordid[0:2])\n",
    "print('Y_wordid is :',Y_wordid[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Calc  Encoding  and  decoding  sequence length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn in seq mean & stdev : 5.601489757914339   3.5508329885869556\n",
      "rnn out seq mean & stdev : 68.77467411545624   67.88573123796462\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "## Calc mean and stdev to fix  rnn_seq_in_lenght and rnn_seq_out_length\n",
    "#####\n",
    "X_lengths = [len(lst) for lst in X]\n",
    "Y_lengths = [len(lst) for lst in Y]\n",
    "#print('X_length is ', X_lengths[:2])\n",
    "\n",
    "rnn_in_seq_mean = np.mean(X_lengths)\n",
    "rnn_in_seq_stdv = np.std(X_lengths)\n",
    "rnn_out_seq_mean = np.mean(Y_lengths)\n",
    "rnn_out_seq_stdv = np.std(Y_lengths)\n",
    "\n",
    "#print('Conv starter is ', conv_starter[:2])\n",
    "#print('Rnn out seq is ', responses[:2])\n",
    "print('rnn in seq mean & stdev :',  rnn_in_seq_mean, ' ', rnn_in_seq_stdv)\n",
    "print('rnn out seq mean & stdev :',  rnn_out_seq_mean, ' ', rnn_out_seq_stdv)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.3 Calc Rnn in/out seq lengths @ 84% coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_in_seq_len =  9.152322746501294  .rnn_out_seq_len : 136.66040535342086\n"
     ]
    }
   ],
   "source": [
    "# 1 sd away from mean = 84% coverage\n",
    "rnn_in_seq_len =  rnn_in_seq_mean + rnn_in_seq_stdv\n",
    "rnn_out_seq_len = rnn_out_seq_mean + rnn_out_seq_stdv\n",
    "\n",
    "print('rnn_in_seq_len = ', rnn_in_seq_len, ' .rnn_out_seq_len :', rnn_out_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No of chats : 537 537\n",
      "Sample 2 trainX : [[34, 83, 569], [43, 56, 6638, 12, 2, 837, 5, 194, 216]]\n",
      "Sample 2 trainY : [[86, 9, 25458, 16, 569, 43, 915, 38, 9, 25458, 2774, 83, 34, 2829, 22, 83, 60, 43, 88, 38, 2829, 647, 43, 90, 38, 735, 2829, 2, 13059, 749, 83, 60, 255, 83, 34, 38, 14, 16, 104, 43, 156, 83], [56, 83, 6638, 287, 1087, 88, 83, 837, 5, 748, 1404, 88, 837, 199, 121, 83, 2370, 43, 2274, 8, 2, 466, 5, 10252, 71, 11204, 348, 102, 335, 116, 4623, 15, 2, 3829, 2774, 400002, 50912, 50912, 139]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total No of chats :\", len(X_wordid), len(X))\n",
    "trainX, trainY = X_wordid, Y_wordid\n",
    "\n",
    "print(\"Sample 2 trainX :\", trainX[0:2])\n",
    "print(\"Sample 2 trainY :\", trainY[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Create Dataset : Batch major (Not Time major)\n",
    "\n",
    "Encoding and decoding length fixed.\n",
    "\n",
    "Dataset should look like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  start_id :  400195  . end_id :  400196\n",
      "\n",
      "_encode_seqs :  [[104, 16, 394, 313, 0, 0, 0, 0, 0], [35, 83, 663, 53, 8, 837, 0, 0, 0]]\n",
      "\n",
      "_decode_seqs :  [[400195, 194, 313, 16, 400003, 38, 2, 400003, 86, 122, 400003, 14, 16, 194, 313, 319, 590, 83, 392, 32, 19021, 38, 590, 104, 16, 394, 313, 400003, 14, 16, 194, 313, 319, 400003, 2774, 83, 119, 103, 43, 90, 38, 348, 104, 83, 1704, 3204, 2774, 83, 90, 194, 1411, 83, 34, 12312, 287, 14, 16, 221, 86, 22, 16, 38, 129, 104, 16, 221, 104, 16, 394, 2802, 194, 2802, 43, 90, 38, 348, 43, 915, 93, 10738, 14, 16, 66, 250, 88, 83, 1363, 287, 647, 2774, 104, 90, 83, 305, 6, 348, 43, 56, 119, 6, 348, 1176, 43, 88, 38, 1363, 83, 1176, 145, 122, 467, 7, 1183, 55, 45, 171, 133, 22, 43, 90, 38, 348, 113, 6, 467, 24, 2, 1086, 104, 16, 2, 1086, 104, 1581, 51, 2], [400195, 2774, 63, 37, 83, 8, 837, 43, 915, 151, 8, 837, 40, 34, 83, 8, 837, 19, 9, 1751, 41, 34, 60180, 43, 271, 102, 319, 102, 740, 90, 38, 83, 119, 103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "_target_seqs :  [[194, 313, 16, 400003, 38, 2, 400003, 86, 122, 400003, 14, 16, 194, 313, 319, 590, 83, 392, 32, 19021, 38, 590, 104, 16, 394, 313, 400003, 14, 16, 194, 313, 319, 400003, 2774, 83, 119, 103, 43, 90, 38, 348, 104, 83, 1704, 3204, 2774, 83, 90, 194, 1411, 83, 34, 12312, 287, 14, 16, 221, 86, 22, 16, 38, 129, 104, 16, 221, 104, 16, 394, 2802, 194, 2802, 43, 90, 38, 348, 43, 915, 93, 10738, 14, 16, 66, 250, 88, 83, 1363, 287, 647, 2774, 104, 90, 83, 305, 6, 348, 43, 56, 119, 6, 348, 1176, 43, 88, 38, 1363, 83, 1176, 145, 122, 467, 7, 1183, 55, 45, 171, 133, 22, 43, 90, 38, 348, 113, 6, 467, 24, 2, 1086, 104, 16, 2, 1086, 104, 1581, 51, 2, 1086], [2774, 63, 37, 83, 8, 837, 43, 915, 151, 8, 837, 40, 34, 83, 8, 837, 19, 9, 1751, 41, 34, 60180, 43, 271, 102, 319, 102, 740, 90, 38, 83, 119, 103, 400196, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "_target_mask :  [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Encode_seq : ['How', 'are' , 'you', <PAD_ID>, <PAD_ID>]\n",
    "====================== Thought vectors =======================================\n",
    "Decode_seq : ['<START_ID>', 'I',  'am',   'fine',   <PAD_ID>, <PAD_ID>, <PAD_ID>, <PAD_ID>  ]\n",
    "Target_seq : [ 'I',         'am', 'fine', <END_ID>  <PAD_ID>, <PAD_ID>, <PAD_ID>, <PAD_ID>  ]\n",
    "Target_mask: [  1           ,1     ,1      ,1        ,0          ,0         ,0      ,0      ]\n",
    "\"\"\"\n",
    "\n",
    "def getEncodeNDecode(X,Y):    \n",
    "    #max_len_x = max([ len(chats) for chats in  X ]   )\n",
    "    #max_len_y = max([ len(response) for response in  Y ]   )\n",
    "    \n",
    "    max_len_x, max_len_y = 99999999,999999999\n",
    "    \n",
    "    max_padlen_x =  int(rnn_in_seq_len) if (int(rnn_in_seq_len) < max_len_x ) else max_len_x\n",
    "    max_padlen_y =  int(rnn_out_seq_len) if (int(rnn_out_seq_len) < max_len_y ) else max_len_y\n",
    "    \n",
    "    _encode_seqs = tl.prepro.pad_sequences(X, maxlen=max_padlen_x, \\\n",
    "                                padding='post', truncating='post', value=word2id['_pad_'] )\n",
    "    \n",
    "    _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n",
    "    _decode_seqs = tl.prepro.pad_sequences(_decode_seqs, maxlen=max_padlen_y,\\\n",
    "                                padding='post', truncating='post', value=word2id['_pad_'] )\n",
    "    \n",
    "    _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n",
    "    _target_seqs = tl.prepro.pad_sequences(_target_seqs, maxlen=max_padlen_y,\\\n",
    "                                padding='post', truncating='post', value=word2id['_pad_'])\n",
    "\n",
    "    _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n",
    "    \n",
    "    return _encode_seqs, _decode_seqs, _target_seqs, _target_mask\n",
    "    \n",
    "_encode_seqs, _decode_seqs, _target_seqs, _target_mask = getEncodeNDecode(\\\n",
    "                                                    X_wordid[3:5], Y_wordid[3:5])\n",
    "\n",
    "print('\\n  start_id : ',word2id['start_id'],' . end_id : ', word2id['end_id'])\n",
    "#print('\\n_encode_seqs : ',seqId2Words( _encode_seqs) )\n",
    "print('\\n_encode_seqs : ',_encode_seqs) \n",
    "print('\\n_decode_seqs : ',_decode_seqs)\n",
    "print('\\n_target_seqs : ',_target_seqs)\n",
    "print('\\n_target_mask : ',_target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['what', 'is', 'your', 'name', '_pad_'],\n",
       " ['have', 'you', 'ever', 'been', 'in']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seqId2Words(seq, get_first_n_words_only = 5):\n",
    "    lst = []\n",
    "    for word_lst in seq:\n",
    "        #print(word_lst)\n",
    "        lst.append([ id2word[word_id] if word_id in id2word.keys() else word_id\\\n",
    "                    for word_id  in word_lst[0:get_first_n_words_only]  ] )            \n",
    "    return  lst\n",
    "\n",
    "lst = [[104, 16, 394, 313, 0, 0, 0, 0, 0], [35, 83, 663, 53, 8, 837, 0, 0, 0]]\n",
    "\n",
    "seqId2Words(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[104]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Rnn Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Rnn param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "batch_size = 4\n",
    "xvocab_size = len(id2word) \n",
    "emb_dim = emb_dims\n",
    "rnn_num_layers = 1\n",
    "dropout_rate = 0.2\n",
    "\n",
    "\n",
    "#  decay learning rate by 25% every 10 step\n",
    "lr_decay_rate =  0.75\n",
    "lr_decay_steps = 10\n",
    "max_grad_clip_to = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Word Embedding Dict to Array\n",
    "\n",
    "Because : EmbeddingInput layers expects the word embeddings to be in array format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2780087830967479, -0.2291490430363281, 0.4263150069620718, 0.8240796365346232, 0.11578251058121602, 0.9783727681590155, 0.6802844059978832, -0.5722520069753132, -0.08437865859280058, 0.007096877190771345, -0.7691950386127993, 0.2704172290790836, -0.20767185852296466, -0.7476782399241471, 0.7596679100535166, -0.8262741784207053, 0.28771884679222604, 0.5728380437464076, -0.4025040775341664, -0.6707764355845043, -0.3360247065696329, -0.8520812026620561, -0.012002413791471156, -0.4155591553458333, 0.05833922642948175, 0.8510067796579759, 0.8050541550133667, 0.6686013492498821, -0.17922428851998817, 0.513702643378094, -0.13408779635199886, 0.6940429323134196, 0.7563422564788356, -0.10907479377709217, -0.09714066317431103, 0.33663577660370736, 0.5674116625579169, 0.6331483626142311, 0.8262918688801566, 0.9123801030584786, -0.9773072563426572, -0.44262581247627697, 0.7593012716463687, 0.7422919238810088, 0.5070318924622625, -0.8808271737635853, 0.027892710372377394, 0.9814989928190994, -0.46743485997621437, 0.4465173857244342]]\n"
     ]
    }
   ],
   "source": [
    "word_embedding_matrix = []\n",
    "for  word in word2id:    \n",
    "    #print('word not found :', word)\n",
    "    if word not in ('start_id','end_id'):\n",
    "        word_embedding_matrix.append(list(wordvec_embedding[word]) )              \n",
    "print( word_embedding_matrix[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2780087830967479,\n",
       " -0.2291490430363281,\n",
       " 0.4263150069620718,\n",
       " 0.8240796365346232,\n",
       " 0.11578251058121602,\n",
       " 0.9783727681590155,\n",
       " 0.6802844059978832,\n",
       " -0.5722520069753132,\n",
       " -0.08437865859280058,\n",
       " 0.007096877190771345,\n",
       " -0.7691950386127993,\n",
       " 0.2704172290790836,\n",
       " -0.20767185852296466,\n",
       " -0.7476782399241471,\n",
       " 0.7596679100535166,\n",
       " -0.8262741784207053,\n",
       " 0.28771884679222604,\n",
       " 0.5728380437464076,\n",
       " -0.4025040775341664,\n",
       " -0.6707764355845043,\n",
       " -0.3360247065696329,\n",
       " -0.8520812026620561,\n",
       " -0.012002413791471156,\n",
       " -0.4155591553458333,\n",
       " 0.05833922642948175,\n",
       " 0.8510067796579759,\n",
       " 0.8050541550133667,\n",
       " 0.6686013492498821,\n",
       " -0.17922428851998817,\n",
       " 0.513702643378094,\n",
       " -0.13408779635199886,\n",
       " 0.6940429323134196,\n",
       " 0.7563422564788356,\n",
       " -0.10907479377709217,\n",
       " -0.09714066317431103,\n",
       " 0.33663577660370736,\n",
       " 0.5674116625579169,\n",
       " 0.6331483626142311,\n",
       " 0.8262918688801566,\n",
       " 0.9123801030584786,\n",
       " -0.9773072563426572,\n",
       " -0.44262581247627697,\n",
       " 0.7593012716463687,\n",
       " 0.7422919238810088,\n",
       " 0.5070318924622625,\n",
       " -0.8808271737635853,\n",
       " 0.027892710372377394,\n",
       " 0.9814989928190994,\n",
       " -0.46743485997621437,\n",
       " 0.4465173857244342]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_matrix[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.2 RNN Many to Many Model Defn\n",
    "\n",
    "1. **Encoding layer**: Look up layer to lookup word id into embedding.\n",
    "2. **Decoding layer**: Look up layer to lookup word id into embedding.\n",
    "3. **Seq2Seq layer**: A simple dynamic Rnn layer.\n",
    "      \n",
    "\n",
    "REF: https://tensorlayer.readthedocs.io/en/stable/modules/layers.html#tensorlayer.layers.Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(encode_seqs, decode_seqs, is_train=True, reuse=False):\n",
    "    with tf.variable_scope(\"model\", reuse=reuse):\n",
    "        with tf.variable_scope(\"embedding\") as vs:\n",
    "            # same embedding / thought vector to be used in both encoding and decoding\n",
    "            # Embedding Input layers is a look up table for  wordid to embedding.            \n",
    "            # same embedding / thought vector to be used in both encoding and decoding\n",
    "            encoding_layer = EmbeddingInputlayer(\n",
    "                inputs = encode_seqs,\n",
    "                vocabulary_size = xvocab_size,\n",
    "                embedding_size = emb_dim,\n",
    "                # use glove embedding vectors as starting point\n",
    "                E_init = tf.constant_initializer(word_embedding_matrix, dtype=tf.float32),\n",
    "                name = 'encode_decode_seq_embedding')\n",
    "            vs.reuse_variables()\n",
    "            # Embedding Input layers is a look up table for  wordid to embedding.\n",
    "            decoding_layer = EmbeddingInputlayer(\n",
    "                inputs = decode_seqs,\n",
    "                vocabulary_size = xvocab_size,\n",
    "                embedding_size = emb_dim,\n",
    "                # use glove embedding vectors as starting point\n",
    "                E_init = tf.constant_initializer(word_embedding_matrix, dtype=tf.float32),\n",
    "                name = 'encode_decode_seq_embedding')\n",
    "        \n",
    "        print('Seq2seq i/p : Encodinglayer must be of size [batch_sie, None, n_featueres] ',\\\n",
    "              encoding_layer)\n",
    "       \n",
    "        net_rnn = Seq2Seq(encoding_layer, decoding_layer,\n",
    "                cell_fn = tf.contrib.rnn.BasicLSTMCell,\n",
    "                n_hidden = emb_dim,\n",
    "                # Using Xavier initializer for better perf\n",
    "                #initializer = tf.random_uniform_initializer(-0.1, 0.1),\n",
    "                initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                encode_sequence_length = retrieve_seq_length_op2(encode_seqs),\n",
    "                decode_sequence_length = retrieve_seq_length_op2(decode_seqs),\n",
    "                initial_state_encode = None,\n",
    "                dropout = (dropout_rate if is_train else None),                \n",
    "                n_layer = rnn_num_layers,\n",
    "                return_seq_2d = True,\n",
    "                name = 'seq2seq_rnn')   \n",
    "        \n",
    "        # Fully connected layer with relu activation\n",
    "        net_out = DenseLayer(net_rnn, n_units=xvocab_size, act=tf.nn.relu, name='output')\n",
    "    return net_out, net_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Tf Placeholder\n",
    "\n",
    "1. **Training Input Placeholder** :\n",
    "2. **Inference Input Placeholders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for training\n",
    "encode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"encode_seqs\")\n",
    "decode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"decode_seqs\")\n",
    "target_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_seqs\")\n",
    "target_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_mask\") \n",
    "# tl.prepro.sequences_get_mask()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Model Train Defn\n",
    "\n",
    "VIMP : During training, the model should not be reused because we will be iteratively making it more and more better. However in test  case, we must set the model reuse to true, as we do want the previously trained model without any modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] EmbeddingInputlayer model/embedding/encode_decode_seq_embedding: (400197, 50)\n",
      "[TL] EmbeddingInputlayer model/embedding/encode_decode_seq_embedding: (400197, 50)\n",
      "Seq2seq i/p : Encodinglayer must be of size [batch_sie, None, n_featueres]    Last layer is: EmbeddingInputlayer (model/embedding/encode_decode_seq_embedding) [4, None, 50]\n",
      "[TL] [*] Seq2Seq model/seq2seq_rnn: n_hidden: 50 cell_fn: BasicLSTMCell dropout: 0.2 n_layer: 1\n",
      "[TL] DynamicRNNLayer model/seq2seq_rnn/encode: n_hidden: 50, in_dim: 3 in_shape: (4, ?, 50) cell_fn: BasicLSTMCell dropout: 0.2 n_layer: 1\n",
      "[TL]        batch_size (concurrent processes): 4\n",
      "[TL] DynamicRNNLayer model/seq2seq_rnn/decode: n_hidden: 50, in_dim: 3 in_shape: (4, ?, 50) cell_fn: BasicLSTMCell dropout: 0.2 n_layer: 1\n",
      "[TL]        batch_size (concurrent processes): 4\n",
      "[TL] DenseLayer  model/output: 400197 relu\n"
     ]
    }
   ],
   "source": [
    "net_out, _ = model(encode_seqs, decode_seqs, is_train=True, reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] EmbeddingInputlayer model/embedding/encode_decode_seq_embedding: (400197, 50)\n",
      "[TL] EmbeddingInputlayer model/embedding/encode_decode_seq_embedding: (400197, 50)\n",
      "Seq2seq i/p : Encodinglayer must be of size [batch_sie, None, n_featueres]    Last layer is: EmbeddingInputlayer (model/embedding/encode_decode_seq_embedding) [1, None, 50]\n",
      "[TL] [*] Seq2Seq model/seq2seq_rnn: n_hidden: 50 cell_fn: BasicLSTMCell dropout: None n_layer: 1\n",
      "[TL] DynamicRNNLayer model/seq2seq_rnn/encode: n_hidden: 50, in_dim: 3 in_shape: (1, ?, 50) cell_fn: BasicLSTMCell dropout: None n_layer: 1\n",
      "[TL]        batch_size (concurrent processes): 1\n",
      "[TL] DynamicRNNLayer model/seq2seq_rnn/decode: n_hidden: 50, in_dim: 3 in_shape: (1, ?, 50) cell_fn: BasicLSTMCell dropout: None n_layer: 1\n",
      "[TL]        batch_size (concurrent processes): 1\n",
      "[TL] DenseLayer  model/output: 400197 relu\n"
     ]
    }
   ],
   "source": [
    "# model for inferencing\n",
    "encode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\n",
    "decode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\n",
    "net, net_rnn = model(encode_seqs2, decode_seqs2, is_train=False, reuse=True)\n",
    "y = tf.nn.softmax(net.outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Loss  Defn\n",
    "\n",
    "1. **Mask Loss**: is used. Mask is multiplied by calculated loss, to ensure padding losses are not accounted during loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  we calculate  cross entropy with mask \n",
    "# because we do not want  the padding to have any effect on the lonss\n",
    "loss = tl.cost.cross_entropy_seq_with_mask(logits=net_out.outputs, target_seqs=target_seqs,\\\n",
    "                                    input_mask=target_mask, return_details=False, name='cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Optimizer Defn : Decaying rate & Grad Clip\n",
    "\n",
    "1. **Vanishing Gradient :** Since we are using relu, vanishing gradient is not a  big problem. \n",
    "2. **Exploding Gradient :** : Clipping gradients to prevent the problem\n",
    "3. **Optimiser :** Adam Optimisers tend to converge faster. SGD although slow tend to outperfom Adam however\n",
    "4. **Decaying learning rate**: For adam it is not required, because ADAM guarantees square root decay as per theorem 4.1 \n",
    "\n",
    "t <- t +1\n",
    "\n",
    "lr_t <- learning_rate * sqrt(1 - beta2^t) / (1 - beta1^t)\n",
    "\n",
    "PS:  Theorem 4.1 of their ICLR article, one of their hypotheses is that the learning rate has a square root decay, αt=α/t√. Furthermore, for their logistic regression experiments they use the square root decay as well. \n",
    "Ref: https://stats.stackexchange.com/questions/200063/adam-optimizer-with-exponential-decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate : 0.1\n"
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "\n",
    "# Learning rate decay ~ lr * decay_rate ^(global_step/ decay_step)\n",
    "print('learning rate :', learning_rate)\n",
    "#learning_rate = tf.train.exponential_decay(\n",
    "#    lr, global_step, decay_steps=lr_decay_steps,\\\n",
    "#    decay_rate=lr_decay_rate, staircase=True)\n",
    "\n",
    "#lr = tf.placeholder(tf.float32, name = 'learning_rate')\n",
    "# We define Adam Optimizer\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Gradient clipping\n",
    "#gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "#gradients, _ = tf.clip_by_global_norm(gradients, max_grad_clip_to)\n",
    "#optimizer = optimizer.apply_gradients(\n",
    "#    zip(gradients, v))\n",
    "\n",
    "#train_op = optimizer\n",
    "\n",
    "#net_out.print_params(False)\n",
    "#train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[83, 34, 38, 194, 1411, 0, 0, 0, 0], [194, 713, 16, 994, 0, 0, 0, 0, 0], [102, 122, 115, 43, 1473, 656, 14983, 83, 271], [34, 83, 1091, 1098, 0, 0, 0, 0, 0]]\n",
      "[[104, 34, 394, 2722, 0, 0, 0, 0, 0], [34, 83, 9, 953, 0, 0, 0, 0, 0], [7476, 2253, 0, 0, 0, 0, 0, 0, 0], [83, 34, 194, 193, 256, 1411, 0, 0, 0]]\n",
      "[[43, 119, 7377, 0, 0, 0, 0, 0, 0], [88, 55, 467, 76, 0, 0, 0, 0, 0], [57, 0, 0, 0, 0, 0, 0, 0, 0], [83, 34, 9, 1858, 0, 0, 0, 0, 0]]\n",
      "[[63, 37, 83, 957, 129, 0, 0, 0, 0], [104, 90, 83, 843, 6, 90, 0, 0, 0], [43, 915, 6334, 14, 2, 4365, 16, 38, 10016], [43, 915, 224, 6, 2, 5579, 0, 0, 0]]\n",
      "[[43, 915, 38, 1903, 6, 83, 132, 58, 0], [43, 915, 9, 475, 0, 0, 0, 0, 0], [83, 34, 9, 953, 0, 0, 0, 0, 0], [2474, 104, 34, 83, 1903, 61, 0, 0, 0]]\n",
      "[[83, 35, 86, 13340, 199, 6, 26949, 48429, 19], [88, 83, 1681, 0, 0, 0, 0, 0, 0], [102, 104, 121, 83, 35, 12, 4155, 0, 0], [90, 83, 3712, 758, 15, 2, 927, 0, 0]]\n",
      "[[43, 915, 2544, 8, 6880, 7636, 7, 8078, 0], [90, 83, 119, 2, 45999, 0, 0, 0, 0], [199, 262, 9, 953, 3625, 6244, 0, 0, 0], [43, 915, 8, 837, 19, 9, 1751, 0, 0]]\n",
      "[[83, 416, 32, 1903, 6, 2, 1799, 901, 0], [199, 183, 8166, 90, 83, 35, 0, 0, 0], [102, 104, 90, 83, 90, 0, 0, 0, 0], [113, 1728, 191, 41, 32, 0, 0, 0, 0]]\n",
      "[[10269, 0, 0, 0, 0, 0, 0, 0, 0], [1363, 287, 45, 43, 663, 4225, 194, 5561, 0], [34, 83, 287, 0, 0, 0, 0, 0, 0], [15919, 888, 97, 255, 85, 83, 88, 2717, 104]]\n",
      "[[83, 513, 9, 532, 0, 0, 0, 0, 0], [1176, 16, 1448, 36, 65, 34, 79, 656, 14], [104, 90, 83, 271, 2, 6699, 16, 0, 0], [34, 83, 11116, 0, 0, 0, 0, 0, 0]]\n",
      "[[37126, 16, 2, 8131, 18519, 5, 98, 927, 2591], [6707, 1228, 16, 569, 0, 0, 0, 0, 0], [43, 119, 6244, 0, 0, 0, 0, 0, 0], [211668, 1903, 35681, 0, 0, 0, 0, 0, 0]]\n",
      "[[40, 118, 83, 0, 0, 0, 0, 0, 0], [104, 16, 9, 25458, 0, 0, 0, 0, 0], [43, 119, 16219, 0, 0, 0, 0, 0, 0], [43, 93327, 1363, 287, 9, 7353, 0, 0, 0]]\n",
      "[[194, 313, 16, 5287, 53199, 7, 43, 915, 9], [199, 169, 915, 43, 0, 0, 0, 0, 0], [43, 915, 38, 1087, 199, 183, 277, 43, 88], [90, 83, 735, 8, 1535, 0, 0, 0, 0]]\n",
      "[[13077, 194, 313, 16, 651, 7, 43, 915, 2], [34, 83, 2, 21951, 0, 0, 0, 0, 0], [104, 16, 2, 2170, 6, 216, 2, 5189, 7], [43, 915, 9, 475, 177, 651, 0, 0, 0]]\n",
      "[[145, 45, 71, 271, 442, 5, 287, 122, 115], [90, 83, 837, 287, 319, 0, 0, 0, 0], [25004, 7098, 16, 21, 994, 21, 25004, 806, 0], [2774, 43, 915, 0, 0, 0, 0, 0, 0]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0], [7944, 0, 0, 0, 0, 0, 0, 0, 0], [65, 16, 50, 997, 0, 0, 0, 0, 0], [8237, 1079, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[5468, 25458, 0, 0, 0, 0, 0, 0, 0], [43, 915, 224, 6, 3829, 722, 0, 0, 0], [400012, 104114, 9, 2656, 2627, 8, 9, 2063, 7], [199, 34, 83, 2520, 375, 0, 0, 0, 0]]\n",
      "[[83, 34, 38, 569, 0, 0, 0, 0, 0], [22, 16, 221, 6, 119, 62321, 319, 16, 38], [9563, 34, 38, 83, 0, 0, 0, 0, 0], [104, 16, 2, 2170, 6, 216, 2, 5189, 7]]\n",
      "[[5553, 83, 0, 0, 0, 0, 0, 0, 0], [83, 392, 35, 287, 11566, 19, 1320, 1728, 0], [400012, 0, 0, 0, 0, 0, 0, 0, 0], [104, 16, 194, 313, 0, 0, 0, 0, 0]]\n",
      "[[104, 88, 83, 90, 12, 287, 0, 0, 0], [34, 83, 1215, 0, 0, 0, 0, 0, 0], [104, 34, 83, 0, 0, 0, 0, 0, 0], [34, 83, 9, 7433, 0, 0, 0, 0, 0]]\n",
      "[[83, 34, 14035, 0, 0, 0, 0, 0, 0], [43, 263, 9, 81, 2360, 0, 0, 0, 0], [104, 90, 71, 205, 61, 83, 0, 0, 0], [86, 86, 86, 104, 90, 83, 305, 6, 4605]]\n",
      "[[43, 17, 2414, 58, 15, 104, 83, 18, 0], [90, 37633, 35, 1331, 0, 0, 0, 0, 0], [43, 56, 6638, 12, 2, 837, 5, 194, 216], [104, 34, 83, 0, 0, 0, 0, 0, 0]]\n",
      "[[55, 191, 1079, 61, 837, 79, 58, 0, 0], [104, 121, 83, 35, 12, 4155, 375, 0, 0], [888, 97, 38, 1079, 61, 287, 66, 2, 81], [43, 35, 9, 2116, 1266, 0, 0, 0, 0]]\n",
      "[[104, 191, 43, 32, 916, 250, 116, 0, 0], [16, 938, 647, 0, 0, 0, 0, 0, 0], [43, 735, 14, 43, 90, 3606, 0, 0, 0], [104, 90, 83, 271, 61, 2, 1622, 24, 2]]\n",
      "[[24058, 0, 0, 0, 0, 0, 0, 0, 0], [34, 83, 9, 9249, 0, 0, 0, 0, 0], [43, 915, 2621, 0, 0, 0, 0, 0, 0], [88, 43, 171, 394, 225, 0, 0, 0, 0]]\n",
      "[[400134, 14, 16, 194, 1390, 38, 9, 313, 0], [93932, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [90, 83, 171, 68, 183, 0, 0, 0, 0]]\n",
      "[[43, 1450, 83, 6, 9, 12917, 0, 0, 0], [83, 96, 38, 153, 1282, 2, 35854, 730, 0], [43, 915, 9, 400044, 43, 915, 9, 65492, 136], [199, 175, 35, 55, 53, 20992, 0, 0, 0]]\n",
      "[[104, 915, 43, 129, 0, 0, 0, 0, 0], [3834, 94, 394, 749, 0, 0, 0, 0, 0], [83, 90, 38, 161, 1382, 0, 0, 0, 0], [20, 16, 9, 302, 0, 0, 0, 0, 0]]\n",
      "[[888, 97, 161, 837, 38, 138, 0, 0, 0], [38889, 20810, 7, 2983, 5, 400044, 6528, 0, 0], [39, 16, 2, 158, 0, 0, 0, 0, 0], [40, 34, 83, 0, 0, 0, 0, 0, 0]]\n",
      "[[221, 12540, 0, 0, 0, 0, 0, 0, 0], [34, 83, 2979, 0, 0, 0, 0, 0, 0], [83, 34, 9, 9249, 0, 0, 0, 0, 0], [90, 83, 735, 14, 6707, 1228, 16, 9, 221]]\n",
      "[[86, 43, 915, 9, 9249, 0, 0, 0, 0], [104, 56, 83, 119, 6, 4605, 0, 0, 0], [34, 83, 1091, 1098, 0, 0, 0, 0, 0], [43, 21130, 1079, 6, 2, 4436, 1858, 380, 0]]\n",
      "[[7, 2, 58, 101707, 28, 1972, 34, 6, 2], [102, 83, 34, 9, 322707, 3084, 6, 702, 83], [90, 83, 305, 6, 2370, 0, 0, 0, 0], [43, 915, 224, 6, 3829, 0, 0, 0, 0]]\n",
      "[[35, 83, 3609, 6, 651, 0, 0, 0, 0], [43, 915, 2112, 0, 0, 0, 0, 0, 0], [43, 915, 9, 5452, 0, 0, 0, 0, 0], [83, 34, 193, 5188, 121, 83, 348, 14, 0]]\n",
      "[[3082, 83, 3712, 7479, 0, 0, 0, 0, 0], [199, 34, 83, 0, 0, 0, 0, 0, 0], [22, 262, 38, 90, 287, 132, 221, 6, 38], [43, 1000, 21170, 0, 0, 0, 0, 0, 0]]\n",
      "[[90, 83, 35, 132, 1382, 5, 6204, 0, 0], [1320, 444, 18, 65, 16, 86, 837, 93, 4297], [4296, 0, 0, 0, 0, 0, 0, 0, 0], [43, 915, 9, 901, 9755, 7, 1456, 0, 0]]\n",
      "[[43, 348, 9, 193, 221, 864, 1751, 8, 363], [83, 34, 5188, 0, 0, 0, 0, 0, 0], [90, 83, 35, 1678, 426, 2165, 0, 0, 0], [43, 915, 9, 475, 83, 348, 14, 250, 0]]\n",
      "[[43, 90, 38, 2201, 1694, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [35, 938, 6, 205, 0, 0, 0, 0, 0], [111, 71, 1079, 8, 46, 4296, 0, 0, 0]]\n",
      "[[88, 83, 11052, 287, 62, 0, 0, 0, 0], [43, 271, 14, 83, 34, 9, 953, 373, 677], [34, 83, 9, 2360, 0, 0, 0, 0, 0], [34, 83, 9, 569, 901, 115, 43, 915, 199]]\n",
      "[[11085, 25458, 0, 0, 0, 0, 0, 0, 0], [199, 34, 83, 0, 0, 0, 0, 0, 0], [1129, 6, 9, 2367, 5, 1518, 0, 0, 0], [104, 90, 83, 90, 12, 2907, 0, 0, 0]]\n",
      "[[104, 1728, 16, 65, 6, 1079, 61, 0, 0], [55, 410, 172, 68382, 0, 0, 0, 0, 0], [199, 169, 34, 83, 129, 0, 0, 0, 0], [4864, 90, 83, 305, 6, 843, 535, 1208, 0]]\n",
      "[[394, 313, 16, 0, 0, 0, 0, 0, 0], [43, 348, 83, 34, 36, 104, 915, 43, 0], [90, 83, 3606, 0, 0, 0, 0, 0, 0], [43, 915, 569, 0, 0, 0, 0, 0, 0]]\n",
      "[[43, 915, 2544, 0, 0, 0, 0, 0, 0], [394, 1723, 12912, 50, 5, 5653, 62860, 41130, 71717], [13218, 4976, 0, 0, 0, 0, 0, 0, 0], [3792, 45, 3429, 6, 3606, 444, 55, 35, 1156]]\n",
      "[[199, 88, 83, 163, 7, 38, 161, 310, 0], [83, 478, 111, 1210, 375, 7, 83, 692, 1916], [43, 915, 24, 165, 83, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[43, 915, 9, 475, 83, 34, 9, 953, 83], [2359, 0, 0, 0, 0, 0, 0, 0, 0], [113, 83, 328, 27, 0, 0, 0, 0, 0], [43, 119, 1948, 0, 0, 0, 0, 0, 0]]\n",
      "[[1363, 287, 9, 525, 0, 0, 0, 0, 0], [83, 34, 122, 31, 21541, 0, 0, 0, 0], [205, 647, 5468, 0, 0, 0, 0, 0, 0], [104, 16, 394, 313, 0, 0, 0, 0, 0]]\n",
      "[[221, 83, 34, 2743, 0, 0, 0, 0, 0], [199, 169, 34, 83, 0, 0, 0, 0, 0], [90, 83, 3469, 83, 96, 7035, 0, 0, 0], [40, 16, 1535, 0, 0, 0, 0, 0, 0]]\n",
      "[[1363, 287, 647, 61, 4963, 0, 0, 0, 0], [43, 96, 32, 394, 1411, 0, 0, 0, 0], [113, 121, 83, 244, 15, 394, 5758, 0, 0], [145, 194, 3019, 16, 1924, 12224, 36, 432, 7]]\n",
      "[[104, 90, 83, 582, 4963, 0, 0, 0, 0], [35, 83, 663, 53, 8, 837, 0, 0, 0], [88, 43, 1363, 83, 9, 1781, 0, 0, 0], [400094, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[22, 16, 9, 24637, 9, 1280, 84707, 5, 8541], [4377, 740, 34, 83, 4377, 0, 0, 0, 0], [88, 83, 28978, 647, 0, 0, 0, 0, 0], [394, 15077, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[104, 16, 394, 313, 0, 0, 0, 0, 0], [104, 35, 43, 753, 14, 16, 8981, 0, 0], [104, 34, 83, 0, 0, 0, 0, 0, 0], [104, 34, 83, 520, 12, 5982, 0, 0, 0]]\n",
      "[[43, 915, 6334, 0, 0, 0, 0, 0, 0], [43, 3469, 12, 938, 36, 14, 83, 416, 1348], [36, 2234, 0, 0, 0, 0, 0, 0, 0], [43, 915, 5281, 83, 88, 38, 7301, 0, 0]]\n",
      "[[199, 183, 90, 83, 8957, 0, 0, 0, 0], [34, 83, 1087, 83, 34, 475, 0, 0, 0], [90, 83, 119, 287, 0, 0, 0, 0, 0], [3995, 16, 2, 1717, 5, 4582, 0, 0, 0]]\n",
      "[[90, 83, 271, 0, 0, 0, 0, 0, 0], [12540, 12540, 0, 0, 0, 0, 0, 0, 0], [83, 392, 32, 883, 6585, 0, 0, 0, 0], [104, 16, 394, 313, 0, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43, 915, 9, 11870, 0, 0, 0, 0, 0], [43, 5752, 643, 1948, 0, 0, 0, 0, 0], [83, 34, 9, 441, 122780, 85, 43, 90, 38], [43, 35, 6, 244, 116, 7, 893, 83, 0]]\n",
      "[[4864, 888, 97, 284, 0, 0, 0, 0, 0], [664, 83, 34, 12219, 24, 2, 1602, 102, 85], [83, 2090, 194, 5046, 0, 0, 0, 0, 0], [90, 83, 119, 3558, 0, 0, 0, 0, 0]]\n",
      "[[43, 743, 6, 1714, 83, 61, 2, 3977, 993], [199, 169, 34, 83, 0, 0, 0, 0, 0], [104, 16, 234, 234, 0, 0, 0, 0, 0], [221, 577, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0], [40, 16, 394, 8370, 0, 0, 0, 0, 0], [205, 647, 5468, 0, 0, 0, 0, 0, 0], [104, 90, 83, 119, 6, 90, 12, 2907, 0]]\n",
      "[[43, 90, 38, 153, 348, 83, 0, 0, 0], [43, 915, 9, 1285, 0, 0, 0, 0, 0], [104, 16, 2, 1830, 0, 0, 0, 0, 0], [590, 4003, 43, 806, 14, 83, 37, 9, 953]]\n",
      "[[43, 90, 38, 271, 0, 0, 0, 0, 0], [43, 271, 394, 1011, 34, 3962, 12318, 0, 0], [2, 6474, 16, 177, 1558, 0, 0, 0, 0], [7944, 65, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[88, 55, 244, 139, 6, 2, 4365, 61, 1535], [86, 43, 915, 224, 6, 2201, 0, 0, 0], [4347, 1622, 0, 0, 0, 0, 0, 0, 0], [14, 16, 5188, 194, 953, 4200, 43, 915, 20722]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0], [83, 34, 9, 5468, 1858, 0, 0, 0, 0], [43, 915, 9, 25458, 0, 0, 0, 0, 0], [104, 90, 83, 119, 6, 90, 8, 394, 6357]]\n",
      "[[83, 348, 83, 34, 193, 5468, 0, 0, 0], [43, 255, 199, 607, 22, 16, 136, 9, 400044], [0, 0, 0, 0, 0, 0, 0, 0, 0], [56, 83, 119, 6, 32, 475, 0, 0, 0]]\n",
      "[[34, 83, 9, 569, 901, 48, 34, 83, 9], [34, 83, 8, 837, 19, 287, 0, 0, 0], [199, 183, 2713, 90, 83, 35, 0, 0, 0], [37670, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[43, 151, 90, 38, 348, 40, 43, 915, 12], [194, 1535, 83, 34, 4436, 0, 0, 0, 0], [4864, 104, 915, 43, 2414, 0, 0, 0, 0], [43, 915, 9, 1007, 27403, 0, 0, 0, 0]]\n",
      "[[104, 88, 83, 90, 12, 287, 0, 0, 0], [104, 1556, 5, 901, 34, 83, 0, 0, 0], [83, 88, 122, 90, 22, 0, 0, 0, 0], [43, 915, 1036, 171, 287, 68, 5, 189, 0]]\n",
      "[[145, 43, 915, 250, 167820, 43, 0, 0, 0], [55, 34, 38, 435, 1382, 115, 55, 34, 38], [55, 271, 0, 0, 0, 0, 0, 0, 0], [2774, 43, 208374, 1356, 0, 0, 0, 0, 0]]\n",
      "[[205, 647, 11873, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [40, 16, 2, 8548, 0, 0, 0, 0, 0]]\n",
      "[[16060, 0, 0, 0, 0, 0, 0, 0, 0], [199, 111, 20910, 90, 83, 35, 0, 0, 0], [90, 83, 271, 43, 915, 2325, 48, 1634, 0], [888, 97, 35, 4155, 0, 0, 0, 0, 0]]\n",
      "[[8237, 1079, 61, 647, 1728, 0, 0, 0, 0], [90, 83, 271, 0, 0, 0, 0, 0, 0], [34, 83, 4436, 0, 0, 0, 0, 0, 0], [83, 42, 2, 217, 806, 0, 0, 0, 0]]\n",
      "[[888, 97, 32, 1097, 7, 16288, 0, 0, 0], [43, 35, 6, 893, 0, 0, 0, 0, 0], [88294, 16, 86, 4297, 5, 1228, 90, 83, 94182], [597, 6, 684, 9, 216, 16, 9, 997, 0]]\n",
      "[[43, 122, 806, 43, 56, 513, 2, 1700, 0], [63, 16, 9, 1890, 38, 9, 1890, 0, 0], [582, 287, 71274, 0, 0, 0, 0, 0, 0], [3901, 228795, 206, 7, 457, 287, 394, 313, 0]]\n",
      "[[3016, 0, 0, 0, 0, 0, 0, 0, 0], [43, 90, 38, 305, 6, 32, 9, 20354, 0], [1363, 287, 9, 525, 61, 9, 6474, 0, 0], [43, 119, 3426, 0, 0, 0, 0, 0, 0]]\n",
      "[[216, 16, 38, 1793, 0, 0, 0, 0, 0], [39, 16, 10928, 0, 0, 0, 0, 0, 0], [199, 88, 55, 702, 0, 0, 0, 0, 0], [194, 1411, 400018, 212, 83, 34, 31, 22491, 11410]]\n",
      "[[34, 71, 441, 75, 3748, 0, 0, 0, 0], [1489, 37, 118, 6, 32, 2323, 0, 0, 0], [90, 83, 3506, 6, 32, 417, 27, 39, 2360], [104, 16, 2, 87, 783, 6, 0, 0, 0]]\n",
      "[[37670, 0, 0, 0, 0, 0, 0, 0, 0], [41, 34, 4742, 0, 0, 0, 0, 0, 0], [90, 83, 735, 8, 1535, 0, 0, 0, 0], [268, 45, 6828, 83, 0, 0, 0, 0, 0]]\n",
      "[[104, 61, 292, 0, 0, 0, 0, 0, 0], [145, 43, 915, 16284, 0, 0, 0, 0, 0], [43, 56, 837, 6, 1079, 6, 83, 0, 0], [457, 287, 66, 394, 51202, 0, 0, 0, 0]]\n",
      "[[90, 83, 119, 1098, 61, 287, 0, 0, 0], [1363, 287, 2, 24637, 0, 0, 0, 0, 0], [83, 34, 193, 5067, 0, 0, 0, 0, 0], [104, 90, 83, 271, 5, 23019, 0, 0, 0]]\n",
      "[[43, 837, 83, 0, 0, 0, 0, 0, 0], [16, 22, 9, 4365, 0, 0, 0, 0, 0], [83, 34, 5207, 0, 0, 0, 0, 0, 0], [14531, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[20, 16, 221, 119, 14, 0, 0, 0, 0], [43, 119, 83, 0, 0, 0, 0, 0, 0], [43, 915, 193, 4436, 0, 0, 0, 0, 0], [4864, 888, 97, 843, 6, 1568, 31439, 0, 0]]\n",
      "[[71, 1142, 171, 104, 16, 783, 6, 103, 2135], [83, 34, 590, 14975, 0, 0, 0, 0, 0], [1363, 287, 9, 525, 0, 0, 0, 0, 0], [43, 122, 240, 2, 2170, 0, 0, 0, 0]]\n",
      "[[104, 90, 83, 271, 43, 915, 0, 0, 0], [56, 83, 119, 6, 32, 1615, 0, 0, 0], [104, 915, 43, 0, 0, 0, 0, 0, 0], [2774, 90, 83, 119, 394, 313, 0, 0, 0]]\n",
      "[[34, 83, 11116, 0, 0, 0, 0, 0, 0], [15919, 1363, 287, 647, 61, 4963, 0, 0, 0], [40, 16, 1535, 0, 0, 0, 0, 0, 0], [1363, 287, 9, 525, 73050, 0, 0, 0, 0]]\n",
      "[[43, 119, 30892, 0, 0, 0, 0, 0, 0], [4870, 14, 39, 319, 319, 49239, 56, 15428, 20191], [43, 119, 136, 16693, 1073, 0, 0, 0, 0], [43, 119, 136, 10819, 0, 0, 0, 0, 0]]\n",
      "[[43, 5022, 116, 56, 32, 9, 355, 81, 0], [104, 16, 394, 313, 0, 0, 0, 0, 0], [14399, 90, 83, 305, 6, 244, 68, 12, 3332], [83, 34, 2, 98, 5207, 901, 43, 35, 663]]\n",
      "[[740, 90, 83, 1450, 287, 0, 0, 0, 0], [104, 16, 2, 2607, 14768, 5, 14261, 0, 0], [328, 15, 8, 66, 5, 83, 14, 16, 22], [83, 90, 38, 161, 1382, 1411, 0, 0, 0]]\n",
      "[[40, 90, 43, 9796, 83, 5, 0, 0, 0], [34, 83, 9, 11298, 0, 0, 0, 0, 0], [7944, 651, 0, 0, 0, 0, 0, 0, 0], [83, 34, 86, 2696, 0, 0, 0, 0, 0]]\n",
      "[[83, 271, 83, 34, 475, 0, 0, 0, 0], [83, 34, 9, 953, 0, 0, 0, 0, 0], [104, 121, 83, 35, 12, 21858, 4387, 0, 0], [16, 39, 9, 221, 236, 5, 394, 81, 0]]\n",
      "[[104, 915, 43, 916, 189, 0, 0, 0, 0], [1187, 16, 127, 9, 3084, 2489, 0, 0, 0], [740, 90, 83, 271, 43, 915, 9, 25458, 0], [13088, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[104, 16, 2, 2170, 6, 216, 2, 5189, 7], [34, 83, 2979, 0, 0, 0, 0, 0, 0], [104, 8239, 16, 394, 3019, 0, 0, 0, 0], [43, 915, 1169, 6, 25703, 0, 0, 0, 0]]\n",
      "[[2, 6699, 1536, 313, 16, 7409, 14, 16, 4003], [104, 34, 83, 2414, 61, 0, 0, 0, 0], [5410, 199, 35, 83, 53, 0, 0, 0, 0], [34, 83, 9, 9249, 0, 0, 0, 0, 0]]\n",
      "[[34, 83, 2325, 0, 0, 0, 0, 0, 0], [35645, 43, 305, 6, 989, 9, 5123, 5627, 0], [90, 83, 348, 3898, 400063, 0, 0, 0, 0], [13077, 50, 1216, 43, 35, 240, 647, 4003, 0]]\n",
      "[[34, 83, 9, 302, 48, 9, 789, 0, 0], [43, 5311, 19, 318, 5311, 0, 0, 0, 0], [2385, 5367, 19, 83, 0, 0, 0, 0, 0], [34, 83, 9, 2360, 0, 0, 0, 0, 0]]\n",
      "[[113, 34, 83, 24, 250, 116, 0, 0, 0], [1527, 66, 8294, 107053, 0, 0, 0, 0, 0], [13077, 11541, 0, 0, 0, 0, 0, 0, 0], [1335, 4963, 19, 9079, 0, 0, 0, 0, 0]]\n",
      "[[104, 90, 83, 348, 61, 1535, 0, 0, 0], [104, 16, 62, 0, 0, 0, 0, 0, 0], [104, 16, 2, 9536, 0, 0, 0, 0, 0], [90, 83, 119, 303, 0, 0, 0, 0, 0]]\n",
      "[[90, 83, 1908, 287, 0, 0, 0, 0, 0], [1376, 34, 122, 2, 64100, 5, 2, 7705, 5], [305, 6, 284, 2084, 0, 0, 0, 0, 0], [21742, 62, 27, 28, 8838, 0, 0, 0, 0]]\n",
      "[[83, 34, 967, 250, 0, 0, 0, 0, 0], [83, 34, 5468, 0, 0, 0, 0, 0, 0], [88, 83, 735, 105, 0, 0, 0, 0, 0], [104, 16, 394, 313, 0, 0, 0, 0, 0]]\n",
      "[[90, 83, 735, 8, 1535, 0, 0, 0, 0], [5286, 55, 2263, 0, 0, 0, 0, 0, 0], [3285, 35, 4700, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[129, 43, 826, 83, 179385, 183, 6244, 8, 2], [0, 0, 0, 0, 0, 0, 0, 0, 0], [43, 119, 48944, 0, 0, 0, 0, 0, 0], [2749, 83, 34, 10269, 0, 0, 0, 0, 0]]\n",
      "[[83, 37140, 407, 9, 875, 61, 3054, 0, 0], [740, 262, 9, 37513, 3831, 178, 2233, 85, 20], [9, 5281, 29516, 0, 0, 0, 0, 0, 0], [38, 590, 860, 85, 83, 34, 9, 721, 0]]\n",
      "[[90, 83, 35, 132, 6769, 15, 199, 43, 88], [43, 93, 903, 83, 4583, 0, 0, 0, 0], [83, 271, 55, 34, 1097, 0, 0, 0, 0], [1087, 837, 287, 60, 0, 0, 0, 0, 0]]\n",
      "[[104, 262, 2213, 287, 1348, 12, 0, 0, 0], [104, 262, 22, 1704, 6, 32, 475, 0, 0], [109, 43, 1714, 394, 313, 0, 0, 0, 0], [31, 7266, 16, 9, 221, 181, 6, 35, 2]]\n",
      "[[400055, 16, 400056, 0, 0, 0, 0, 0, 0], [18438, 17105, 0, 0, 0, 0, 0, 0, 0], [194, 953, 16, 2051, 0, 0, 0, 0, 0], [400100, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[13478, 16, 38, 224, 145, 16, 22, 0, 0], [35, 83, 53, 6, 2, 3379, 0, 0, 0], [88, 83, 2201, 1694, 0, 0, 0, 0, 0], [2, 2303, 5, 216, 0, 0, 0, 0, 0]]\n",
      "[[40, 34, 83, 0, 0, 0, 0, 0, 0], [83, 34, 38, 183, 12, 4365, 375, 0, 0], [104, 56, 83, 119, 6, 1079, 61, 0, 0], [66, 394, 817, 34, 6290, 6, 97, 0, 0]]\n",
      "[[43, 35, 6, 893, 116, 22, 16, 102, 5281], [199, 34, 83, 375, 0, 0, 0, 0, 0], [334, 1107, 6, 32, 9, 8858, 36, 43, 45], [43, 90, 38, 348, 36, 244, 56110, 79, 4582]]\n",
      "[[145, 43, 35, 6, 244, 116, 0, 0, 0], [104, 34, 83, 2520, 0, 0, 0, 0, 0], [90, 83, 119, 11881, 1319, 0, 0, 0, 0], [34, 83, 8032, 287, 0, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36, 43, 305, 79, 4296, 0, 0, 0, 0], [199, 16, 394, 124, 53, 0, 0, 0, 0], [83, 34, 12312, 287, 116, 0, 0, 0, 0], [44, 36354, 121, 83, 808, 24, 0, 0, 0]]\n",
      "[[1535, 16, 194, 8382, 0, 0, 0, 0, 0], [34, 83, 569, 0, 0, 0, 0, 0, 0], [121, 83, 119, 22, 0, 0, 0, 0, 0], [199, 56, 43, 256, 15353, 83, 0, 0, 0]]\n",
      "[[104, 34, 83, 0, 0, 0, 0, 0, 0], [83, 34, 4436, 0, 0, 0, 0, 0, 0], [104, 16, 3368, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[83, 0, 0, 0, 0, 0, 0, 0, 0], [86, 1089, 0, 0, 0, 0, 0, 0, 0], [104, 16, 2, 5161, 81, 0, 0, 0, 0], [88, 43, 35, 394, 2253, 0, 0, 0, 0]]\n",
      "[[22, 16, 5493, 19, 1187, 4754, 0, 0, 0], [90, 83, 837, 287, 0, 0, 0, 0, 0], [34, 83, 9, 9249, 0, 0, 0, 0, 0], [7392, 8, 1882, 14, 16, 193, 992, 2414, 0]]\n",
      "[[88, 83, 685, 5213, 0, 0, 0, 0, 0], [83, 34, 4801, 15, 39, 2559, 0, 0, 0], [83, 34, 9, 4436, 25497, 400012, 0, 0, 0], [199, 61, 9, 188, 5, 7164, 0, 0, 0]]\n",
      "[[199, 90, 83, 664, 0, 0, 0, 0, 0], [1363, 287, 647, 58, 27, 394, 9203, 0, 0], [45, 953, 8078, 50, 124, 192, 76, 2, 87], [1363, 287, 9, 525, 0, 0, 0, 0, 0]]\n",
      "[[199, 169, 34, 83, 0, 0, 0, 0, 0], [2, 270, 210, 5, 2, 210, 5, 569, 1323], [90, 83, 119, 2626, 104581, 0, 0, 0, 0], [34, 83, 2979, 0, 0, 0, 0, 0, 0]]\n",
      "[[43, 915, 394, 5561, 0, 0, 0, 0, 0], [915, 43, 17251, 0, 0, 0, 0, 0, 0], [1309, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[262, 81, 3606, 0, 0, 0, 0, 0, 0], [83, 837, 287, 0, 0, 0, 0, 0, 0], [13077, 0, 0, 0, 0, 0, 0, 0, 0], [4864, 888, 97, 284, 0, 0, 0, 0, 0]]\n",
      "[[43, 915, 9, 34667, 1751, 0, 0, 0, 0], [43, 915, 6334, 43, 121, 38, 1704, 6, 2090], [43, 735, 83, 34, 9, 2360, 0, 0, 0], [115, 43, 957, 83, 0, 0, 0, 0, 0]]\n",
      "[[83, 34, 11560, 140, 2, 1017, 0, 0, 0], [199, 90, 43, 171, 2524, 0, 0, 0, 0], [43, 915, 1903, 6, 1320, 52, 116, 5553, 83], [104, 923, 5, 966, 7723, 121, 83, 35, 0]]\n",
      "[[199, 169, 34, 83, 0, 0, 0, 0, 0], [2400, 1536, 9, 20615, 7, 28678, 3126, 12, 2621], [81, 12, 287, 6, 244, 129, 0, 0, 0], [34, 83, 122, 50, 9249, 0, 0, 0, 0]]\n",
      "[[113, 37, 83, 63, 281, 3882, 2299, 17, 637], [4864, 0, 0, 0, 0, 0, 0, 0, 0], [43, 3469, 194, 216, 42, 58, 2303, 0, 0], [90, 83, 35, 132, 562, 8, 9082, 43, 119]]\n",
      "[[2774, 145, 43, 915, 1087, 83, 34, 9, 418], [88, 83, 26808, 2, 1748, 5, 1535, 0, 0], [129, 104, 90, 83, 305, 6, 90, 0, 0], [194, 313, 16, 12531, 0, 0, 0, 0, 0]]\n",
      "[[9, 25458, 16, 2, 217, 21, 9, 475, 93], [6707, 1228, 16, 569, 0, 0, 0, 0, 0], [194, 22499, 33, 147149, 69, 433, 22, 16, 66], [1363, 287, 104, 83, 90, 0, 0, 0, 0]]\n",
      "[[400050, 83, 34, 19317, 0, 0, 0, 0, 0], [104, 90, 83, 348, 61, 3292, 0, 0, 0], [34, 83, 569, 0, 0, 0, 0, 0, 0], [43, 915, 6334, 199, 183, 16, 2, 1251, 5442]]\n",
      "[[55, 34, 152, 136, 10819, 4387, 0, 0, 0], [43, 122, 90, 38, 1855, 83, 555, 0, 0], [104, 16, 394, 21947, 0, 0, 0, 0, 0], [34, 83, 2277, 62, 19, 194, 7273, 139, 0]]\n",
      "[[83, 161, 12847, 664, 119, 4066, 18501, 7, 12847], [154267, 116, 83, 34, 883, 41363, 0, 0, 0], [740, 90, 62321, 3625, 71, 0, 0, 0, 0], [90, 83, 35, 9, 1382, 5, 6204, 0, 0]]\n",
      "[[43, 915, 2, 6699, 0, 0, 0, 0, 0], [199, 169, 34, 83, 0, 0, 0, 0, 0], [216, 16, 216, 0, 0, 0, 0, 0, 0], [43, 35, 9, 475, 7, 83, 90, 38, 0]]\n",
      "[[56, 83, 590, 305, 6, 161, 1815, 0, 0], [43, 915, 7394, 55, 96, 35, 39, 8294, 4365], [41, 70345, 7, 88034, 76, 83, 0, 0, 0], [43, 305, 6, 1079, 61, 2118, 83, 0, 0]]\n",
      "[[83, 34, 9, 16436, 463, 373, 36, 83, 45], [90, 83, 348, 0, 0, 0, 0, 0, 0], [4864, 43, 837, 83, 9814, 0, 0, 0, 0], [83, 34, 6556, 0, 0, 0, 0, 0, 0]]\n",
      "[[104, 56, 83, 5752, 0, 0, 0, 0, 0], [104, 90, 83, 305, 6, 1473, 0, 0, 0], [34, 83, 1753, 0, 0, 0, 0, 0, 0], [15919, 6334, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[104, 34, 83, 0, 0, 0, 0, 0, 0], [199, 33, 394, 124, 53, 0, 0, 0, 0], [104, 16, 394, 466, 0, 0, 0, 0, 0], [4864, 81, 12, 4155, 0, 0, 0, 0, 0]]\n",
      "[[194, 58549, 16, 551, 5, 29654, 0, 0, 0], [400058, 0, 0, 0, 0, 0, 0, 0, 0], [121, 83, 192, 394, 8926, 375, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[[10928, 0, 0, 0, 0, 0, 0, 0, 0], [43, 3469, 43, 17, 9, 9537, 0, 0, 0], [83, 34, 1903, 6, 287, 0, 0, 0, 0], [3549, 7, 39148, 0, 0, 0, 0, 0, 0]]\n",
      "[[88, 43, 35, 9, 7858, 0, 0, 0, 0], [12540, 0, 0, 0, 0, 0, 0, 0, 0], [86, 43, 915, 9, 475, 136, 0, 0, 0], [90, 83, 119, 20992, 6, 287, 0, 0, 0]]\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0], [56, 83, 2273, 2, 17731, 1527, 238, 6, 238], [4864, 122, 90, 38, 32963, 287, 0, 0, 0], [86, 1089, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3\n",
    "\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run( tf.global_variables_initializer() )\n",
    "\n",
    "n_epoch = 1\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    total_err, steps = 0, 0\n",
    "    epoch_time = time.time()    \n",
    "    trainX, trainY = shuffle(trainX, trainY, random_state=0)\n",
    "    \n",
    "    # use minibatches generated from trainX, trainY \n",
    "    for X, Y in tl.iterate.minibatches(inputs=trainX, targets=trainY,\\\n",
    "                                batch_size=batch_size, shuffle=False):\n",
    "        step_time = time.time()\n",
    "        _encode_seqs, _decode_seqs, _target_seqs, _target_mask = getEncodeNDecode(X,Y)\n",
    "        \n",
    "        print(_encode_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/2000] Iter: 0 loss:12.930230 learning rate:1.000000 \n",
      "Epoch[0/2000] Iter: 20 loss:10.471587 learning rate:1.000000 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-91e2e584f207>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m         _, err = sess.run([train_op, loss],\n\u001b[0;32m     22\u001b[0m                         {encode_seqs: _encode_seqs,    decode_seqs: _decode_seqs,\n\u001b[1;32m---> 23\u001b[1;33m                         target_seqs: _target_seqs,     target_mask: _target_mask } )\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;31m#                lr: learning_rate }  )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mtotal_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#learning_rate = 0.3\n",
    "#asdasda\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run( tf.global_variables_initializer() )\n",
    "\n",
    "n_epoch = 2000\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    total_err, steps = 0, 0\n",
    "    epoch_time = time.time()    \n",
    "    trainX, trainY = shuffle(trainX, trainY, random_state=0)\n",
    "    \n",
    "    # use minibatches generated from trainX, trainY \n",
    "    for X, Y in tl.iterate.minibatches(inputs=trainX, targets=trainY,\\\n",
    "                                batch_size=batch_size, shuffle=False):\n",
    "        step_time = time.time()\n",
    "        _encode_seqs, _decode_seqs, _target_seqs, _target_mask = getEncodeNDecode(X,Y)\n",
    "        \n",
    "        _, err = sess.run([train_op, loss],\n",
    "                        {encode_seqs: _encode_seqs,    decode_seqs: _decode_seqs,\n",
    "                        target_seqs: _target_seqs,     target_mask: _target_mask } )\n",
    "        #                lr: learning_rate }  )\n",
    "        total_err += err; \n",
    "        \n",
    "        if steps % (20) == 0:\n",
    "            print(\"Epoch[%d/%d] Iter: %d loss:%f learning rate:%f \" \\\n",
    "                    % (epoch, n_epoch, steps,  err, 1) )\n",
    "            #print('global steps is ', global_step)\n",
    "        steps += 1\n",
    "\n",
    "        ####### \n",
    "        ## test for  bot responses\n",
    "        #######\n",
    "        if steps % (100) == 0:\n",
    "            test_qns = [\"happy birthday have a nice day\", \"is it going to rain tomorrow\"  ]\n",
    "            for qn in test_qns:\n",
    "                print(\"Qn >\", qn)\n",
    "                seed_id = [word2id[w] for w in qn.split(\" \")]\n",
    "                \n",
    "                # Get 2  responses\n",
    "                for _ in range(2):\n",
    "                    # Use encoding layers final state\n",
    "                    state = sess.run(net_rnn.final_state_encode,\n",
    "                                    {encode_seqs2: [seed_id]})\n",
    "                    \n",
    "                    # Start decoding : decode indicator : start_id, get first word                    \n",
    "                    o, state = sess.run([y, net_rnn.final_state_decode],\n",
    "                                    {net_rnn.initial_state_decode: state,\\\n",
    "                                     decode_seqs2: [[start_id]]})\n",
    "                    \n",
    "                    w_id = tl.nlp.sample_top(o[0], top_k=3)\n",
    "                    w = id2word[w_id]\n",
    "                    \n",
    "                    # 3. decode, feed state iteratively\n",
    "                    sentence = [w]\n",
    "                    for _ in range(30): # max sentence length\n",
    "                        o, state = sess.run([y, net_rnn.final_state_decode],\n",
    "                                        {net_rnn.initial_state_decode: state,\n",
    "                                        decode_seqs2: [[w_id]]})\n",
    "                        w_id = tl.nlp.sample_top(o[0], top_k=2)\n",
    "                        w = id2word[w_id]\n",
    "                        if w_id == end_id:\n",
    "                            break\n",
    "                        sentence = sentence + [w]\n",
    "                    print(\"BOT ===> \", ' '.join(sentence))\n",
    "    ##########\n",
    "    ## Decay Learning rate @ every epoch\n",
    "    ##########\n",
    "    #learning_rate = learning_rate * lr_decay_rate\n",
    "    #if learning_rate < min_learning_rate:\n",
    "    #    learning_rate = min_learning_rate\n",
    "        \n",
    "        \n",
    "    print(\"Epoch[%d/%d] averaged loss:%f took:%.5fs\" % (epoch, n_epoch, total_err/steps, time.time()-epoch_time))\n",
    "    \n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Computation Space/ time reduction :\n",
    "\n",
    "1. Negative Sampling for target sequences :\n",
    "2. Training : Smaller / limited WordVector embeddings\n",
    "3. Prediction : Trained embedding + Bigger embedding containing words not in the trained embeddings\n",
    "\n",
    "During training, use  only the words in the training instance. But since we want to be able to predict words that  are limited not only  by the  training words. During the prediction or chat bot resonse generation phase, we can use bigger embedding matrix for lookup to supplement the trained embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Future  :  To Dos\n",
    "\n",
    "\n",
    "\n",
    "1. **Loss Function** : Evaluate it against other loss functions such as perplexity, BLEU or other measures\n",
    "2. **Beam Search**: Increased performance implied by literatures.\n",
    "3. **Attention Mechanism** : Increased performance implied by literatures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
